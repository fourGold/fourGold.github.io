<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Spark_SparkCore, 觉浅">
    <meta name="description" content="Jinxin Li的个人博客">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Spark_SparkCore | 觉浅</title>
    <link rel="icon" type="image/png" href="/favicon.png">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">

    <script src="/libs/jquery/jquery.min.js"></script>

<meta name="generator" content="Hexo 5.3.0"></head>




<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">觉浅</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/contact" class="waves-effect waves-light">
      
      <i class="fas fa-comments" style="zoom: 0.6;"></i>
      
      <span>留言板</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>友情链接</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">觉浅</div>
        <div class="logo-desc">
            
            Jinxin Li的个人博客
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/contact" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-comments"></i>
			
			留言板
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			友情链接
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="http://github.com/fourgold/" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="http://github.com/fourgold/" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/featureimages/14.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Spark_SparkCore</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        width: 345px;
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        padding: 35px 0 15px 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content {
        padding-bottom: 30px;
        overflow: auto;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;

        position: absolute;
        right: 23.5vw;
        display: block;
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 15px;
        bottom: 76px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/spark/">
                                <span class="chip bg-color">spark</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/spark/" class="post-category">
                                spark
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2021-01-03
                </div>
                

                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    9.4k
                </div>
                

                

                
            </div>
        </div>
        <hr class="clearfix">

        
        <!-- 是否加载使用自带的 prismjs. -->
        <link rel="stylesheet" href="/libs/prism/prism.css">
        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <h1 id="0x1-spark概述"><a href="#0x1-spark概述" class="headerlink" title="0x1 spark概述"></a>0x1 spark概述</h1><h2 id="1-历史"><a href="#1-历史" class="headerlink" title="1.历史"></a>1.历史</h2><p>在之前的学习中，Hadoop的MapReduce是大家广为熟知的计算框架，那为什么咱们还要学习新的计算框架Spark呢，这里就不得不提到Spark和Hadoop的关系。</p>
<p>首先从时间节点上来看:</p>
<p>Hadoop</p>
<ol>
<li>2006年1月，Doug Cutting加入Yahoo，领导Hadoop的开发</li>
<li>2008年1月，Hadoop成为Apache顶级项目</li>
<li>2011年1.0正式发布</li>
<li>2012年3月稳定版发布</li>
<li>2013年10月发布2.X (Yarn)版本</li>
</ol>
<p>Spark</p>
<ol>
<li>2009年，Spark诞生于伯克利大学的AMPLab实验室</li>
<li>2010年，伯克利大学正式开源了Spark项目</li>
<li>2013年6月，Spark成为了Apache基金会下的项目</li>
<li>2014年2月，Spark以飞快的速度成为了Apache的顶级项目</li>
<li>2015年至今，Spark变得愈发火爆，大量的国内公司开始重点部署或者使用Spark</li>
</ol>
<h2 id="2-Spark核心模块"><a href="#2-Spark核心模块" class="headerlink" title="2.Spark核心模块"></a>2.Spark核心模块</h2><p><img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_21_01/Spark%E6%A0%B8%E5%BF%83%E6%A8%A1%E5%9D%97.png" alt="spark核心模块"></p>
<p><strong>Spark Core</strong></p>
<p>Spark Core中提供了Spark最基础与最核心的功能，Spark其他的功能如：Spark SQL，Spark Streaming，GraphX, MLlib都是在Spark Core的基础上进行扩展的</p>
<p><strong>Spark SQL</strong></p>
<p>Spark SQL是Spark用来操作结构化数据的组件。通过Spark SQL，用户可以使用SQL或者Apache Hive版本的SQL方言（HQL）来查询数据。</p>
<p><strong>Spark Streaming</strong></p>
<p>Spark Streaming是Spark平台上针对实时数据进行流式计算的组件，提供了丰富的处理数据流的API。</p>
<p><strong>Spark MLlib</strong></p>
<p>MLlib是Spark提供的一个机器学习算法库。MLlib不仅提供了模型评估、数据导入等额外的功能，还提供了一些更底层的机器学习原语。</p>
<p><strong>Spark GraphX</strong></p>
<p>GraphX是Spark面向图计算提供的框架与算法库。</p>
<h2 id="3-入门wordCount"><a href="#3-入门wordCount" class="headerlink" title="3.入门wordCount"></a>3.入门wordCount</h2><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql">object wordCountTest &#123;
  def main(args: Array[String]): Unit &#x3D; &#123;
    &#x2F;&#x2F;spark标准获取流程
    val sparkConf: SparkConf &#x3D; new SparkConf().setAppName(&quot;wordCount&quot;).setMaster(&quot;local[1]&quot;)
    val sc &#x3D; new SparkContext(sparkConf)
    &#x2F;&#x2F;创建RDD,使用textFile的方式创建RDD
    val line: RDD[String] &#x3D; sc.textFile(&quot;.&#x2F;word.txt&quot;, 1)
    val result: RDD[(String, Int)] &#x3D; line.flatMap(_.split(&quot; &quot;)).map((_, 1)).reduceByKey(_ + _)
    &#x2F;&#x2F;打印
    result.collect().foreach(println)
    sc.close()
  &#125;
&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="4-spark运行模式"><a href="#4-spark运行模式" class="headerlink" title="4.spark运行模式"></a>4.spark运行模式</h2><table>
<thead>
<tr>
<th>Spark运行环境</th>
<th>用法</th>
</tr>
</thead>
<tbody><tr>
<td>Local模式</td>
<td>测试</td>
</tr>
<tr>
<td>Standalone模式</td>
<td>独立部署Matser-Worker<br/>自身提供计算资源,降低了耦合性</td>
</tr>
<tr>
<td>Yarn模式</td>
<td>提供集群模式</td>
</tr>
<tr>
<td>K8S模式</td>
<td></td>
</tr>
<tr>
<td>Mesos模式</td>
<td>Twitter</td>
</tr>
<tr>
<td>Windows模式</td>
<td></td>
</tr>
</tbody></table>
<h2 id="5-spark端口号"><a href="#5-spark端口号" class="headerlink" title="5.spark端口号"></a>5.spark端口号</h2><table>
<thead>
<tr>
<th>Spark端口号</th>
<th>详解</th>
<th>Hadoop</th>
</tr>
</thead>
<tbody><tr>
<td>内部通信:==7077==</td>
<td>Spark Master内部通信服务端口号</td>
<td>8020/9000</td>
</tr>
<tr>
<td>Master资源监控:8080/改==8989==</td>
<td>Standalone模式下，Spark Master Web端口号</td>
<td>9870</td>
</tr>
<tr>
<td>Spark-Shell监控:==4040==</td>
<td>Spark查看当前Spark-shell运行任务情况端口号</td>
<td></td>
</tr>
<tr>
<td>Spark使用HDFS端口:==8020==</td>
<td></td>
<td></td>
</tr>
<tr>
<td>历史服务器UI端口:==18080==</td>
<td>Spark历史服务器端口号</td>
<td>19888</td>
</tr>
<tr>
<td>Yarn:==8088==</td>
<td>Hadoop YARN任务运行情况查看端口号</td>
<td>8088</td>
</tr>
</tbody></table>
<h2 id="6-Spark运行组件"><a href="#6-Spark运行组件" class="headerlink" title="6.Spark运行组件"></a>6.Spark运行组件</h2><p>Spark框架的核心是一个计算引擎，整体来说，它采用了标准 master-slave 的结构。</p>
<p><img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_21_01/Spark%E8%BF%90%E8%A1%8C%E7%BB%84%E4%BB%B6.png" alt="Spark运行组件"></p>
<h3 id="6-1Spark-Executor"><a href="#6-1Spark-Executor" class="headerlink" title="6.1Spark Executor"></a>6.1Spark Executor</h3><p>集群中运行在==工作节点（Worker）中的一个JVM进程==，是整个集群中的专门用于计算的节点。在提交应用中，可以提供参数指定计算节点的个数，以及对应的资源。这里的资源一般指的是工作节点Executor的内存大小和使用的虚拟CPU核（Core）数量。</p>
<table>
<thead>
<tr>
<th>名称</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>–num-executors</td>
<td>配置Executor的数量</td>
</tr>
<tr>
<td>–executor-memory</td>
<td>配置每个Executor的内存大小</td>
</tr>
<tr>
<td>–executor-cores</td>
<td>配置每个Executor的虚拟CPU   core数量</td>
</tr>
<tr>
<td></td>
<td>Executor如果是3核,设备是单核,模拟的多线程操作,其实是并发操作</td>
</tr>
</tbody></table>
<h3 id="6-2并行与并发"><a href="#6-2并行与并发" class="headerlink" title="6.2并行与并发"></a>6.2并行与并发</h3><p>我们会给Executor分配虚拟的核心数量,如果核心不够会触发多线程操作,并发</p>
<p>如果核心够用,则进行并行操作,可以进行配置</p>
<h3 id="6-3有向无环图"><a href="#6-3有向无环图" class="headerlink" title="6.3有向无环图"></a>6.3有向无环图</h3><p>表示一种依赖关系,依赖关系形成的拓扑图形称为DAG,有向无环图</p>
<pre class="mermaid">graph LR
A-->B
B-->C
B-->D
D--禁止-->A</pre>

<p>如图,D向A会形成无环图,有环会形成死循环(与maven类似)</p>
<h2 id="7-Yarn-Cluster任务提交流程"><a href="#7-Yarn-Cluster任务提交流程" class="headerlink" title="7.Yarn Cluster任务提交流程"></a>7.Yarn Cluster任务提交流程</h2><p><strong>核心</strong>:分两大块 1.资源的申请 2.计算的准备 任务发给资源</p>
<p>Client与Cluster区别在于Driver程序运行的节点位置</p>
<p><img src="https://i.loli.net/2020/12/23/JjlbBD6sYXgC3Hx.png" alt="Spark-Yarn提交流程"></p>
<ol>
<li>任务提交</li>
<li>向ResourceManager通讯申请启动ApplicationMaster</li>
<li>ApplicationMaster选择合适的节点借用NodeManager启动一个container</li>
<li>在container中运行AppMatser=Driver</li>
<li>Driver启动后,向RM申请container运行Executor进程</li>
<li>Executor进程启动后反向向Driver进行注册</li>
<li>全部注册完成后开始执行main函数</li>
<li>执行到action算子,触发一个job,根据是否发生shuffle开始划分stage</li>
<li>每个stage生成对应的TaskSet[task1,task2,task3…]</li>
<li>然后将task分发到各个Executor上执行</li>
</ol>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">[shenneng@hadoop102 spark-yarn]$ bin&#x2F;spark-submit \
--class org.apache.spark.examples.SparkPi \
--master yarn \
--deploy-mode cluster \
.&#x2F;examples&#x2F;jars&#x2F;spark-examples_2.12-3.0.0.jar \
10<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h1 id="0x2-Spark框架"><a href="#0x2-Spark框架" class="headerlink" title="0x2 Spark框架"></a>0x2 Spark框架</h1><p>Spark和Hadoop的根本差异是多个作业之间的数据通信问题</p>
<p><img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_20_12/Spark%E8%BF%90%E8%A1%8C%E6%A8%A1%E5%BC%8F%E7%A4%BA%E6%84%8F%E5%9B%BE.png" alt="Spark运行模式示意图"></p>
<table>
<thead>
<tr>
<th>Spark运行环境</th>
<th>用法</th>
</tr>
</thead>
<tbody><tr>
<td>Local模式</td>
<td>测试</td>
</tr>
<tr>
<td>Standalone模式</td>
<td>独立部署Matser-Worker<br/>自身提供计算资源,降低了耦合性</td>
</tr>
<tr>
<td>Yarn模式</td>
<td>提供集群模式</td>
</tr>
<tr>
<td>K8S模式</td>
<td></td>
</tr>
<tr>
<td>Mesos模式</td>
<td>Twitter</td>
</tr>
<tr>
<td>Windows模式</td>
<td></td>
</tr>
</tbody></table>
<h2 id="1-Yarn模式"><a href="#1-Yarn模式" class="headerlink" title="1.Yarn模式"></a>1.Yarn模式</h2><p><img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_20_12/Yarn-Cluster%E6%8F%90%E4%BA%A4%E6%B5%81%E7%A8%8B.png" alt="Yarn-Cluster提交流程"></p>
<p>Client与Cluster的主要区别是Driver是否在本地运行</p>
<h2 id="2-分布式计算模拟"><a href="#2-分布式计算模拟" class="headerlink" title="2.分布式计算模拟"></a>2.分布式计算模拟</h2><p><img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_20_12/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97%E6%A0%B8%E5%BF%83.png" alt="分布式计算核心-拆分Task"></p>
<p>通过简单的分布式计算模拟,理解任务的拆分,运行的模块,并行的原理,RDD的封装,底层数据结构</p>
<p><strong>DRIVER</strong></p>
<pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">&#x2F;&#x2F;DRIVER

package com.ecust.saprkcore

import java.io.&#123;ObjectOutputStream, OutputStream&#125;
import java.net.&#123;ServerSocket, Socket&#125;

&#x2F;**
 * @author Jinxin Li
 * @create 2020-12-31 13:43
 *&#x2F;
object Driver &#123;
  def main(args: Array[String]): Unit &#x3D; &#123;

    &#x2F;&#x2F;进行逻辑的封装,计算的准备,数据的提交

    val client1 &#x3D; new Socket(&quot;localhost&quot;, 9999)
    val client2 &#x3D; new Socket(&quot;localhost&quot;, 8888)

    val out1: OutputStream &#x3D; client1.getOutputStream
    val out2: OutputStream &#x3D; client2.getOutputStream

    val objOut1 &#x3D; new ObjectOutputStream(out1)
    val objOut2 &#x3D; new ObjectOutputStream(out2)

    val  task &#x3D; new Task()

    val subTask1 &#x3D; new SubTask()
    subTask1.logic&#x3D;task.logic
    subTask1.data&#x3D;task.data.take(2)

    val subTask2 &#x3D; new SubTask()
    subTask2.logic&#x3D;task.logic
    subTask2.data&#x3D;task.data.takeRight(2)

    objOut1.writeObject(subTask1)
    objOut1.flush()
    objOut1.close()

    objOut2.writeObject(subTask2)
    objOut2.flush()
    objOut2.close()

    &#x2F;&#x2F;发送,注意在网络中传递的数据要进行序列化,不可能传递对象,必须序列化
    println(&quot;任务发送完毕&quot;)

    &#x2F;&#x2F;关闭客户端
    client1.close()
    client2.close()
  &#125;
&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><strong>EXECUTOR1</strong></p>
<pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">object Executor1 &#123;
  def main(args: Array[String]): Unit &#x3D; &#123;

    &#x2F;&#x2F;启动服务器,接受数据
    val server &#x3D; new ServerSocket(9999)

    println(&quot;服务器9999启动,等待接受数据...&quot;)

    val client: Socket &#x3D; server.accept()

    val in: InputStream &#x3D; client.getInputStream

    val objIn &#x3D; new ObjectInputStream(in)

    val task &#x3D; objIn.readObject().asInstanceOf[SubTask]

    val ints: List[Int] &#x3D; task.compute()

    println(&quot;接收到客户端9999接受的数据:&quot;+ints)

    objIn.close()
    client.close()
    server.close()
  &#125;
&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><strong>EXECUTOR2</strong></p>
<pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">object Executor2 &#123;
  def main(args: Array[String]): Unit &#x3D; &#123;

    &#x2F;&#x2F;启动服务器,接受数据
    val server &#x3D; new ServerSocket(8888)

    println(&quot;服务器9999启动,等待接受数据...&quot;)

    val client: Socket &#x3D; server.accept()

    val in: InputStream &#x3D; client.getInputStream

    val objIn &#x3D; new ObjectInputStream(in)

    val task&#x3D; objIn.readObject().asInstanceOf[SubTask]

    val ints: List[Int] &#x3D; task.compute()

    println(&quot;接收到客户端8888接受的数据:&quot;+ints)

    objIn.close()
    client.close()
    server.close()
  &#125;
&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><strong>SUBTASK</strong></p>
<pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">class SubTask extends Serializable &#123;
  &#x2F;&#x2F;这是一种特殊的数据结构,其中包含了数据的格式,数据的计算逻辑与算子转换
  &#x2F;&#x2F;接收到数据之后,可以进行计算
  &#x2F;&#x2F;RDD 广播变量 累加器 就是类似的数据结构
  var data :List[Int] &#x3D; _
  var logic:Int&#x3D;&gt;Int &#x3D; _

  &#x2F;&#x2F;计算任务
  def  compute() &#x3D;&#123;
    data.map(logic)
  &#125;
&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><strong>TASK</strong></p>
<pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">class Task extends Serializable &#123;&#x2F;&#x2F;实现序列化 特质
  &#x2F;&#x2F;包含原数据的数据结构
  val data &#x3D; List(1, 2, 3, 4)

  val function: Int &#x3D;&gt; Int &#x3D; (num: Int) &#x3D;&gt; &#123;
    num * 2
  &#125;

  &#x2F;&#x2F;注意函数的类型是Int&#x3D;&gt;Int
  val logic:Int&#x3D;&gt;Int &#x3D; _*2

  &#x2F;&#x2F;计算任务
  def  compute() &#x3D;&#123;
    data.map(logic)
  &#125;
&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="3-RDD与IO"><a href="#3-RDD与IO" class="headerlink" title="3.RDD与IO"></a>3.RDD与IO</h2><p><strong>字节流&amp;字符流</strong></p>
<pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">InputStream in &#x3D; new FileInputStream(&quot;path&quot;)
int i &#x3D; -1
while(i &#x3D; in.read()!&#x3D;-1)&#123;
    println(i)
&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre class="mermaid">graph LR
File-->FileInputStream--read-->console</pre>

<p><strong>缓冲流</strong></p>
<pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">InputStream in &#x3D; new BufferedInputStream(new FileInputStream(&quot;path&quot;))
int i &#x3D; -1
while(i &#x3D; in.read()!&#x3D;-1)&#123;
    println(i)
&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre class="mermaid">graph LR
File-->FileInputStream-->BufferedInputStream--read-->console</pre>

<p><img src="C:%5CUsers%5C89388%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20201231224057782.png" alt="缓冲区的缓冲流"></p>
<p><strong>转换流</strong>InputStreamReader</p>
<pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">Reader in &#x3D; new BufferedReader(
    new InputStreamReader(
        new FileInputStream(&quot;path&quot;),
        &quot;UTF-8&quot;
        )
    )
String s &#x3D; null
while((s&#x3D;in.readLine())!&#x3D;null)&#123;
    println(i);
    )<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_20_12/%E6%B5%81%E7%9A%84%E8%A3%85%E9%A5%B0%E8%80%85%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F.png" alt="装饰者设计模式的IO"></p>
<p>可以看出核心是FileInputFormat,转换流与缓冲流都是包装,这种设计模式成为装饰者设计模式</p>
<p>哪些inputformat,都是对读取逻辑的封装,没有真正的读取数据</p>
<p>readLine才会真正的执行,new的过程仅仅是建立连接,但是没有真正的读取,有种延迟加载的感觉</p>
<p>RDD的组装方式,与IO的封装也是非常的类似</p>
<pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">new HadoopRDD&#x2F;&#x2F;textFile
new MapPartitionsRDD()&#x2F;&#x2F;flatMap
new MapPartitionsRDD()&#x2F;&#x2F;map
new ShuffleRDD()&#x2F;&#x2F;reduceByKey

&#x2F;&#x2F;执行
rdd.collect()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>一层一层的包装</p>
<p><img src="C:%5CUsers%5C89388%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20201231232951858.png" alt="RDD的装饰者模式理解"></p>
<p>RDD的数据处理方式类似于IO流,也有装饰者设计模式</p>
<p>RDD的数据只有在调用collect方法时,才会真正的执行</p>
<p>RDD是不保存数据的,但是IO可以临时保存一部分数据</p>
<h2 id="4-分区"><a href="#4-分区" class="headerlink" title="4. 分区"></a>4. 分区</h2><p>RDD是一个最基本的数据处理模型</p>
<p>类似于Kafka中的分区,我们将数据进行分区,分区之后分成不成的Task,可以分发至Executor进行计算</p>
<p>RDD是最小的数据处理单元,里面包含了分区信息,提高并行计算的能力</p>
<p><img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_20_12/RDD%E6%95%B0%E6%8D%AE%E7%9A%84%E5%88%87%E5%88%86.png" alt="数据的分区"></p>
<h1 id="0x3-spark核心"><a href="#0x3-spark核心" class="headerlink" title="0x3 spark核心"></a>0x3 spark核心</h1><h2 id="1-spark核心三大数据结构"><a href="#1-spark核心三大数据结构" class="headerlink" title="1. spark核心三大数据结构"></a>1. spark核心三大数据结构</h2><p>RDD : 弹性分布式数据集</p>
<p>累加器：分布式共享只写变量</p>
<p>广播变量：分布式共享只读变量</p>
<h2 id="2-RDD基本概念"><a href="#2-RDD基本概念" class="headerlink" title="2.RDD基本概念"></a>2.RDD基本概念</h2><p>RDD（Resilient Distributed Dataset）叫做弹性分布式数据集，是Spark中最基本的数据处理模型。代码中是一个抽象类，它代表一个弹性的、不可变、可分区、里面的元素可并行计算的集合。</p>
<p><strong>1.弹性</strong></p>
<p>存储的弹性：内存与磁盘的自动切换；</p>
<p>容错的弹性：数据丢失可以自动恢复；</p>
<p>计算的弹性：计算出错重试机制；</p>
<p>分片的弹性：可根据需要重新分片。</p>
<p><strong>2.分布式</strong>：数据存储在大数据集群不同节点上</p>
<p><strong>3.数据集</strong>：RDD封装了计算逻辑，并不保存数据</p>
<p><strong>4.数据抽象</strong>：RDD是一个抽象类，需要子类具体实现</p>
<p><strong>5.不可变</strong>：RDD封装了计算逻辑，是不可以改变的，想要改变，只能产生新的RDD，在新的RDD里面封装计算逻辑</p>
<p><strong>6.可分区、并行计算</strong></p>
<h2 id="3-RDD核心属性"><a href="#3-RDD核心属性" class="headerlink" title="3.RDD核心属性"></a>3.RDD核心属性</h2><p><a target="_blank" rel="noopener" href="https://data-flair.training/blogs/spark-rdd-tutorial/">RDD详细描述</a></p>
<p><img src="https://i.loli.net/2020/12/23/PTqaCxnHeBdl91G.png" alt="SparkRDD的核心属性"></p>
<p>图:Spark RDD核心属性</p>
<ol>
<li><p>粗粒度操作(无法对单个元素进行操作)</p>
</li>
<li><p>内存中计算</p>
</li>
<li><p>懒执行</p>
</li>
<li><p>不变性</p>
</li>
<li><p>容错性</p>
</li>
<li><p>持久性(cache可以选择等级与checkpoint)</p>
<pre class="line-numbers language-sql" data-language="sql"><code class="language-sql">--数据缓存
wordToOneRdd.cache()
--可以更改存储级别
mapRdd.persist(StorageLevel.MEMORY_AND_DISK_2)

--设置检查点路径
sc.setCheckpointDir(&quot;.&#x2F;checkpoint1&quot;)
--数据检查点：针对wordToOneRdd做检查点计算
wordToOneRdd.checkpoint()
--一般两者联合使用<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li>
<li><p>可分区(分区列表)</p>
</li>
<li><p>粘度分区(自定分区)</p>
</li>
</ol>
<h2 id="4-RDD缺点"><a href="#4-RDD缺点" class="headerlink" title="4.RDD缺点"></a>4.RDD缺点</h2><ol>
<li>没有内置的优化引擎,RDD无法利用Spark的高级优化器（包括catalyst optimizer与Tungsten执行引擎）的优势。开发人员需要根据其属性优化每个RDD</li>
<li>只能处理结构化数据与DataFrame和数据集不同，RDD不会推断所摄取数据的模式，而是需要用户指定它。</li>
<li>性能限制,作为内存中的JVM对象，RDD涉及垃圾收集和Java序列化的开销，这在数据增长时非常昂贵。</li>
<li>没有足够的内存来存储RDD时，它们会拖慢运行速度。也可以将RDD的该分区存储在不适合RAM的磁盘上。结果，它将提供与当前数据并行系统类似的性能。</li>
</ol>
<h2 id="5-RDD的来源"><a href="#5-RDD的来源" class="headerlink" title="5.RDD的来源"></a>5.RDD的来源</h2><ol>
<li>使用集合创建parallelize MakeRDD</li>
<li>外部存储文件创建RDD textfile</li>
<li>从其他RDD创建(血缘关系,cache,checkpoint)</li>
<li>直接创建RDD 内部使用</li>
</ol>
<h2 id="6-RDD的-分区分片问题"><a href="#6-RDD的-分区分片问题" class="headerlink" title="6.RDD的==分区分片问题=="></a>6.RDD的==分区分片问题==</h2><p>RDD分区意味着一个分区一个job么,</p>
<p>RDD分区3意味着要在三个executor里执行么</p>
<p>重新分区,加入三个executor在不同的container里是如何发生shuffle里的,还是三个分区是一个job,这一个job在一个container里执行</p>
<h2 id="7-RDD的序列化"><a href="#7-RDD的序列化" class="headerlink" title="7.RDD的序列化"></a>7.RDD的序列化</h2><p>从计算的角度, 算子以外的代码都是在Driver端执行, 算子里面的代码都是在Executor端执行。</p>
<p>那么在scala的函数式编程中，就会导致算子内经常会用到算子外的数据，这样就形成了闭包的效果，如果使用的算子外的数据无法序列化，就意味着无法传值给Executor端执行，就会发生错误</p>
<p>所以需要在执行任务计算前，检测闭包内的对象是否可以进行序列化，这个操作我们称之为闭包检测。Scala2.12版本后闭包编译方式发生了改变  </p>
<p><strong>Kryo序列化框架</strong></p>
<p>Spark2.0开始支持另外一种Kryo序列化机制。Kryo速度是Serializable的10倍。当RDD在Shuffle数据的时候，简单数据类型、数组和字符串类型已经在Spark内部使用Kryo来序列化。</p>
<p>即使使用Kryo序列化，也要继承Serializable接口</p>
<h2 id="8-RDD依赖与血缘"><a href="#8-RDD依赖与血缘" class="headerlink" title="8.RDD依赖与血缘"></a>8.RDD依赖与血缘</h2><h3 id="8-1概述"><a href="#8-1概述" class="headerlink" title="8.1概述"></a>8.1概述</h3><p>RDD的Lineage会记录RDD的元数据信息和转换行为，当该RDD的部分分区数据丢失时，它可以根据这些信息来重新运算和恢复丢失的数据分区。</p>
<p>相邻的两个RDD之间的关系称为<strong>依赖关系</strong></p>
<p>多个连续的RDD的依赖关系,称之为<strong>血缘关系</strong></p>
<p>我们的每一个RDD都会保存我们的血缘关系,会保存之前的血缘关系</p>
<p>RDD为了提供容错性,需要将RDD间的关系保存下来,一旦出现错误,可以根据血缘关系将数据源重新计算</p>
<pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">val rdd1 &#x3D; rdd.map(_.2)
&#x2F;&#x2F;新的RDD依赖于旧的RDD<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<pre class="mermaid">graph LR
RDD1--依赖-->RDD2--依赖-->RDD3--依赖-->RDD4
RDD4--flatmap-->RDD3
RDD3--map-->RDD2
RDD2--reduceByKey-->RDD1</pre>

<h3 id="8-2血缘关系的查看"><a href="#8-2血缘关系的查看" class="headerlink" title="8.2血缘关系的查看"></a>8.2血缘关系的查看</h3><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">&#x2F;&#x2F;血缘关系的演示
&#x2F;&#x2F;每个RDD记录了以前所有的血缘关系
package com.test
import org.apache.spark.api.java.JavaSparkContext.fromSparkContext
import org.apache.spark.rdd.RDD
import org.apache.spark.&#123;SparkConf, SparkContext&#125;

&#x2F;**
 * @author Jinxin Li
 * @create 2020-10-26 10:04
 *&#x2F;
object wordCount &#123;
  def main(args: Array[String]): Unit &#x3D; &#123;
    val config: SparkConf &#x3D; new SparkConf().setAppName(&quot;MyWordCount&quot;).setMaster(&quot;local[*]&quot;)
    val sc &#x3D; new SparkContext(config)
    val lines: RDD[String] &#x3D; sc.textFile(&quot;.&#x2F;word.txt&quot;)
    println(&quot;++++++++++++++++++++++++++++++++++&#x3D;&quot;)
    println(lines.toDebugString)&#x2F;&#x2F;打印血缘关系
    val words: RDD[String] &#x3D; lines.flatMap(_.split(&quot; &quot;))
    println(&quot;++++++++++++++++++++++++++++++++++&#x3D;&quot;)
    println(words.toDebugString)
    val pairs: RDD[(String, Int)] &#x3D; words.map((_, 1))
    println(&quot;++++++++++++++++++++++++++++++++++&#x3D;&quot;)
    println(pairs.toDebugString)
    val word: RDD[(String, Int)] &#x3D; pairs.reduceByKey(_ + _)
    println(&quot;++++++++++++++++++++++++++++++++++&#x3D;&quot;)
    println(word.toDebugString)
    println(&quot;++++++++++++++++++++++++++++++++++&#x3D;&quot;)
    word.collect().foreach(println(_))
    sc.close;
  &#125;
&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-java" data-language="java"><code class="language-java">++++++++++++++++++++++++++++++++++&#x3D;
(2) .&#x2F;word.txt MapPartitionsRDD[1] at textFile at wordCount.scala:15 []
 |  .&#x2F;word.txt HadoopRDD[0] at textFile at wordCount.scala:15 []
++++++++++++++++++++++++++++++++++&#x3D;
(2) MapPartitionsRDD[2] at flatMap at wordCount.scala:18 []
 |  .&#x2F;word.txt MapPartitionsRDD[1] at textFile at wordCount.scala:15 []
 |  .&#x2F;word.txt HadoopRDD[0] at textFile at wordCount.scala:15 []
++++++++++++++++++++++++++++++++++&#x3D;
(2) MapPartitionsRDD[3] at map at wordCount.scala:21 []
 |  MapPartitionsRDD[2] at flatMap at wordCount.scala:18 []
 |  .&#x2F;word.txt MapPartitionsRDD[1] at textFile at wordCount.scala:15 []
 |  .&#x2F;word.txt HadoopRDD[0] at textFile at wordCount.scala:15 []
++++++++++++++++++++++++++++++++++&#x3D;
(2) ShuffledRDD[4] at reduceByKey at wordCount.scala:24 []
    &#x2F;&#x2F;这个地方断开,表示shuffle +-
 +-(2) MapPartitionsRDD[3] at map at wordCount.scala:21 []
    |  MapPartitionsRDD[2] at flatMap at wordCount.scala:18 []
    |  .&#x2F;word.txt MapPartitionsRDD[1] at textFile at wordCount.scala:15 []
    |  .&#x2F;word.txt HadoopRDD[0] at textFile at wordCount.scala:15 []
++++++++++++++++++++++++++++++++++&#x3D;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>可以看出每个RDD会存储所有的血缘关系</p>
<p>同时使用dependices可以查看依赖关系</p>
<pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">object wordCount &#123;
  def main(args: Array[String]): Unit &#x3D; &#123;
    val config: SparkConf &#x3D; new SparkConf().setAppName(&quot;MyWordCount&quot;).setMaster(&quot;local[*]&quot;)
    val sc &#x3D; new SparkContext(config)
    val lines: RDD[String] &#x3D; sc.textFile(&quot;.&#x2F;word.txt&quot;)
    println(&quot;++++++++++++++++++++++++++++++++++&#x3D;&quot;)
    println(lines.dependencies)
    val words: RDD[String] &#x3D; lines.flatMap(_.split(&quot; &quot;))
    println(&quot;++++++++++++++++++++++++++++++++++&#x3D;&quot;)
    println(words.dependencies)
    val pairs: RDD[(String, Int)] &#x3D; words.map((_, 1))
    println(&quot;++++++++++++++++++++++++++++++++++&#x3D;&quot;)
    println(pairs.dependencies)
    val word: RDD[(String, Int)] &#x3D; pairs.reduceByKey(_ + _)
    println(&quot;++++++++++++++++++++++++++++++++++&#x3D;&quot;)
    println(word.dependencies)
    println(&quot;++++++++++++++++++++++++++++++++++&#x3D;&quot;)
    word.collect().foreach(println(_))
    sc.close;
  &#125;
&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">++++++++++++++++++++++++++++++++++&#x3D;
List(org.apache.spark.OneToOneDependency@1a2bcd56)
++++++++++++++++++++++++++++++++++&#x3D;
List(org.apache.spark.OneToOneDependency@3c3a0032)
++++++++++++++++++++++++++++++++++&#x3D;
List(org.apache.spark.OneToOneDependency@5e519ad3)
++++++++++++++++++++++++++++++++++&#x3D;
List(org.apache.spark.ShuffleDependency@765d55d5)
++++++++++++++++++++++++++++++++++&#x3D;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>可以看出存在两种依赖关系,一种OneToOneDependency与ShuffleDependency</p>
<p>新的RDD的一个分区的数据依赖于旧的RDD的一个分区的数据,这种依赖称之为OneToOne依赖</p>
<p>新的RDD的一个分区的数据依赖于旧的RDD的多个分区的数据,这种依赖称为Shuffle依赖(数据被打乱重新组合)</p>
<p>源码中的依赖关系</p>
<p><img src="https://i.loli.net/2020/12/23/zWdaBIDnpNuxlr7.png" alt="RDD依赖关系的继承关系"></p>
<p><img src="https://i.loli.net/2020/12/23/w2M9xlshmoVAHL4.png" alt="宽依赖的图"></p>
<h3 id="8-3阶段划分与源码"><a href="#8-3阶段划分与源码" class="headerlink" title="8.3阶段划分与源码"></a>8.3阶段划分与源码</h3><p>Shuffle划分阶段</p>
<p>如果是oneToOne不需要划分阶段</p>
<p>不同的阶段要保证Task执行完毕才能执行下一个阶段</p>
<p>阶段的数量等于shuffle依赖的数量+1</p>
<pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">collect
dagScheduler.runjob
val waiter &#x3D; submitJob&#x2F;&#x2F;DAGScheduler-681
&#x2F;&#x2F;让下翻
override def run(): Unit &#x3D; eventProcessLoop.post(JobSubmitted)&#x2F;&#x2F;DAGScheduler-714

private[scheduler] def handleJobSubmitted&#x2F;&#x2F;DAGScheduler-975
&#123;
    var finalStage: ResultStage &#x3D; null&#x2F;&#x2F;判定finalStage是否存在 985
    finalStage &#x3D; createResultStage(finalRDD, func, partitions, jobId, callSite)&#x2F;&#x2F;如果不存在则创建一个空的ResultStage 986
&#125;
&#x2F;&#x2F;也就是说ResultStage只有一个
private def createResultStage:ResultStage &#x3D; &#123;
    &#x2F;&#x2F;445
    val parents &#x3D; getOrCreateParentStages(rdd, jobId)&#x2F;&#x2F;有没有上一个阶段,这个rdd是当前的reduceBykey最后的rdd
    val stage &#x3D; new ResultStage(id, rdd, func, partitions, parents, jobId, callSite)
    stage
  &#125;

&#x2F;**获得父阶段列表*&#x2F;
private def getOrCreateParentStages(rdd: RDD[_], firstJobId: Int): List[Stage] &#x3D; &#123;&#x2F;&#x2F;466
    getShuffleDependencies(rdd).map &#123; shuffleDep &#x3D;&gt;
      getOrCreateShuffleMapStage(shuffleDep, firstJobId)&#x2F;&#x2F;一个shuffle就会转换为一个阶段
    &#125;.toList
  &#125;

private[scheduler] def getShuffleDependencies(&#x2F;&#x2F;508
      rdd: RDD[_]): HashSet[ShuffleDependency[_, _, _]] &#x3D; &#123;
    val parents &#x3D; new HashSet[ShuffleDependency[_, _, _]]
    val visited &#x3D; new HashSet[RDD[_]]
    val waitingForVisit &#x3D; new ListBuffer[RDD[_]]
    waitingForVisit +&#x3D; rdd&#x2F;&#x2F;放入当前rdd reduceByKey的rdd
    while (waitingForVisit.nonEmpty) &#123;
      val toVisit &#x3D; waitingForVisit.remove(0)
      if (!visited(toVisit)) &#123;&#x2F;&#x2F;判断之前是否访问过
        visited +&#x3D; toVisit
        toVisit.dependencies.foreach &#123;
          case shuffleDep: ShuffleDependency[_, _, _] &#x3D;&gt;
            parents +&#x3D; shuffleDep&#x2F;&#x2F;模式匹配判断是否是shuffle依赖
          case dependency &#x3D;&gt;
            waitingForVisit.prepend(dependency.rdd)
        &#125;
      &#125;
    &#125;
    parents
  &#125;

private def getOrCreateShuffleMapStage( &#x2F;&#x2F;338
    ... ShuffleMapStage &#x3D; &#123;...
        createShuffleMapStage(shuffleDep, firstJobId)
                          &#125;
    
 def createShuffleMapStage[K, V, C](&#x2F;&#x2F;384
      ... ShuffleMapStage &#x3D; &#123;
    val rdd &#x3D; shuffleDep.rdd
          ...
    val numTasks &#x3D; rdd.partitions.length
    val parents &#x3D; getOrCreateParentStages(rdd, jobId)
    val id &#x3D; nextStageId.getAndIncrement()
    val stage &#x3D; new ShuffleMapStage()
      ...
    stage
  &#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="8-4RDD的任务划分"><a href="#8-4RDD的任务划分" class="headerlink" title="8.4RDD的任务划分"></a>8.4RDD的任务划分</h3><p>行动算子底层是runJob</p>
<p><strong>Application</strong>:初始化一个SparkContext即生成一个Application</p>
<p><strong>Job</strong>:一个Action算子就会生成一个Job</p>
<p><strong>Stage</strong>:Stage等于宽依赖(ShuffleDependency)+1</p>
<p><strong>Task</strong>:一个Stage阶段中,最后一个RDD分区个数就是Task的个数</p>
<p>注意:Application-&gt;Job-&gt;Stage-&gt;Task每一层都是一对n的关系</p>
<p>提交过程是一个阶段一个阶段的提交</p>
<pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">private def submitStage(stage: Stage): Unit &#x3D; &#123;&#x2F;&#x2F;1084
    val jobId &#x3D; activeJobForStage(stage)
    if (jobId.isDefined) &#123;
      logDebug(s&quot;submitStage($stage (name&#x3D;$&#123;stage.name&#125;;&quot; +
        s&quot;jobs&#x3D;$&#123;stage.jobIds.toSeq.sorted.mkString(&quot;,&quot;)&#125;))&quot;)
      if (!waitingStages(stage) &amp;&amp; !runningStages(stage) &amp;&amp; !failedStages(stage)) &#123;
        val missing &#x3D; getMissingParentStages(stage).sortBy(_.id)&#x2F;&#x2F;有没有上一级阶段
        logDebug(&quot;missing: &quot; + missing)
        if (missing.isEmpty) &#123;&#x2F;&#x2F;如果没有上一级的stage,则为空
          logInfo(&quot;Submitting &quot; + stage + &quot; (&quot; + stage.rdd + &quot;), which has no missing parents&quot;)
          submitMissingTasks(stage, jobId.get)&#x2F;&#x2F;为空就提交stage&#x2F;tasks
        &#125; else &#123;
          for (parent &lt;- missing) &#123;
            submitStage(parent)
          &#125;
          waitingStages +&#x3D; stage
        &#125;
      &#125;
    &#125; else &#123;
      abortStage(stage, &quot;No active job for stage &quot; + stage.id, None)
    &#125;
  &#125;


val tasks: Seq[Task[_]] &#x3D; try &#123;&#x2F;&#x2F;1217
      val serializedTaskMetrics &#x3D; closureSerializer.serialize(stage.latestInfo.taskMetrics).array()
      stage match &#123;&#x2F;&#x2F;匹配的阶段类型
        case stage: ShuffleMapStage &#x3D;&gt;&#x2F;&#x2F;shuffleMaptask
          &#x2F;&#x2F;new 几个跟map相关,ShuffleMapStage
          stage.pendingPartitions.clear()
          partitionsToCompute.map &#123; id &#x3D;&gt;&#x2F;&#x2F;计算分区
            val locs &#x3D; taskIdToLocations(id)
            val part &#x3D; partitions(id)
            stage.pendingPartitions +&#x3D; id
            new ShuffleMapTask(stage.id, stage.latestInfo.attemptNumber,
              taskBinary, part, locs, properties, serializedTaskMetrics, Option(jobId),
              Option(sc.applicationId), sc.applicationAttemptId, stage.rdd.isBarrier())
          &#125;

        case stage: ResultStage &#x3D;&gt;
          partitionsToCompute.map &#123; id &#x3D;&gt;&#x2F;&#x2F;这里面有多少元素
            val p: Int &#x3D; stage.partitions(id)
            val part &#x3D; partitions(p)
            val locs &#x3D; taskIdToLocations(id)&#x2F;&#x2F;到底有多个new
            new ResultTask(stage.id, stage.latestInfo.attemptNumber,
              taskBinary, part, locs, id, properties, serializedTaskMetrics,
              Option(jobId), Option(sc.applicationId), sc.applicationAttemptId,
              stage.rdd.isBarrier())
          &#125;
      &#125;
&#125;

&#x2F;&#x2F;计算分区
&#x2F;&#x2F; Figure out the indexes of partition ids to compute.
val partitionsToCompute: Seq[Int] &#x3D; stage.findMissingPartitions()&#123;&#125;
    
&#x2F;&#x2F;ResultStage 61
override def findMissingPartitions(): Seq[Int] &#x3D; &#123;
    val job &#x3D; activeJob.get
    (0 until job.numPartitions).filter(id &#x3D;&gt; !job.finished(id))
&#125;&#x2F;&#x2F;此处的job.numPartitions就是最后一个RDD的分区
&#x2F;&#x2F;三个分区就是0-3
&#x2F;&#x2F;一个RDD的三个分区,从并行角度就会分配为3个Task

&#x2F;&#x2F;SuffleMapStage 91
override def findMissingPartitions(): Seq[Int] &#x3D; &#123;
    mapOutputTrackerMaster
      .findMissingPartitions(shuffleDep.shuffleId)
      .getOrElse(0 until numPartitions)
  &#125;
&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>一个应用程序会对应多个job(一个行动算子算是一个job)</p>
<p>ShuffleMapStage=&gt;ShuffleMapTask</p>
<p>ResultStage=&gt;ResultTask</p>
<h2 id="9-RDD分区器"><a href="#9-RDD分区器" class="headerlink" title="9.RDD分区器"></a>9.RDD分区器</h2><ol>
<li>Hash分区(默认)</li>
<li>Range分区</li>
<li>自定义分区</li>
</ol>
<p>只有Key-Value类型的RDD才有分区器，非Key-Value类型的RDD分区的值是None</p>
<p>每个RDD的分区ID范围：0 ~ (numPartitions - 1)，决定这个值是属于那个分区的。</p>
<p><strong>Hash</strong>分区：对于给定的key，计算其hashCode,并除以分区个数取余</p>
<p><strong>Range分区</strong>：将一定范围内的数据映射到一个分区中，尽量保证每个分区数据均匀，而且分区间有序</p>
<h3 id="1-自定义分区器"><a href="#1-自定义分区器" class="headerlink" title="1.自定义分区器"></a>1.自定义分区器</h3><h4 id="a-gt-HashPartitioner"><a href="#a-gt-HashPartitioner" class="headerlink" title="a&gt;HashPartitioner"></a>a&gt;HashPartitioner</h4><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">object PartitionBy &#123;
  def main(args: Array[String]): Unit &#x3D; &#123;
    val sparkConf: SparkConf &#x3D; new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;Main&quot;)
    val sc: SparkContext &#x3D; new SparkContext(sparkConf)
    val rdd1: RDD[(String, Int)] &#x3D; sc.textFile(&quot;Day06&#x2F;input&#x2F;number&quot;).map((_, 1))
    &#x2F;&#x2F;只有k-v值才有分区器
    rdd1.saveAsTextFile(&quot;.&#x2F;output&quot;)
    &#x2F;&#x2F;使用hash分区器
    val rdd2: RDD[(String, Int)] &#x3D; rdd1.partitionBy(new HashPartitioner(3))
    rdd2.saveAsTextFile(&quot;.&#x2F;output2&quot;)
  &#125;
&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="b-gt-RangePartitioner"><a href="#b-gt-RangePartitioner" class="headerlink" title="b&gt;RangePartitioner"></a>b&gt;RangePartitioner</h4><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">object PartitionBy &#123;
  def main(args: Array[String]): Unit &#x3D; &#123;
    val sparkConf: SparkConf &#x3D; new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;Main&quot;)
    val sc: SparkContext &#x3D; new SparkContext(sparkConf)
    val rdd1&#x3D; sc.textFile(&quot;Day06&#x2F;input&#x2F;number&quot;).map((_, 1))
    &#x2F;&#x2F;只有k-v值才有分区器
    rdd1.saveAsTextFile(&quot;.&#x2F;output&quot;)
    &#x2F;&#x2F;使用rangePartitioner
    val value &#x3D; new RangePartitioner[String, Int](2, rdd1.sample(false, 0.5))
    &#x2F;&#x2F;range分区器的使用,要定义泛型,传递分区,传递sample
    &#x2F;&#x2F;首先要传递一个分区,传递一个
    rdd1.partitionBy(value)
    rdd1.saveAsTextFile(&quot;.&#x2F;output2&quot;)
  &#125;
&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="c-gt-自定义分区"><a href="#c-gt-自定义分区" class="headerlink" title="c&gt;自定义分区"></a>c&gt;自定义分区</h3><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">&#x2F;&#x2F;自定义分区器
case class MyPartitioner(numPartition:Int) extends Partitioner &#123;
    override def numPartitions: Int &#x3D; numPartition
    override def getPartition(key: Any): Int &#x3D; (math.random() * numPartition).toInt
&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">&#x2F;&#x2F;自定义分区器的使用
object PartitionBy &#123;
  def main(args: Array[String]): Unit &#x3D; &#123;
    val sparkConf: SparkConf &#x3D; new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;Main&quot;)
    val sc: SparkContext &#x3D; new SparkContext(sparkConf)
    val rdd1&#x3D; sc.textFile(&quot;Day06&#x2F;input&#x2F;number&quot;).map((_, 1))
    &#x2F;&#x2F;只有k-v值才有分区器
    rdd1.saveAsTextFile(&quot;.&#x2F;output&quot;)
    &#x2F;&#x2F;使用rangePartitioner
    val value &#x3D; new RangePartitioner[String, Int](2, rdd1.sample(false, 0.5))
    &#x2F;&#x2F;range分区器的使用,要定义泛型,传递分区,传递sample
    &#x2F;&#x2F;首先要传递一个分区,传递一个
    val value1: RDD[(String, Int)] &#x3D; rdd1.partitionBy(MyPartitioner(2))
    value1.saveAsTextFile(&quot;.&#x2F;output2&quot;)
  &#125;
&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>


<h2 id="10-RDD累加器"><a href="#10-RDD累加器" class="headerlink" title="10.RDD累加器"></a>10.RDD累加器</h2><p><strong>系统累加器</strong></p>
<pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">object MyAccumulator &#123;
  def main(args: Array[String]): Unit &#x3D; &#123;
    val sc &#x3D; new SparkContext(new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;myAccumulator&quot;))
    val rdd: RDD[Int] &#x3D; sc.makeRDD(List(1, 2, 3, 4, 5))
    &#x2F;&#x2F;声明系统累加器
    val sum: LongAccumulator &#x3D; sc.longAccumulator(&quot;sum&quot;)
    rdd.foreach(
      num&#x3D;&gt;&#123;
        sum.add(num)
      &#125;
    )
    &#x2F;&#x2F;获取累加器
    println(sum.value)
  &#125;
&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="自定义累加器"><a href="#自定义累加器" class="headerlink" title="自定义累加器"></a><strong>自定义累加器</strong></h3><p><strong>1.自定wordcount累加器</strong></p>
<pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">&#x2F;&#x2F;自定义累加器实现wordcount
class DefineAccumulator extends AccumulatorV2[String,mutable.Map[String,Int]]&#123;
  val map: mutable.Map[String, Int] &#x3D; mutable.Map()
  &#x2F;&#x2F;判断累加器是否为初始状态
  override def isZero: Boolean &#x3D; &#123;map.isEmpty&#125;
  &#x2F;&#x2F;复制累加器
  override def copy(): AccumulatorV2[String, mutable.Map[String, Int]] &#x3D; &#123;new DefineAccumulator()&#125;
  &#x2F;&#x2F;重置累加器
  override def reset(): Unit &#x3D; map.clear()
  &#x2F;&#x2F;区内相加
  override def add(v: String): Unit &#x3D; &#123;
    &#x2F;&#x2F;区内相加的定义,如果存在元素,就对key值+1,如果不存在,就添加当前元素,key+1
    map(v)&#x3D;map.getOrElse(v,0)+1
  &#125;
  &#x2F;&#x2F;区间相加
  override def merge(other: AccumulatorV2[String, mutable.Map[String, Int]]): Unit &#x3D; &#123;
    &#x2F;&#x2F;区间相加,固定
    val map1: mutable.Map[String, Int] &#x3D; this.value
    val map2: mutable.Map[String, Int] &#x3D; other.value
    map2.foreach&#123;
      case (k,v) &#x3D;&gt; map1(k)&#x3D;map1.getOrElse(k,0)+v
    &#125;
  &#125;
  override def value: mutable.Map[String, Int] &#x3D; map
&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><strong>2.注册并使用定义累加器</strong></p>
<pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">object MyAccumulator2 &#123;
  def main(args: Array[String]): Unit &#x3D; &#123;
    val sc &#x3D; new SparkContext(new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;accumulator&quot;))
    val word: RDD[String] &#x3D; sc.textFile(&quot;.&#x2F;word.txt&quot;)
    val words &#x3D; word.flatMap(_.split(&quot; &quot;))
    &#x2F;&#x2F;new出累加器
    val uacc &#x3D; new DefineAccumulator
    &#x2F;&#x2F;注册累加器
    sc.register(uacc)
    &#x2F;&#x2F;使用累加器
    words.foreach(uacc.add(_))
    println(uacc.value)&#x2F;&#x2F;注意输出为accumulator的值
  &#125;
&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="11-广播变量"><a href="#11-广播变量" class="headerlink" title="11.广播变量"></a>11.广播变量</h2><p>广播变量用来高效分发较大的对象。向所有工作节点发送一个较大的只读值，以供一个或多个Spark操作使用。比如，如果你的应用需要向所有节点发送一个较大的只读查询表，广播变量用起来都很顺手。在多个并行操作中使用同一个变量，但是 Spark会为每个任务分别发送。</p>
<p>在整个队列中,仅仅存在一次</p>
<pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">object BoardCast &#123;
  def main(args: Array[String]): Unit &#x3D; &#123;
    val sc &#x3D; new SparkContext(new SparkConf().setAppName(&quot;emm&quot;).setMaster(&quot;local[*]&quot;))
    val rdd1 &#x3D; sc.makeRDD(List( (&quot;a&quot;,1), (&quot;b&quot;, 2), (&quot;c&quot;, 3), (&quot;d&quot;, 4) ),4)
    val list &#x3D; List((&quot;a&quot;,4), (&quot;b&quot;, 5), (&quot;c&quot;, 6), (&quot;d&quot;, 7))
    val broadcast: Broadcast[List[(String, Int)]] &#x3D; sc.broadcast(list)
    val value &#x3D; rdd1.map &#123;
      case (key, num) &#x3D;&gt; &#123;
        var num1 &#x3D; 0
        for ((k, v) &lt;- broadcast.value) &#123;
          if (k &#x3D;&#x3D; key)&#123;
            num1&#x3D;v
          &#125;
        &#125;
        (key, num+num1)
      &#125;
    &#125;
    value.collect().foreach(println)
  &#125;
&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="12-RDD的持久化"><a href="#12-RDD的持久化" class="headerlink" title="12.RDD的持久化"></a>12.RDD的持久化</h2><h3 id="12-1为什么要使用RDD的持久化"><a href="#12-1为什么要使用RDD的持久化" class="headerlink" title="12.1为什么要使用RDD的持久化"></a>12.1为什么要使用RDD的持久化</h3><p>数据不存储在RDD中</p>
<p><img src="https://i.loli.net/2020/12/25/m4gxMj5cHAh8KaG.png" alt="RDD的重用"></p>
<p>如果一个RDD需要重复使用,需要从头再次执行来获取数据</p>
<p>RDD的对象可以重用,但是数据没法重用</p>
<pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">object SparkCore02_RDD_Persist &#123;

  def main(args: Array[String]): Unit &#x3D; &#123;
    val sparkConf: SparkConf &#x3D; new SparkConf().setAppName(&quot;WordCountPersist&quot;).setMaster(&quot;local[1]&quot;)
    val sc &#x3D; new SparkContext(sparkConf)

    val list &#x3D; List(&quot;Hello Spark&quot;, &quot;Hello Scala&quot;)

    &#x2F;&#x2F;生成RDD RDD中不存储数据
    val listRDD: RDD[String] &#x3D; sc.makeRDD(list, 1)
    val wordRDD: RDD[String] &#x3D; listRDD.flatMap(word &#x3D;&gt; word.split(&quot; &quot;))
    val tupleRDD: RDD[(String, Int)] &#x3D; wordRDD.map(word &#x3D;&gt;
    &#123; println(&quot;map阶段&quot;)
      (word, 1)
    &#125;)

    &#x2F;&#x2F;分组的操作
    val groupRDD: RDD[(String, Iterable[Int])] &#x3D; tupleRDD.groupByKey()
    val resultRDD: RDD[(String, Int)] &#x3D; tupleRDD.reduceByKey(_ + _)

    resultRDD.collect().foreach(println)
    println(&quot;------------华丽分割线----------------&quot;)
    groupRDD.collect().foreach(println)
    sc.stop()
  &#125;
&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">&#x2F;&#x2F;结果
map阶段
map阶段
map阶段
map阶段
(Spark,1)
(Hello,2)
(Scala,1)
------------华丽分割线----------------
map阶段
map阶段
map阶段
map阶段
(Spark,CompactBuffer(1))
(Hello,CompactBuffer(1, 1))
(Scala,CompactBuffer(1))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>发现map阶段运行了两波,所有的执行都会从头开始计算</p>
<p>这样的执行影响了效率</p>
<p>要想解决这个问题,数据持久化提高效率</p>
<p><img src="https://i.loli.net/2020/12/25/MAwm1jeOrELZWqK.png" alt="RDD持久化的作用"></p>
<pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">&#x2F;&#x2F;进行缓存
tupleRDD.cache() &#x2F;&#x2F;本质是persist

&#x2F;&#x2F;tupleRDD.cache()
tupleRDD.persist(StorageLevel.MEMORY_AND_DISK)
&#x2F;&#x2F;memory_only当内存不够的情况下,数据不能溢写到磁盘,会丢失数据
&#x2F;&#x2F;memory_and_disk当内存不够的情况下,会将数据落到磁盘
&#x2F;&#x2F;持久化操作,必须在行动算子执行时,完成的
sc.setCheckpointDir(&quot;.&#x2F;checkPoint&quot;)
&#x2F;&#x2F;一般要保存到分布式存储中
tupleRDD.checkpoint()
&#x2F;&#x2F;检查点路径,在作业执行完毕之后也是不会删除的<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>RDD对象的持久化操作不一定为了重用,在数据执行较长,或者数据比较重要的场合也可以进行持久化操作</p>
<h3 id="12-2-三种持久化方法"><a href="#12-2-三种持久化方法" class="headerlink" title="12.2 三种持久化方法"></a>12.2 三种持久化方法</h3><ol>
<li><p><strong>cache</strong>:将数据临时存储在内存中进行数据重用,会添加新的依赖,出现问题从头开始计算</p>
</li>
<li><p><strong>persist</strong>:将数据临时存储在磁盘文件中进行数据重用,涉及到磁盘IO,性能较低,但是数据安全,如果作业执行完毕,临时保存在数据文件就会丢失</p>
</li>
<li><p><strong>checkpoint</strong>:将磁盘长久地保存在磁盘文件中进行数据重用,涉及到磁盘IO时,性能较低,但是会切断血缘关系,相当于改变数据源</p>
<blockquote>
<p>但是数据安全,为了保证数据安全,所以一般情况下,会独立的执行作业,为了能够提高效率,一般情况下,会跟cache联合使用,先cache在使用checkpoint这个时候会保存cache的文件,而不会独立的跑一个单独的任务</p>
</blockquote>
</li>
</ol>
<pre class="mermaid">graph LR
sc-->map-reduceByKey-->cache--保存-->CheckPoint
cache-->collect</pre>

<p><strong>大区别</strong></p>
<p>cache会添加新的依赖</p>
<pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">package com.ecust.rdd.persist

import org.apache.spark.rdd.RDD
import org.apache.spark.storage.StorageLevel
import org.apache.spark.&#123;SparkConf, SparkContext&#125;

&#x2F;**
 * @author Jinxin Li
 * @create 2020-12-24 14:03
 *&#x2F;
object SparkCore03_RDD_CheckPoint &#123;

  def main(args: Array[String]): Unit &#x3D; &#123;
    val sparkConf: SparkConf &#x3D; new SparkConf().setAppName(&quot;WordCountPersist&quot;).setMaster(&quot;local[1]&quot;)
    val sc &#x3D; new SparkContext(sparkConf)

    val list &#x3D; List(&quot;Hello Spark&quot;, &quot;Hello Scala&quot;)

    val listRDD: RDD[String] &#x3D; sc.makeRDD(list, 1)
    val wordRDD: RDD[String] &#x3D; listRDD.flatMap(word &#x3D;&gt; word.split(&quot; &quot;))
    val tupleRDD: RDD[(String, Int)] &#x3D; wordRDD.map(word &#x3D;&gt; (word, 1))
    tupleRDD.cache()
    println(tupleRDD.toDebugString)&#x2F;&#x2F;运行之前看血缘关系

&#x2F;&#x2F;    tupleRDD.persist(StorageLevel.MEMORY_AND_DISK)

&#x2F;&#x2F;    sc.setCheckpointDir(&quot;.&#x2F;checkPoint&quot;)
&#x2F;&#x2F;    tupleRDD.checkpoint()

    val groupRDD: RDD[(String, Iterable[Int])] &#x3D; tupleRDD.groupByKey()
    groupRDD.collect().foreach(println)
    println(&quot;----------------------------&quot;)
    println(tupleRDD.toDebugString)&#x2F;&#x2F;运行之后看血缘
    sc.stop()
  &#125;
&#125;
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">(1) MapPartitionsRDD[2] at map at SparkCore03_RDD_CheckPoint.scala:21 [Memory Deserialized 1x Replicated]
 |  MapPartitionsRDD[1] at flatMap at SparkCore03_RDD_CheckPoint.scala:20 [Memory Deserialized 1x Replicated]
 |  ParallelCollectionRDD[0] at makeRDD at SparkCore03_RDD_CheckPoint.scala:19 [Memory Deserialized 1x Replicated]
(Spark,CompactBuffer(1))
(Hello,CompactBuffer(1, 1))
(Scala,CompactBuffer(1))
----------------------------
(1) MapPartitionsRDD[2] at map at SparkCore03_RDD_CheckPoint.scala:21 [Memory Deserialized 1x Replicated]
 |       CachedPartitions: 1; MemorySize: 368.0 B; ExternalBlockStoreSize: 0.0 B; DiskSize: 0.0 B&#x2F;&#x2F;这里添加了新的依赖
 |  MapPartitionsRDD[1] at flatMap at SparkCore03_RDD_CheckPoint.scala:20 [Memory Deserialized 1x Replicated]
 |  ParallelCollectionRDD[0] at makeRDD at SparkCore03_RDD_CheckPoint.scala:19 [Memory Deserialized 1x Replicated]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>因此cache会在血缘关系中添加新的依赖,一旦出现问题,可以重头读取数据</p>
<pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">&#x2F;&#x2F;使用checkPoint之后
(1) MapPartitionsRDD[2] at map at SparkCore03_RDD_CheckPoint.scala:21 []
 |  MapPartitionsRDD[1] at flatMap at SparkCore03_RDD_CheckPoint.scala:20 []
 |  ParallelCollectionRDD[0] at makeRDD at SparkCore03_RDD_CheckPoint.scala:19 []
(Spark,CompactBuffer(1))
(Hello,CompactBuffer(1, 1))
(Scala,CompactBuffer(1))
----------------------------
(1) MapPartitionsRDD[2] at map at SparkCore03_RDD_CheckPoint.scala:21 []
 |  ReliableCheckpointRDD[4] at collect at SparkCore03_RDD_CheckPoint.scala:31 []<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>使用checkPoint会切断血缘关系,重新建立新的血缘关系等同于改变数据源</p>
<h3 id="12-3源码解析CheckPoint单独执行任务"><a href="#12-3源码解析CheckPoint单独执行任务" class="headerlink" title="12.3源码解析CheckPoint单独执行任务"></a>12.3源码解析CheckPoint单独执行任务</h3><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">&#x2F;&#x2F;SparkContext.scala 2093-2095
dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get)
    progressBar.foreach(_.finishAll())
    rdd.doCheckpoint()&#x2F;&#x2F;--&gt;
&#x2F;&#x2F;runJob之后调用了doCheckPoint()方法

&#x2F;&#x2F;-----------------------------
&#x2F;&#x2F;RDD.scala 1789-1805 doCheckPoint
if (checkpointData.isDefined) &#123;
    if (checkpointAllMarkedAncestors) &#123;
        dependencies.foreach(_.rdd.doCheckpoint())
          &#125;
    checkpointData.get.checkpoint()&#x2F;&#x2F;如果需要checkPoint然后进行checkPoint
&#125; else &#123;
    dependencies.foreach(_.rdd.doCheckpoint())
&#125;

&#x2F;&#x2F;----------------------------------\
&#x2F;&#x2F;org.apache.spark.rdd.LocalRDDCheckpointData 53-54
if (missingPartitionIndices.nonEmpty) &#123;
      rdd.sparkContext.runJob(rdd, action, missingPartitionIndices)
&#125;&#x2F;&#x2F;单独执行任务<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="12-4使用CheckPoint恢复计算"><a href="#12-4使用CheckPoint恢复计算" class="headerlink" title="12.4使用CheckPoint恢复计算"></a>12.4使用CheckPoint恢复计算</h3><p>checkpoint会将结果写到hdfs上，当driver 关闭后数据不会被清除。所以可以在其他driver上重复利用该checkpoint的数据。</p>
<p>checkpoint write data:</p>
<pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">sc.setCheckpointDir(&quot;data&#x2F;checkpoint&quot;)
val rddt &#x3D; sc.parallelize(Array((1,2),(3,4),(5,6)),2)
rddt.checkpoint()
rddt.count() &#x2F;&#x2F;要action才能触发checkpoint<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<p>read from checkpoint data:</p>
<pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">package org.apache.spark

import org.apache.spark.rdd.RDD

object RDDUtilsInSpark &#123;
  def getCheckpointRDD[T](sc:SparkContext, path:String) &#x3D; &#123;
  	&#x2F;&#x2F;path要到part-000000的父目录
    val result : RDD[Any] &#x3D; sc.checkpointFile(path)
    result.asInstanceOf[T]
  &#125;
&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><em>note:因为sc.checkpointFile(path)是private[spark]的，所以该类要写在自己工程里新建的package org.apache.spark中</em></p>
<p>example:</p>
<pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">val rdd : RDD[(Int, Int)]&#x3D; RDDUtilsInSpark.getCheckpointRDD(sc, &quot;data&#x2F;checkpoint&#x2F;963afe46-eb23-430f-8eae-8a6c5a1e41ba&#x2F;rdd-0&quot;)
   println(rdd.count())
   rdd.collect().foreach(println)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<p>这样就可以原样复原了。</p>
<p><strong>Demo</strong></p>
<pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">object SparkCore05_RDD_CheckPointUse &#123;

  def main(args: Array[String]): Unit &#x3D; &#123;
    val sparkConf: SparkConf &#x3D; new SparkConf().setAppName(&quot;WordCountPersist&quot;).setMaster(&quot;local[1]&quot;)
    val sc &#x3D; new SparkContext(sparkConf)
	&#x2F;&#x2F;使用工具类,注意工具类的包,要自己建立,注意泛型
    val rdd: RDD[(String, Int)] &#x3D; RDDUtilsInSpark.getCheckpointRDD[RDD[(String, Int)]](sc, &quot;.&#x2F;checkPoint&#x2F;1186c961-ddb4-4af5-b7dc-6cc99776490b&#x2F;rdd-2&quot;)
      &#x2F;&#x2F;之前的map之后reduceBykey之前的checkPoint文件

    val result: RDD[(String, Int)] &#x3D; rdd.reduceByKey(_ + _)

    result.collect().foreach(println)
    sc.stop()
  &#125;
&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">import org.apache.spark.rdd.RDD

&#x2F;&#x2F;可以恢复checkPoint的工具类,注意放置的包
object RDDUtilsInSpark &#123;
  def getCheckpointRDD[T](sc: SparkContext, path: String) &#x3D; &#123;
    &#x2F;&#x2F;path要到part-000000的父目录
    val result: RDD[Any] &#x3D; sc.checkpointFile(path)
    result.asInstanceOf[T]
  &#125;
&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="13-RDD自定义分区器"><a href="#13-RDD自定义分区器" class="headerlink" title="13.RDD自定义分区器"></a>13.RDD自定义分区器</h2><p>Spark目前支持Hash分区、Range分区和用户自定义分区。Hash分区为当前的默认分区。分区器直接决定了RDD中分区的个数、RDD中每条数据经过Shuffle后进入哪个分区和Reduce的个数。</p>
<p>1）注意：</p>
<p>（1）只有Key-Value类型的RDD才有分区器，非Key-Value类型的RDD分区的值是None</p>
<p>（2）每个RDD的分区ID范围：0~numPartitions-1，决定这个值是属于那个分区的。</p>
<pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">package com.ecust.rdd.partition

import org.apache.spark.rdd.RDD
import org.apache.spark.&#123;HashPartitioner, Partitioner, SparkConf, SparkContext&#125;

&#x2F;**
 * @author Jinxin Li
 * @create 2020-12-26 10:52
 * 自定义分区规则
 *&#x2F;
object SparkCore01_RDD_Partitioner &#123;
  def main(args: Array[String]): Unit &#x3D; &#123;
    val sparkConf: SparkConf &#x3D; new SparkConf().setMaster(&quot;local&quot;).setAppName(&quot;Partitioner&quot;)
    val sc &#x3D; new SparkContext(sparkConf)

    val rdd: RDD[(String, String)] &#x3D; sc.makeRDD(List(
      (&quot;nba&quot;, &quot;xxxxxxxxxxxxxxx&quot;),
      (&quot;wba&quot;, &quot;aaaaaaaaaaaaaa&quot;),
      (&quot;cba&quot;, &quot;dddddddddddd&quot;),
      (&quot;wcba&quot;, &quot;ppppppppppppppppppppppp&quot;)
    ), 3)

    &#x2F;*
    自动义分区器,决定数据去哪个分区
     *&#x2F;
    val rddPar: RDD[(String, String)] &#x3D; rdd.partitionBy(new MyPartitioner())

    rddPar.saveAsTextFile(&quot;.&#x2F;par&quot;)

    sc.stop()
  &#125;

  class MyPartitioner extends Partitioner&#123;
    &#x2F;&#x2F;分区数量
    override def numPartitions: Int &#x3D; 3

    &#x2F;&#x2F;返回Int类型,返回数据的分区索引 从零开始
    &#x2F;&#x2F;Key表示数据的KV到底是什么
    &#x2F;&#x2F;根据数据的key值返回数据所在分区索引
    override def getPartition(key: Any): Int &#x3D; &#123;
      key match &#123;
        case &quot;nba&quot; &#x3D;&gt; 0
        case &quot;cba&quot; &#x3D;&gt; 1
        case _ &#x3D;&gt; 2
      &#125;

      &#x2F;*if (key &#x3D;&#x3D; &quot;nba&quot;)&#123;
        0
      &#125; else if(key &#x3D;&#x3D; &quot;cba&quot;)&#123;
        1
      &#125;else&#123;
        2
      &#125;*&#x2F;
    &#125;
  &#125;

&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="14-RDD的存储与保存"><a href="#14-RDD的存储与保存" class="headerlink" title="14.RDD的存储与保存"></a>14.RDD的存储与保存</h2><p>Spark的数据读取及数据保存可以从两个维度来作区分：文件格式以及文件系统。</p>
<p>文件格式分为：Text文件、Json文件、Csv文件、Sequence文件以及Object文件；</p>
<p>文件系统分为：本地文件系统、HDFS以及数据库。</p>
<pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">&#x2F;&#x2F;集群文件系统存储示例
hdfs:&#x2F;&#x2F;hadoop102:8020&#x2F;input&#x2F;1.txt<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<h2 id="15-RDD的累加器"><a href="#15-RDD的累加器" class="headerlink" title="15.RDD的累加器"></a>15.RDD的累加器</h2><p>如果没有累加器,我们计算时只能使用reduce,要想把executor的变量拉回到Driver困难</p>
<p><img src="https://i.loli.net/2020/12/26/b24Xm8BYltu1PF3.png" alt="引入问题"></p>
<pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">&#x2F;&#x2F;学前测试

object SparkCore02_RDD_accumulator &#123;
  def main(args: Array[String]): Unit &#x3D; &#123;
    val sparkConf: SparkConf &#x3D; new SparkConf().setMaster(&quot;local&quot;).setAppName(&quot;Partitioner&quot;)
    val sc &#x3D; new SparkContext(sparkConf)

    val rdd: RDD[Int] &#x3D; sc.makeRDD(List(1, 2, 3, 4),1)

    var sum:Int &#x3D; 0
    &#x2F;&#x2F;行动算子返回非RDD
    rdd.foreach(num&#x3D;&gt;&#123;
      sum+&#x3D;num
      println(&quot;executor:&quot;+sum)
    &#125;)

    println(&quot;driver:&quot;+sum)&#x2F;&#x2F;打印结果为零,Driver-&gt;executor,结果返回不了
    sc.stop()

  &#125;
&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><img src="https://i.loli.net/2020/12/26/9PtGzVBQ8dD3sya.png" alt="累加器的主要目的"></p>
<p>累加器：分布式共享只写变量。（Executor和Executor之间不能读数据）</p>
<p>累加器用来把Executor端变量信息聚合到Driver端。在Driver程序中定义的变量，在Executor端的每个task都会得到这个变量的一份新的副本，每个task更新这些副本的值后，传回Driver端进行merge。</p>
<p><strong>Long累加器Demo</strong></p>
<pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">object SparkCore03_RDD_accumulator &#123;
  def main(args: Array[String]): Unit &#x3D; &#123;
    val sparkConf: SparkConf &#x3D; new SparkConf().setMaster(&quot;local&quot;).setAppName(&quot;Partitioner&quot;)
    val sc &#x3D; new SparkContext(sparkConf)

    val rdd: RDD[Int] &#x3D; sc.makeRDD(List(1, 2, 3, 4),1)

    &#x2F;&#x2F;todo 自定义累加器
    val sum: LongAccumulator &#x3D; sc.longAccumulator(&quot;sum&quot;)
    &#x2F;&#x2F;系统自带了一些累加器
&#x2F;&#x2F;    sc.doubleAccumulator
&#x2F;&#x2F;    sc.collectionAccumulator()

    rdd.foreach(num&#x3D;&gt;sum.add(num))

    println(&quot;driver:&quot;+sum.value)
    sc.stop()
  &#125;
&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><strong>特殊情况</strong></p>
<p>少加:转换算子中调用累加器,如果没有行动算子的话,name不会执行</p>
<p>多加:转换算子中调用累加器,多次行动算子会调用多次,一般会放在行动算子中进行操作</p>
<pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">object SparkCore04_RDD_accumulator &#123;
  def main(args: Array[String]): Unit &#x3D; &#123;
    val sparkConf: SparkConf &#x3D; new SparkConf().setMaster(&quot;local&quot;).setAppName(&quot;Partitioner&quot;)
    val sc &#x3D; new SparkContext(sparkConf)

    val rdd: RDD[Int] &#x3D; sc.makeRDD(List(1, 2, 3, 4),1)

    &#x2F;&#x2F;todo 自定义累加器
    val sum: LongAccumulator &#x3D; sc.longAccumulator(&quot;sum&quot;)
    &#x2F;&#x2F;系统自带了一些累加器
&#x2F;&#x2F;    sc.doubleAccumulator
&#x2F;&#x2F;    sc.collectionAccumulator()

    val result: RDD[Unit] &#x3D; rdd.map(num &#x3D;&gt; sum.add(num))
    
    result.collect()
    result.collect()&#x2F;&#x2F;两个行动算子会多加

    println(&quot;driver:&quot;+sum.value)
    sc.stop()
  &#125;
&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="16-RDD的自定义广播变量"><a href="#16-RDD的自定义广播变量" class="headerlink" title="16.RDD的自定义广播变量"></a>16.RDD的自定义广播变量</h2><p>分布式共享只写变量</p>
<p>表示累加器的值互相之间是没法访问的,自己能读自己,只有Driver进行读到,然后在Driver端进行合并</p>
<p>我们可以将一些Shuffle的东西使用累加器来实现(==优化==)</p>
<p>比方:需要shuffle的方法就不要shuffle了</p>
<p>闭包数据,都是以Task为单位发送的,每个人物中包含的闭包数据这样可能会导致,一个Executor中含有大量的重复的数据,并且占用大量的内存</p>
<p>Executor本质其实就是JVM,所以在启动时,会自动分配内存</p>
<p> 完全可以将任务中的闭包数据放置到Executor的内存中,达到共享的目的</p>
<p>Spark中的广播变量可以将闭包的数据保存在Executor的内存中</p>
<p>分布式共享只读变量</p>
<pre class="mermaid">graph TD
map-->Executor/task1
map-->Executor/task2
map-->Executor/task3</pre>

<pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">object SparkCore04_RDD_BroadCast &#123;
  def main(args: Array[String]): Unit &#x3D; &#123;

    val sparkConf: SparkConf &#x3D; new SparkConf().setMaster(&quot;local&quot;).setAppName(&quot;acc&quot;)
    val sc &#x3D; new SparkContext(sparkConf)

    val rdd1: RDD[(String, Int)] &#x3D; sc.makeRDD(List((&quot;a&quot;, 1), (&quot;b&quot;, 2), (&quot;c&quot;, 3)))

    val map: mutable.Map[String, Int] &#x3D; mutable.Map((&quot;a&quot;, 4), (&quot;b&quot;, 5), (&quot;c&quot;, 6))

    &#x2F;&#x2F;定义广播变量
    val value: Broadcast[mutable.Map[String, Int]] &#x3D; sc.broadcast(map)

    &#x2F;&#x2F;每个task都有一份数据
    val result: RDD[(String, (Int, Int))] &#x3D; rdd1.map &#123; case (w, c) &#x3D;&gt; &#123;
      val i: Int &#x3D; value.value.getOrElse(w, 0)
      (w, (c, i))
    &#125;&#125;
    result.collect().foreach(println)
    sc.stop()
  &#125;
&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/about" rel="external nofollow noreferrer">Jinxin Li</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="http://fourgold.github.io/2021/01/03/Spark_SparkCore/">http://fourgold.github.io/2021/01/03/Spark_SparkCore/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/about" target="_blank">Jinxin Li</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/spark/">
                                    <span class="chip bg-color">spark</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <style>
    #reward {
        margin: 40px 0;
        text-align: center;
    }

    #reward .reward-link {
        font-size: 1.4rem;
        line-height: 38px;
    }

    #reward .btn-floating:hover {
        box-shadow: 0 6px 12px rgba(0, 0, 0, 0.2), 0 5px 15px rgba(0, 0, 0, 0.2);
    }

    #rewardModal {
        width: 320px;
        height: 350px;
    }

    #rewardModal .reward-title {
        margin: 15px auto;
        padding-bottom: 5px;
    }

    #rewardModal .modal-content {
        padding: 10px;
    }

    #rewardModal .close {
        position: absolute;
        right: 15px;
        top: 15px;
        color: rgba(0, 0, 0, 0.5);
        font-size: 1.3rem;
        line-height: 20px;
        cursor: pointer;
    }

    #rewardModal .close:hover {
        color: #ef5350;
        transform: scale(1.3);
        -moz-transform:scale(1.3);
        -webkit-transform:scale(1.3);
        -o-transform:scale(1.3);
    }

    #rewardModal .reward-tabs {
        margin: 0 auto;
        width: 210px;
    }

    .reward-tabs .tabs {
        height: 38px;
        margin: 10px auto;
        padding-left: 0;
    }

    .reward-content ul {
        padding-left: 0 !important;
    }

    .reward-tabs .tabs .tab {
        height: 38px;
        line-height: 38px;
    }

    .reward-tabs .tab a {
        color: #fff;
        background-color: #ccc;
    }

    .reward-tabs .tab a:hover {
        background-color: #ccc;
        color: #fff;
    }

    .reward-tabs .wechat-tab .active {
        color: #fff !important;
        background-color: #22AB38 !important;
    }

    .reward-tabs .alipay-tab .active {
        color: #fff !important;
        background-color: #019FE8 !important;
    }

    .reward-tabs .reward-img {
        width: 210px;
        height: 210px;
    }
</style>

<div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">你的赏识是我前进的动力</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="/medias/reward/alipay.jpg" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                    <div id="wechat">
                        <img src="/medias/reward/wechat.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>

            
        </div>
    </div>

    

    

    

    
    <div class="livere-card card" data-aos="fade-up">
    <!-- 来必力City版安装代码 -->
    <div id="lv-container" class="card-content" data-id="city" data-uid="">
        <script type="text/javascript">
            (function (d, s) {
                let j, e = d.getElementsByTagName(s)[0];
                if (typeof LivereTower === 'function') {
                    return;
                }

                j = d.createElement(s);
                j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
                j.async = true;

                e.parentNode.insertBefore(j, e);
            })(document, 'script');
        </script>
        <noscript>为正常使用来必力评论功能请激活JavaScript。</noscript>
    </div>
    <!-- City版安装代码已完成 -->
</div>
    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/2021/01/04/Spark_SparkStreaming/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/7.jpg" class="responsive-img" alt="Spark_SparkStreaming">
                        
                        <span class="card-title">Spark_SparkStreaming</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2021-01-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/spark/" class="post-category">
                                    spark
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/sparkstreaming/">
                        <span class="chip bg-color">sparkstreaming</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2021/01/03/Spark_SparkSQL/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/11.jpg" class="responsive-img" alt="Spark_SparkSQL">
                        
                        <span class="card-title">Spark_SparkSQL</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2021-01-03
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/spark/" class="post-category">
                                    spark
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/sparkSQL/">
                        <span class="chip bg-color">sparkSQL</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>

<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>


    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('1'),
            headingSelector: 'h1,h2, h3, h4'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h1,h2, h3, h4').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/libs/aplayer/APlayer.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js"></script>

    
    <div class="container row center-align" style="margin-bottom: 0px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2019-2021</span>
            
            <span id="year">2019</span>
            <a href="/about" target="_blank">Jinxin Li</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            <br>
            
            &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                class="white-color">205k</span>&nbsp;字
            
            
            
            
            
            
            <span id="busuanzi_container_site_pv">
                |&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv"
                    class="white-color"></span>&nbsp;次
            </span>
            
            
            <span id="busuanzi_container_site_uv">
                |&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv"
                    class="white-color"></span>&nbsp;人
            </span>
            
            <br>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/fourgold" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:799392914@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>







    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=799392914" class="tooltipped" target="_blank" data-tooltip="QQ联系我: 799392914" data-position="top" data-delay="50">
        <i class="fab fa-qq"></i>
    </a>







    <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50">
        <i class="fas fa-rss"></i>
    </a>

</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

	
    

    

    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    

</body>

</html>
