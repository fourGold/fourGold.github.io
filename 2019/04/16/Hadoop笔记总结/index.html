<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="hadoop的入门学习笔记, 觉浅">
    <meta name="description" content="Jinxin Li的个人博客">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>hadoop的入门学习笔记 | 觉浅</title>
    <link rel="icon" type="image/png" href="/favicon.png">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">

    <script src="/libs/jquery/jquery.min.js"></script>

<meta name="generator" content="Hexo 5.3.0"></head>




<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">觉浅</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/contact" class="waves-effect waves-light">
      
      <i class="fas fa-comments" style="zoom: 0.6;"></i>
      
      <span>留言板</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>友情链接</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">觉浅</div>
        <div class="logo-desc">
            
            Jinxin Li的个人博客
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/contact" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-comments"></i>
			
			留言板
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			友情链接
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="http://github.com/fourgold/" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="http://github.com/fourgold/" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/featureimages/0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">hadoop的入门学习笔记</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        width: 345px;
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        padding: 35px 0 15px 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content {
        padding-bottom: 30px;
        overflow: auto;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;

        position: absolute;
        right: 23.5vw;
        display: block;
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 15px;
        bottom: 76px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                          <div class="article-tag">
                            <span class="chip bg-color">无标签</span>
                          </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2019-04-16
                </div>
                

                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    14.3k
                </div>
                

                

                
            </div>
        </div>
        <hr class="clearfix">

        
        <!-- 是否加载使用自带的 prismjs. -->
        <link rel="stylesheet" href="/libs/prism/prism.css">
        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <h1 id="Hadoop入门"><a href="#Hadoop入门" class="headerlink" title="Hadoop入门"></a>Hadoop入门</h1><h2 id="学习目标："><a href="#学习目标：" class="headerlink" title="学习目标："></a>学习目标：</h2><ul>
<li> 了解大数据的概念以及应用场景和发展前景（这部分还是会讲故事即可）</li>
<li> 初步掌握大数据部门业务分析流程以及完整的大数据部门的组织架构（还是了解讲故事…）</li>
<li> 通俗易懂的说明白Hadoop的概念以及发展历史</li>
<li> 掌握Hadoop的前后的版本迭代更新以及Hadoop的优势</li>
<li> <strong>重点理解Hadoop框架的三大组成部分，并准确的表述各自的作用</strong></li>
<li> 掌握大数据生态的概念</li>
<li> <strong>熟练操作Hadoop运行环境的搭建（重点掌握）</strong></li>
<li> <strong>熟练掌握Hadoop的运行模式（重点掌握）</strong></li>
<li> 掌握Hadoop2.x和Hadoop3.x版本的差异</li>
<li> 能够对Hadoop的源码进行编译</li>
<li> <strong>掌握常见的错误和问题（重点）</strong></li>
</ul>
<h2 id="一、大数据概论"><a href="#一、大数据概论" class="headerlink" title="一、大数据概论"></a>一、大数据概论</h2><p><strong>前言：</strong>这部分主要讲解的就是大数据的概念，以及大数据的应用领域和发展前景，要求大家能够用自己的话去描述，讲给别人听即可！</p>
<h3 id="1-大数据的发展史"><a href="#1-大数据的发展史" class="headerlink" title="1.大数据的发展史"></a>1.大数据的发展史</h3><pre class="line-numbers language-txt" data-language="txt"><code class="language-txt">In pioneer days they used oxen for heavy pulling, and when one ox couldn’t budge a log,they didn’t try to grow a larger ox. We shouldn’t be trying for bigger computers, but formore systems of computers.
—Grace Hopper<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<h3 id="2-大数据的概念"><a href="#2-大数据的概念" class="headerlink" title="2.大数据的概念"></a>2.大数据的概念</h3><p>​    大数据（big data），IT行业术语，是指无法在一定时间范围内用常规软件工具进行捕捉、管理和处理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的海量、高增长率和多样化的信息资产。</p>
<p>​    简单的讲 <strong>大数据</strong> 就是海量数据，我们想要利用这海量数据，必然要对它进行<strong>存储</strong> ，然后又想让其实现价值，必须得通过 <strong>分析计算</strong> 得到结果，而分析计算也不能没有时间限制，那就得在合理的时间内分析计算。最后一句话就是 <strong>大数据技术就是来完成海量数据的存储以及对海量数据在合理时间内进行分析运算的</strong></p>
<p>​    最小的基本单位是bit，按顺序给出所有单位：bit、Byte、KB、MB、GB、TB、PB、EB、ZB、YB、BB、NB、DB，它们按照进率1024（2的十次方）来计算：</p>
<p>8bit= 1Byte</p>
<p>1KB= 1,024 Bytes</p>
<p>1MB= 1,024 KB = 1,048,576 Bytes</p>
<p>1GB= 1,024 MB = 1,048,576 KB</p>
<p><strong>1TB= 1,024 GB = 1,048,576 MB</strong></p>
<p><strong>1PB= 1,024 TB = 1,048,576 GB</strong></p>
<p><strong>1EB= 1,024 PB = 1,048,576 TB</strong></p>
<p>1ZB= 1,024 EB = 1,048,576 PB</p>
<p>1YB= 1,024 ZB = 1,048,576 EB</p>
<p>1BB= 1,024 YB = 1,048,576 ZB</p>
<p>1NB= 1,024 BB = 1,048,576 YB</p>
<p>1 DB = 1,024 NB = 1,048,576 BB</p>
<h3 id="3-大数据的特点"><a href="#3-大数据的特点" class="headerlink" title="3.大数据的特点"></a>3.大数据的特点</h3><p>​    <strong>3.1大量（Volume）</strong></p>
<p>​    想要贴近大数据的概念，必然要求海量数据，用量化的单位来描述的话至少也得PB级别的起步。</p>
<p>​    <strong>3.2高速（Velocity）</strong></p>
<p>​    所谓的高速是指海量数据产生的速度是非常快的，例如 <strong>天猫双十一</strong> 大约1分钟左右成交100亿的，100亿背后所涉及的数据可想而知。同时数据产生速度的也要求我们对数据的处理的效率要跟上节奏才可以。</p>
<p>​    <strong>3.3多样（Variety）</strong></p>
<p>​    多样是指数据的体现形式是多样化的，大体分为三种形式  <strong>结构化数据</strong>  <strong>半结构化数据</strong>  <strong>非结构化化数据</strong>，这些所说的基本上都是原始数据，我们将来要想地数据更高效的运算都会对原始数据进行清洗。</p>
<p>​    <strong>3.4低价值密度（Value）</strong></p>
<p>​    在通常情况下，面对海量数据，往往我们需要的可能只是其中的一小部分，这就是说 <strong>价值密度的高低和数据总量是成反比的</strong> 这也是大数据比较显著的一个特点，所以 高效快速的对有价值的数据进行<strong>“提纯”</strong> 成为目前大数据领域一个攻坚破阻的难题。</p>
<h3 id="4-大数据的应用场景"><a href="#4-大数据的应用场景" class="headerlink" title="4.大数据的应用场景"></a>4.大数据的应用场景</h3><p>​    本章节主要了解大数据的真实应用场景和领域。这部分大家作为了解即可，推荐下面一片文章作为参考！</p>
<p>​    <a target="_blank" rel="noopener" href="https://www.jianshu.com/p/bb989c2fbc76">https://www.jianshu.com/p/bb989c2fbc76</a></p>
<h3 id="5-大数据的发展前景"><a href="#5-大数据的发展前景" class="headerlink" title="5.大数据的发展前景"></a>5.大数据的发展前景</h3><p>​    大数据行业的前景毋庸置疑是非常好的，从国家政策的推动再到行业的人才缺口以及未来的发展趋势都让大数据成为一个很有前途的专业。但是还是要求大家稳扎稳打 技术到家 才能翻江倒海！</p>
<h3 id="6-大数据部门业务流程分析"><a href="#6-大数据部门业务流程分析" class="headerlink" title="6.大数据部门业务流程分析"></a>6.大数据部门业务流程分析</h3><p>​    本小节主要介绍在工作当中我们将来完成一个项目的业务流程，我们大数据的工作在哪一环节崭露头角！我们大数据主要任务就是根据具体的需求对数据进行存储和分析运算，最后获取想要的数据结果。</p>
<h3 id="7-大数据部门组织结构（重点）"><a href="#7-大数据部门组织结构（重点）" class="headerlink" title="7.大数据部门组织结构（重点）"></a>7.大数据部门组织结构（重点）</h3><p>​    这一小节主要阐述一个公司通常大数据部门的智能分布，可以参考下图：</p>
<p><img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_21_01/hadoop%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%83%A8%E9%97%A8%E7%9A%84%E7%BB%84%E7%BB%87%E6%9E%B6%E6%9E%84.png" alt="大数据部门组织结构"></p>
<h2 id="二、从Hadoop框架讨论大数据生态"><a href="#二、从Hadoop框架讨论大数据生态" class="headerlink" title="二、从Hadoop框架讨论大数据生态"></a>二、从Hadoop框架讨论大数据生态</h2><h3 id="1-Hadoop的概念"><a href="#1-Hadoop的概念" class="headerlink" title="1. Hadoop的概念"></a>1. Hadoop的概念</h3><p>​    <strong>理解Hadoop是什么要从两个层面去入手：</strong></p>
<p>​    <strong>1.1 狭义：</strong>Hadoop是Apache旗下的一个用java语言实现开源软件框架，是一个开发和运行处理大规模数据的软件平台。允许使用简单的编程模型在大量计算机集群上对大型数据集进行分布式处理。它的核心组件有：</p>
<p>HDFS（分布式文件系统）：解决海量数据存储</p>
<p>YARN（作业调度和集群资源管理的框架）：解决资源任务调度</p>
<p>MAPREDUCE（分布式运算编程框架）：解决海量数据计算</p>
<p>​    <img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_21_01/hadoop图表.png" alt="image-20200815231415210" style="zoom:50%;" /></p>
<p>​    <strong>1.2 广义：</strong>广义上来说，Hadoop通常是指一个更广泛的概念——Hadoop生态圈。 </p>
<p>​    <img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_21_01/hadoop%E7%94%9F%E6%80%81%E5%9C%88.png" alt="hadoop生态圈"></p>
<p>当下的Hadoop已经成长为一个庞大的体系，随着生态系统的成长，新出现的项目越来越多，其中不乏一些非Apache主管的项目，这些项目对HADOOP是很好的补充或者更高层的抽象。比如：</p>
<p>HDFS：分布式文件系统</p>
<p>MAPREDUCE：分布式运算程序开发框架</p>
<p>HIVE：基于HADOOP的分布式数据仓库，提供基于SQL的查询数据操作</p>
<p>HBASE：基于HADOOP的分布式海量数据库</p>
<p>ZOOKEEPER：分布式协调服务基础组件</p>
<p>Mahout：基于mapreduce/spark/flink等分布式运算框架的机器学习算法库</p>
<p>OOZIE：工作流调度框架</p>
<p>Sqoop：数据导入导出工具（比如用于mysql和HDFS之间）</p>
<p>FLUME：日志数据采集框架</p>
<p>IMPALA：基于hive的实时sql查询分析</p>
<h3 id="2-Hadoop的发展史"><a href="#2-Hadoop的发展史" class="headerlink" title="2. Hadoop的发展史"></a>2. Hadoop的发展史</h3><p><img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_21_01/hadoop%E5%88%9B%E5%BB%BA%E8%80%85.png" alt="hadoop的创建者"></p>
<ol>
<li><p>2002年10月，Doug Cutting和Mike Cafarella创建了开源网页爬虫项目Nutch。</p>
</li>
<li><p>2003年10月，Google发表Google File System论文。</p>
</li>
<li><p>2004年7月，Doug Cutting和Mike Cafarella在Nutch中实现了类似GFS的功能，即后来HDFS的前身。</p>
</li>
<li><p>2004年10月，Google发表了MapReduce论文。</p>
</li>
<li><p>2005年2月，Mike Cafarella在Nutch中实现了MapReduce的最初版本。</p>
</li>
<li><p>2005年12月，开源搜索项目Nutch移植到新框架，使用MapReduce和NDFS在20个节点稳定运行。</p>
</li>
<li><p>2006年1月，Doug Cutting加入雅虎，Yahoo!提供一个专门的团队和资源将Hadoop发展成一个可在网络上运行的系统。</p>
</li>
<li><p>2006年2月，Apache Hadoop项目正式启动以支持MapReduce和HDFS的独立发展。</p>
</li>
<li><p>2006年3月，Yahoo!建设了第一个Hadoop集群用于开发。</p>
</li>
</ol>
<p>10.2006年4月，第一个Apache Hadoop发布。</p>
<p>11.2006年11月，Google发表了Bigtable论文，激起了Hbase的创建。</p>
<p>12.2007年10月，第一个Hadoop用户组会议召开，社区贡献开始急剧上升。</p>
<p>13.2007年，百度开始使用Hadoop做离线处理。</p>
<p>14.2007年，中国移动开始在“大云”研究中使用Hadoop技术。</p>
<p>15.2008年，淘宝开始投入研究基于Hadoop的系统——云梯，并将其用于处理电子商务相关数据。</p>
<p>16.2008年1月，Hadoop成为Apache顶级项目。</p>
<p>17.2008年2月，Yahoo!运行了世界上最大的Hadoop应用，宣布其搜索引擎产品部署在一个拥有1万个内核的Hadoop集群上。</p>
<p>18.2008年4月，在900个节点上运行1TB排序测试集仅需209秒，成为世界最快。</p>
<p>19.2008年8月，第一个Hadoop商业化公司Cloudera成立。</p>
<p>20.2008年10月，研究集群每天装载10TB的数据。</p>
<p>21.2009 年3月，Cloudera推出世界上首个Hadoop发行版——CDH（Cloudera’s Distribution including Apache Hadoop）平台，完全由开放源码软件组成。</p>
<p>22.2009年6月，Cloudera的工程师Tom White编写的《Hadoop权威指南》初版出版，后被誉为Hadoop圣经。</p>
<p>23.2009年7月 ，Hadoop Core项目更名为Hadoop Common;</p>
<p>24.2009年7月 ，MapReduce 和 Hadoop Distributed File System (HDFS) 成为Hadoop项目的独立子项目。</p>
<p>25.2009年8月，Hadoop创始人Doug Cutting加入Cloudera担任首席架构师。</p>
<p>26.2009年10月，首届Hadoop World大会在纽约召开。</p>
<p>27.2010年5月，IBM提供了基于Hadoop 的大数据分析软件——InfoSphere BigInsights，包括基础版和企业版。</p>
<p>28.2011年3月，Apache Hadoop获得Media Guardian Innovation Awards媒体卫报创新奖</p>
<p>29.2012年3月，企业必须的重要功能HDFS NameNode HA被加入Hadoop主版本。</p>
<p>30.2012年8月，另外一个重要的企业适用功能YARN成为Hadoop子项目。</p>
<p>31.2014年2月，Spark逐渐代替MapReduce成为Hadoop的缺省执行引擎，并成为Apache基金会顶级项目。</p>
<p>2017年12月，Release 3.0.0 generally available</p>
<h3 id="3-Hadoop三大发行版本"><a href="#3-Hadoop三大发行版本" class="headerlink" title="3. Hadoop三大发行版本"></a>3. Hadoop三大发行版本</h3><p><strong>3.1 Apache</strong></p>
<p>企业实际使用并不多。最原始（基础）版本。这是学习hadoop的基础。</p>
<p><strong>3.2 cloudera</strong></p>
<p>对hadoop的升级，打包，开发了很多框架。flume、hue、impala都是这个公司开发</p>
<p>2008 年成立的 Cloudera 是最早将 Hadoop 商用的公司，为合作伙伴提 供 Hadoop 的商用解决方案，主要是包括支持，咨询服务，培训。</p>
<p>2009年Hadoop的创始人 Doug Cutting也加盟 Cloudera公司。Cloudera 产品主要 为 CDH，Cloudera Manager，Cloudera Support</p>
<p>CDH是Cloudera的Hadoop发行版，完全开源，比Apache Hadoop在兼容性，安全 性，稳定性上有所增强。</p>
<p>Cloudera Manager是集群的软件分发及管理监控平台，可以在几个小时内部署 好一个Hadoop集群，并对集群的节点及服务进行实时监控。Cloudera Support即 是对Hadoop的技术支持。</p>
<p>Cloudera 的标价为每年每个节点4000美元。Cloudera开发并贡献了可实时处理大 数据的Impala项目。</p>
<p><strong>3.3 Hortonworks</strong></p>
<p>2011年成立的Hortonworks是雅虎与硅谷风投公司Benchmark Capital合资组建</p>
<p>公司成立之初就吸纳了大约25名至30名专门研究Hadoop的雅虎工程师，上述工 程师均在2005年开始协助雅虎开发Hadoop，贡献了Hadoop 80%的代码。</p>
<p>雅虎工程副总裁、雅虎Hadoop开发团队负责人Eric Baldeschwieler出任<br> Hortonworks的首席执行官。</p>
<p>Hortonworks 的主打产品是Hortonworks Data Platform (HDP)，也同样是100%开 源的产品，HDP除常见的项目外还包含了Ambari，一款开源的安装和管理系统</p>
<p>HCatalog，一个元数据管理系统，HCatalog现已集成到Facebook 开源的Hive中<br> 。Hortonworks的Stinger开创性地极大地优化了Hive项目。Hortonworks为入门提 供了一个非常好的，易于使用的沙盒。</p>
<p>Hortonworks开发了很多增强特性并提交至核心主干，这使得Apache Hadoop能 够在包括Windows Server和Windows Azure在内的Microsoft Windows平台上本地 运行。定价以集群为基础，每10个节点每年为12500美元。</p>
<h3 id="4-Hadoop的优势"><a href="#4-Hadoop的优势" class="headerlink" title="4. Hadoop的优势"></a>4. Hadoop的优势</h3><h5 id="4-1-高可靠性"><a href="#4-1-高可靠性" class="headerlink" title="4.1 高可靠性"></a>4.1 高可靠性</h5><p> Hadoop底层维护多个数据副本，所以即使Hadoop某个计算元素或存储出现故障，也不会导致数据的丢失。</p>
<h5 id="4-2-高可扩展性"><a href="#4-2-高可扩展性" class="headerlink" title="4.2 高可扩展性"></a>4.2 高可扩展性</h5><p> 在集群间分配任务数据，可方便的扩展数以千计的节点。</p>
<h5 id="4-3-高效性"><a href="#4-3-高效性" class="headerlink" title="4.3 高效性"></a>4.3 高效性</h5><p> 在MapReduce的思想下，Hadoop是并行工作的，以加快任务处理速度。</p>
<h5 id="4-4-高容错性"><a href="#4-4-高容错性" class="headerlink" title="4.4 高容错性"></a>4.4 高容错性</h5><p> 能够自动将失败的任务重新分配。</p>
<h3 id="5-Hadoop框架组成"><a href="#5-Hadoop框架组成" class="headerlink" title="5. Hadoop框架组成"></a>5. Hadoop框架组成</h3><p>Hadoop是一个能够对大量数据进行分布式处理的软件框架，以一种可靠、高效、可伸缩的方式进行数据处理，其有许多元素构成，以下是其组成元素：</p>
<p><img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_21_01/hadoop%E6%9E%B6%E6%9E%84.png" alt="hadoop结构"></p>
<p><strong>注意：</strong> 通过上图我们要掌握的重点是Hadoop是由核心的三大组件构成的，在hadoop1.x的版本中 只有两大组件分别是 <strong>HDFS(负责文件的存储)**和</strong>MapReduce(负责文件的计算和资源调度)** 。后来在hadoop2.x的时候出于架构的解耦考虑以及让 资源调度 工作能更加灵活多样化就把 原来MapReduce中的负责资源调度的功能剥离出来 单独形成 Yarn 这个核心组件。</p>
<h4 id="5-1HDFS理论概述"><a href="#5-1HDFS理论概述" class="headerlink" title="5.1HDFS理论概述"></a>5.1HDFS理论概述</h4><p><strong>HDFS:</strong> Hadoop Distributed File System(hadoop分布式文件系统)</p>
<p><strong>注意：</strong> 本小节主要是从理论的角度先去理解HDFS的概念，HDFS中还包含很多概念我们逐个来分析理解。</p>
<p><strong>1.HDFS的特点：</strong> </p>
<pre class="line-numbers language-mark" data-language="mark"><code class="language-mark">1. 保存多个副本，且提供容错机制，副本丢失或宕机自动恢复。默认存3份。

2. 运行在廉价的机器上。

3. 适合大数据的处理。HDFS默认会将文件分割成block，64M为1个block。
   然后将block按键值对存储在HDFS上，并将键值对的映射存到内存中。如果小文件太多，那内存的负担会很  重。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><strong>2.在HDFS中有三个重要的角色相互协调工作，分别是NameNode  SecondaryNameNode   DataNode</strong> </p>
<pre class="line-numbers language-mark" data-language="mark"><code class="language-mark">1.NameNode   Master节点，大领导。
  -- 管理数据块映射；
  -- 处理客户端的读写请求；
  -- 配置副本策略；
  -- 管理HDFS的名称空间。 
  -- namenode 内存中存储的是 &#x3D; fsimage + edits。
     其中fsimage元数据镜像文件（文件系统的目录树），edits元数据的操作日志（针对文件系统做的修改操	  作记录）
  总之：NameNode很重要，在海量数据的存储和管理，NameNode就相当于是所有数据的描述或者指针，有了它才能进一步操作真实数据。
  
2.SecondaryNameNode  它是个小弟，分担大哥NameNode的工作量。
  -- SecondaryNameNode负责定时默认1小时，从namenode上，获取fsimage和edits来进行合并，然后再      发送给namenode。减少namenode的工作量。
  -- NameNode的冷备份。
  
3.DataNode  真实数据的存储位置
  -- 存储client发来的数据块block；
  -- 执行数据块的读写操作。
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>


<h4 id="5-2-YARN架构概述"><a href="#5-2-YARN架构概述" class="headerlink" title="5.2 YARN架构概述"></a>5.2 YARN架构概述</h4><p>​        本小节主要了解YARN架构中重要的几个 组件。本次接触YARN不要求掌握其本质原理，只要求混个脸熟，大概了解YARN的作用以及组成部分，为后面的学习建立基础。</p>
<p>​    <strong>1.为什么要用YARN？</strong></p>
<p>​        首先我们要知道的是在Hadoop1.x时代 是没有YARN的，那时候所有的数据计算以及计算过程的任务分配和资源调度都是在MapReduce中进行的，这样存在很多问题和隐患，典型的就是JobTracker容易存在单点故障和JobTracker负担重，既要负责资源管理，又要进行作业调度；当需处理太多任务时，会造成过多的资源消耗。所以在Hadoop2.x的时候，推出了YARN这套系统，其主要目的就是将Hadoop中的资源调度功能独立的分离出来，这样更方便扩展，也能高效合理的调度资源。</p>
<p>​    <strong>2.YARN中的几大角色</strong></p>
<p>​        <img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_21_01/Yarn%E7%BB%93%E6%9E%84.png" alt="yarn结构"></p>
<p>​        <strong>– ResourceManager</strong></p>
<p>​            YARN 分层结构的本质是 ResourceManager。这个实体控制整个集群并管理应用程序向基础计算资源的分配。ResourceManager 将各个资源部分（计算、内存、带宽等）精心安排给基础 NodeManager（YARN 的每节点代理）。ResourceManager 还与 ApplicationMaster 一起分配资源，与 NodeManager 一起启动和监视它们的基础应用程序。在此上下文中，ApplicationMaster 承担了以前的 TaskTracker 的一些角色，ResourceManager 承担了 JobTracker 的角色。</p>
<p>​            <strong>总的来说，RM有以下作用：</strong></p>
<pre><code>        1）处理客户端请求

        2）启动或监控ApplicationMaster</code></pre>
<p>​            3）监控NodeManager</p>
<pre><code>        4）资源的分配与调度</code></pre>
<p>​        <strong>– NodeManager</strong></p>
<p>​                ApplicationMaster 管理在YARN内运行的每个应用程序实例。ApplicationMaster 负责协调来自 ResourceManager 的资源，并通过 NodeManager 监视容器的执行和资源使用（CPU、内存等的资源分配）。请注意，尽管目前的资源更加传统（CPU 核心、内存），但未来会带来基于手头任务的新资源类型（比如图形处理单元或专用处理设备）。从 YARN 角度讲，ApplicationMaster 是用户代码，因此存在潜在的安全问题。YARN 假设 ApplicationMaster 存在错误或者甚至是恶意的，因此将它们当作无特权的代码对待。</p>
<p>​                <strong>总的来说,AM有以下作用：</strong></p>
<pre><code>            1）负责数据的切分</code></pre>
<p>​                2）为应用程序申请资源并分配给内部的任务</p>
<p>​                3）任务的监控与容错</p>
<p>​        <strong>– ApplicationMaster</strong></p>
<p>​                NodeManager管理YARN集群中的每个节点。NodeManager 提供针对集群中每个节点的服务，从监督对一个容器的终生管理到监视资源和跟踪节点健康。MRv1 通过插槽管理 Map 和 Reduce 任务的执行，而 NodeManager 管理抽象容器，这些容器代表着可供一个特定应用程序使用的针对每个节点的资源。</p>
<p>​                <strong>总的来说，NM有以下作用：</strong></p>
<pre><code>            1）管理单个节点上的资源

            2）处理来自ResourceManager的命令

            3）处理来自ApplicationMaster的命令</code></pre>
<p>​        <strong>– Container</strong></p>
<p>​            Container 是 YARN 中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等，当AM向RM申请资源时，RM为AM返回的资源便是用Container表示的。YARN会为每个任务分配一个Container，且该任务只能使用该Container中描述的资源。</p>
<p>​            <strong>总的来说，Container有以下作用：</strong></p>
<pre><code>       1）对任务运行环境进行抽象，封装CPU、内存等多维度的资源以及环境变量、启动命令等任务运行相关的信息</code></pre>
<p><strong>总结：要使用一个 YARN 集群，首先需要一个包含应用程序的客户的请求。ResourceManager 协商一个容器的必要资源，启动一个 ApplicationMaster 来表示已提交的应用程序。通过使用一个资源请求协议，ApplicationMaster 协商每个节点上供应用程序使用的资源容器。执行应用程序时，ApplicationMaster 监视容器直到完成。当应用程序完成时，ApplicationMaster 从 ResourceManager 注销其容器，执行周期就完成了。</strong></p>
<h4 id="5-3-MapReduce架构概述"><a href="#5-3-MapReduce架构概述" class="headerlink" title="5.3 MapReduce架构概述"></a>5.3 MapReduce架构概述</h4><h3 id="6-大数据技术生态体系"><a href="#6-大数据技术生态体系" class="headerlink" title="6. 大数据技术生态体系"></a>6. 大数据技术生态体系</h3><p><img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_21_01/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E7%94%9F%E6%80%81.png" alt="大数据技术生态"></p>
<p><strong>小结：</strong>大概描述就是 首先 大数据的根本就是分析计算数据，那一定要定位数据来源，数据来源大体包含三个方面，分别是 正规的数据库（结构化数据），其次还有文件日志（半结构化数据）以及通过一些爬虫手段获取的互联网数据（非结构化数据）这就组成了我们的**<em>数据来源层**</em>。 </p>
<p>​        有了数据来源接下来就需要将这些数据传输到我们的分布式文件存储系统（HDFS）或者直接通过消息队列（kafka）将数据传输到数据计算层来做数据分析和运算，这里我们把专门做数据传输的技术层称之为*<strong>数据传输层***，同时保存到HDFS中后，我们成这块内容为 *</strong>数据存储层***。</p>
<p>​        有了具体的数据那后续就可以做数据分析运算了，这时候就要有 <strong><em>数据计算层</em></strong> 来完成，这部分大概根据数据结果的实效性可以分为两类数据分析运算的场景，一种是离线运算，一种实时运算，离线的话我们通常采用MapReduce和Hive来完成。实时的话就会用到Spark体系架构完成或者用Fink框架。</p>
<p>​        结合上面提到的概念，我们还要加入 <strong><em>资源管理层</em></strong>   主要有 YARN 来完成，它的主要工作就是来分配调度计算资源的，用来协作 MapReduce 作业。同时在实行数据运算的时候 我们考虑到服务器的资源分配以及人物先后执行的顺序，有加入了一个 <strong><em>任务调度层</em></strong>  专门来控制运算作业的执行时间和先后顺序</p>
<p>​        以上就是大数据架构体系的协作规则和架构说明，但是我们最后又考虑到 分布式集群的操作，各个版块和服务一定会交叉协同工作，所以最后利用Zookeeper来统一管理 分布式集群架构。OK，以上就是关于大数据技术生态体系的话术表现。</p>
<p>​        </p>
<h3 id="7-推荐系统框架图"><a href="#7-推荐系统框架图" class="headerlink" title="7. 推荐系统框架图"></a>7. 推荐系统框架图</h3><img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_21_01/推荐系统架构图.png" alt="image-20200729111555275" style="zoom: 50%;" />

<p><strong>小结：</strong>以上的一个推荐系统的大概描述，首先一定从用户的行为开始入手，当用户购买一件商品加入购物车后，我们往往会给用户推荐相关的类似产品或者连带产品，这是目前电商系统很常见的一种营销手段。这个推荐的数据是如何产生的呢？</p>
<p>1.用户将商品加入购物车，这是会产生购物车数据，这就是我们的数据来源</p>
<p>2.利用数据传输层的相关技术将数据进行搜集处理然后通过Kafak消息队列直接将数据传输到 实时运算的框架中进行分析运算。</p>
<p>3.当 数据计算层 把数据分析运算后会得到最终的结果，根据结果为依据找到相关的类似商品的数据进行整合。</p>
<p>4.最后回到电商系统中 的推荐模块 通过调用接口的方式获取最终的分析处理后整合的商品数据的结果，将其展示到客户端页面中。</p>
<p>上面大概就是一个推荐的流程，你学到了吗！！！</p>
<h2 id="三、Hadoop运行环境搭建（重点）"><a href="#三、Hadoop运行环境搭建（重点）" class="headerlink" title="三、Hadoop运行环境搭建（重点）"></a>三、Hadoop运行环境搭建（重点）</h2><h3 id="1-虚拟机环境准备"><a href="#1-虚拟机环境准备" class="headerlink" title="1. 虚拟机环境准备"></a>1. 虚拟机环境准备</h3><ul>
<li><p><strong>1). 准备模板机</strong>（安装最小化的Linux系统）</p>
<ul>
<li><p>yum安装必要的插件</p>
 <pre class="line-numbers language-mark" data-language="mark"><code class="language-mark">sudo yum install -y epel-release

sudo yum install -y psmisc nc net-tools rsync vim lrzsz ntp libzstd openssl-static tree iotop git<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li>
<li><p>修改 /etc/hosts 文件</p>
 <pre class="line-numbers language-mark" data-language="mark"><code class="language-mark">192.168.2.100 hadoop100
192.168.2.101 hadoop101
192.168.2.102 hadoop102
192.168.2.103 hadoop103
192.168.2.104 hadoop104
192.168.2.105 hadoop105
192.168.2.106 hadoop106
192.168.2.107 hadoop107
192.168.2.108 hadoop108<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li>
<li><p>设置Linux的防火墙开机不自启</p>
 <pre class="line-numbers language-mark" data-language="mark"><code class="language-mark">systemctl stop firewalld
systemctl disable firewalld<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li>
<li><p>创建 atguigu 用户</p>
 <pre class="line-numbers language-mark" data-language="mark"><code class="language-mark">useradd atguigu<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li>
<li><p>修改/etc/sudoers文件 配置atguigu用户具有root权限</p>
 <pre class="line-numbers language-mark" data-language="mark"><code class="language-mark">在第92行的位置加上以下内容
atguigu ALL&#x3D;(ALL)  NOPASSWD:ALL

:wq! 强制保存退出。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li>
<li><p>在/opt目录下创建两个文件夹 </p>
 <pre class="line-numbers language-mark" data-language="mark"><code class="language-mark">mkdir &#x2F;opt&#x2F;software   --放置需要安装的软件的安装包
madir &#x2F;opt&#x2F;module     --软件的安装目录<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li>
<li><p>配置 两个文件夹 属于 atguigu 用户和 atguigu 组</p>
 <pre class="line-numbers language-mark" data-language="mark"><code class="language-mark">chown atguigu:atguigu &#x2F;opt&#x2F;software

chown atguigu:atguigu &#x2F;opt&#x2F;module<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li>
</ul>
</li>
<li><p><strong>2). 准备开发用的虚拟机</strong></p>
<ul>
<li><p>根据模板机克隆一台机器</p>
<ul>
<li> 根据克隆的步骤进行克隆就可以(参考Linux阶段的克隆操作)</li>
<li> 启动虚拟机</li>
</ul>
</li>
<li><p>修改克隆机的主机名</p>
 <pre class="line-numbers language-mark" data-language="mark"><code class="language-mark">1.编辑hostname文件
vim &#x2F;etc&#x2F;hostname

2.修改主机名称
hadoop101

3.重启机器 
reboot<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li>
<li><p>修改克隆机的ip</p>
 <pre class="line-numbers language-mark" data-language="mark"><code class="language-mark">1.编辑ifcfg-ens33文件
vim &#x2F;etc&#x2F;sysconfig&#x2F;network-spcripts&#x2F;ifcfg-ens33

2.重点修改的一下标注的地方<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<p> <img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_21_01/%E4%BF%AE%E6%94%B9%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AE%E7%BD%91%E7%BB%9C.png" alt="修改集群配置网络"></p>
</li>
<li><p>利用FinallShell工具连接Linux</p>
<p> <img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_21_01/%E9%9B%86%E7%BE%A4%E6%B5%8B%E8%AF%95.png" alt="集群测试VM"></p>
</li>
</ul>
</li>
</ul>
<h3 id="2-在准备好开发机hadoop101安装JDK"><a href="#2-在准备好开发机hadoop101安装JDK" class="headerlink" title="2. 在准备好开发机hadoop101安装JDK"></a>2. 在准备好开发机hadoop101安装JDK</h3><p>​     <strong>概述：</strong>本小节主要讲解在Linux中如何安装jdk，首先要明白Hadoop是用Java开发的，换言之Hadoop就是一款Java写的软件，那么想要运行Hadoop必然需要jdk环境。在Linux中安装Jdk和Windows中安装原理相同，只不过在Linux中Jdk的体现形式是一个 tar.gz的压缩包而Windows中是一个可视化安装程序。</p>
<ul>
<li><p><strong>1). 卸载现有JDK</strong></p>
<p> ​    <strong>注意：如果首次安装就没必要进行这一步，如果想更换jdk,非首次安装则需要先把已有的卸载掉</strong></p>
 <pre class="line-numbers language-mark" data-language="mark"><code class="language-mark">rpm -qa | grep -i java | xargs -n1 sudo rpm -e --nodeps<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li>
<li><p><strong>2). 将jdk的tar包导入到Linux中opt目录下的software下</strong></p>
 <pre class="line-numbers language-mark" data-language="mark"><code class="language-mark">在我们的FinallShell工具中，直接找到opt目录下的software文件夹，将Windows目录下的jdk-8u212-linux-x64.tar.gz 包拖拽到software文件夹里即可<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li>
<li><p><strong>3).解压jdk压缩包到opt目录下的module文件夹中</strong></p>
 <pre class="line-numbers language-mark" data-language="mark"><code class="language-mark">tar -zxvf jdk-8u212-linux-x64.tar.gz -C &#x2F;opt&#x2F;module&#x2F;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li>
<li><p><strong>4). 配置jdk的环境变量</strong></p>
<p> <strong>概述：</strong>接下来我们就要配置jdk的环境变量，思路和在windows系统下配置环境变量类似。这里注意一下，在Linux中 我们可以通过修改 Linux的核心profile文件来添加jdk的环境变量，但是我们通常不会这么做，原因就是不希望改动Linux原有的核心文件，以免引起不必要的麻烦，那我们怎么做呢？推荐方式就是自己在指定的目录下创建一个xxx.sh文件用来充当我们自己的配置文件。当Linux系统启动后会加载profile 文件，而profile文件中的脚本会循环遍历加载 /etc/profile.d/ 目录下所有以sh为后缀名的文件，所以我们自己创建xxx.sh文件也就被加载到了。固然环境变量也就生效了！</p>
<ul>
<li><p>在/etc/profile.d/目录下新建文件 my_env.sh文件</p>
 <pre class="line-numbers language-mark" data-language="mark"><code class="language-mark">sudo vim &#x2F;etc&#x2F;profile.d&#x2F;my_env.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li>
<li><p>在my_env.sh文件中添加一下内容</p>
 <pre class="line-numbers language-mark" data-language="mark"><code class="language-mark">#JAVA_HOME
export JAVA_HOME&#x3D;&#x2F;opt&#x2F;module&#x2F;jdk1.8.0_212
export PATH&#x3D;$PATH:$JAVA_HOME&#x2F;bin<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li>
<li><p>保存后退出</p>
 <pre class="line-numbers language-mark" data-language="mark"><code class="language-mark">:wq<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li>
<li><p>source 重新加载 /etc/profile文件，环境变量生效</p>
 <pre class="line-numbers language-mark" data-language="mark"><code class="language-mark">source &#x2F;etc&#x2F;profile<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li>
<li><p>验证jdk是否安装以及配置成功</p>
 <pre class="line-numbers language-mark" data-language="mark"><code class="language-mark">java -version<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p> 如下图就成功了！</p>
 <img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_21_01/java的linux编译命令.png" alt="image-20200729231158594" style="zoom:80%;" />

<p> 如果没成功就reboot重启Linux，如果没问题就不用了重启！</p>
</li>
</ul>
</li>
</ul>
<h3 id="3-在开发机hadoop101安装Hadoop"><a href="#3-在开发机hadoop101安装Hadoop" class="headerlink" title="3. 在开发机hadoop101安装Hadoop"></a>3. 在开发机hadoop101安装Hadoop</h3><p><strong>概述：</strong>终于要安装hadoop了，hadoop我们把它看做适合jdk是同一类型的软件，jdk怎么操作hadoop也怎么操作就可以！</p>
<ul>
<li><p> <strong>1). 将hadoop的tar包拖拽到/opt/software目录下</strong></p>
</li>
<li><p><strong>2). 将hadoop解压缩到/opt/module目录下</strong></p>
 <pre class="line-numbers language-mark" data-language="mark"><code class="language-mark">tar -zxvf hadoop-3.1.3.tar.gz -C &#x2F;opt&#x2F;module&#x2F;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li>
<li><p><strong>3).配置hadoop的环境变量</strong></p>
<p> <strong>注意：</strong>hadoop中有一个特别之处，就是在hadoop的目录下的bin目录和sbin目录都是hadoop的执行脚本，所以我们在配置hadoop的环境变量的时候要注意把这两个都配上才可以！剩下其他的操作都和jdk一样了！</p>
<ul>
<li><p>打开/etc/profile.d/my_env.sh文件</p>
 <pre class="line-numbers language-mark" data-language="mark"><code class="language-mark">sudo vim &#x2F;etc&#x2F;profile.d&#x2F;my_env.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li>
<li><p>在my_env.sh文件末尾添加如下内容：（shift+g）</p>
 <pre class="line-numbers language-mark" data-language="mark"><code class="language-mark">#HADOOP_HOME
export HADOOP_HOME&#x3D;&#x2F;opt&#x2F;module&#x2F;hadoop-3.1.3
export PATH&#x3D;$PATH:$HADOOP_HOME&#x2F;bin
export PATH&#x3D;$PATH:$HADOOP_HOME&#x2F;sbin<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li>
<li><p>保存退出</p>
 <pre class="line-numbers language-mark" data-language="mark"><code class="language-mark">:wq<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li>
<li><p>source 重新加载 /etc/profile文件，环境变量生效</p>
 <pre class="line-numbers language-mark" data-language="mark"><code class="language-mark">source &#x2F;etc&#x2F;profile<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li>
<li><p>验证hadoop是否安装以及配置成功</p>
 <pre class="line-numbers language-markdown" data-language="markdown"><code class="language-markdown">hadoop version<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p> <img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_21_01/hadoop%E7%BC%96%E8%AF%91%E9%A2%86%E5%91%BD.png" alt="hadoop编译命令"></p>
<p> 如图所示表示安装成功！</p>
</li>
</ul>
</li>
</ul>
<h3 id="4-Hadoop目录结构"><a href="#4-Hadoop目录结构" class="headerlink" title="4. Hadoop目录结构"></a>4. Hadoop目录结构</h3><ul>
<li> <strong>bin：</strong> bin目录是Hadoop最基本的管理脚本和使用脚本所在的目录，这些脚本是sbin目录下管理脚本的基础实现，用户可以直接使用这些脚本管理和使用Hadoop</li>
<li> <strong>etc：</strong> Hadoop配置文件所在的目录，包括：core-site.xml、hdfs-site.xml、mapred-site.xml和yarn-site.xml等配置文件。</li>
<li> <strong>include：</strong>对外提供的编程库头文件（具体的动态库和静态库在lib目录中），这些文件都是用C++定义的，通常用于C++程序访问HDFS或者编写MapReduce程序。</li>
<li> <strong>lib：</strong>包含了Hadoop对外提供的编程动态库和静态库，与include目录中的头文件结合使用。</li>
<li> <strong>libexec：</strong>各个服务对应的shell配置文件所在的目录，可用于配置日志输出目录、启动参数（比如JVM参数）等基本信息。</li>
<li> <strong>sbin：</strong> Hadoop管理脚本所在目录，主要包含HDFS和YARN中各类服务启动/关闭的脚本。</li>
<li> <strong>share：</strong> Hadoop各个模块编译后的Jar包所在目录，这个目录中也包含了Hadoop文档。</li>
</ul>
<h2 id="四、Hadoop运行模式"><a href="#四、Hadoop运行模式" class="headerlink" title="四、Hadoop运行模式"></a>四、Hadoop运行模式</h2><p><strong>前言：</strong>本章节主要来学习Hadoop的运行模式，何谓运行模式呢？简单的讲就是Hadoop该如何运作起来，或者理解为玩Hadoop的游戏规则，是单台机器运行，还是多台协作运行，不同的运行模式有不一样的配置和处理。Hadoop中一共存在三种运行模式， 本地模式、伪分布式模式、完全分布式模式。</p>
<p><strong>本地模式：</strong>在一台单机上运行，没有分布式文件系统，而是直接读写本地操作系统的文件系统。</p>
<p><strong>伪分布式：</strong>这种模式也是在一台单机上运行，但用不同的Java进程模仿分布式运行中的各类结点: (NameNode,DataNode,JobTracker,TaskTracker,SecondaryNameNode) ，同理 集群中的结点由一个JobTracker和若干个TaskTracker组成，JobTracker负责任务的调度，TaskTracker负责并行执行任务。TaskTracker必须运行在DataNode上，这样便于数据的本地计算。JobTracker和NameNode则无须在同一台机器上。一个机器上，既当namenode，又当datanode,或者说 既 是jobtracker,又是tasktracker。没有所谓的在多台机器上进行真正的分布式计算，故称为”伪分布式”。</p>
<p><strong>完全分布式：</strong>真正的分布式，由3个及以上的实体机或者虚拟机组件的机群。</p>
<p><strong>注意：</strong>我们在课程中 用本地模式来入门开胃，然后集中火力做 <strong>完全分布式</strong> 伪分布式只做了解即可，没有太大意义！</p>
<h3 id="1-本地运行模式"><a href="#1-本地运行模式" class="headerlink" title="1.本地运行模式"></a>1.本地运行模式</h3><p>​    本小节主要就是感受一把Hadoop的运行过程，根据Hadoop官方提供的示例来操作几个Hadoop的基本功能点。更重要的是掌握基本操作Hadoop的步骤和思路。</p>
<p><strong>案例1需求描述：</strong>利用hadoop的grep过滤功能，将一批文件中的一些内容过滤出来。</p>
<p><strong>实现步骤：</strong></p>
<p><strong>1.1 在hadoop的解压目录创建一个文件夹input，作为需要过滤的文件的输入目录</strong></p>
<pre class="line-numbers language-mark" data-language="mark"><code class="language-mark">mkdir input<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>*<em>1.2 将hadoop目录下的 etc/hadoop/</em>.xml文件都复制到 input目录下，作为被过滤文件**</p>
<pre class="line-numbers language-mark" data-language="mark"><code class="language-mark">cp &#x2F;etc&#x2F;hadoop&#x2F;*.xml input<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p><strong>1.3 执行 bin/hadoop 命令，运行share/hadoop/mapreduce/目录下的hadoop-mapreduce-examples-3.1.3.jar包中的 grep 过滤功能，并限制一定的规则</strong></p>
<pre class="line-numbers language-mark" data-language="mark"><code class="language-mark">bin&#x2F;hadoop jar share&#x2F;hadoop&#x2F;mapreduce&#x2F;hadoop-mapreduce-examples-3.1.3.jar grep input output &#39;dfs[a-z.]+&#39;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p><strong>1.4 最后在output目录下查看过滤的结果即可！</strong></p>
<p><strong>案例2需求描述：</strong>利用Hadoop完成经典wordcount(单词统计)，就是针对一些文件计算统计里面相同单词的个数。</p>
<p><strong>实现步骤：</strong></p>
<p><strong>1.1 创建在hadoop-3.1.3文件下面创建一个wcinput文件夹</strong></p>
<pre class="line-numbers language-mark" data-language="mark"><code class="language-mark">mkdir wcinput<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p><strong>1.2 在wcinput文件下创建一个word.txt文件</strong></p>
<pre class="line-numbers language-mark" data-language="mark"><code class="language-mark">cd wcinput<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p><strong>1.3 编辑word.txt文件</strong></p>
<pre class="line-numbers language-mark" data-language="mark"><code class="language-mark">vim word.txt

在文件中输入如下内容(内容随意)
hadoop yarn
hadoop mapreduce
atguigu
atguigu<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><strong>1.4 回到Hadoop目录/opt/module/hadoop-3.1.3</strong>  <strong>执行程序</strong></p>
<pre class="line-numbers language-mark" data-language="mark"><code class="language-mark">hadoop jar share&#x2F;hadoop&#x2F;mapreduce&#x2F;hadoop-mapreduce-examples-3.1.3.jar wordcount wcinput wcoutput<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p><strong>1.5 查看结果</strong></p>
<pre class="line-numbers language-mark" data-language="mark"><code class="language-mark"> cat wcoutput&#x2F;part-r-00000
 
看到如下结果：
atguigu 2
hadoop  2
mapreduce 1
yarn    1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="2-完全分布式运行模式-重点掌握"><a href="#2-完全分布式运行模式-重点掌握" class="headerlink" title="2.完全分布式运行模式(重点掌握)"></a>2.完全分布式运行模式(重点掌握)</h3><p>​    本章节是重中之重，主要讲解完全分布式运行模式。</p>
<h4 id="2-1-准备3台服务器"><a href="#2-1-准备3台服务器" class="headerlink" title="2.1 准备3台服务器"></a>2.1 准备3台服务器</h4><p>为了满足集群的环境，我们需要准备三台服务器，准备方式就是根据我们之前做好的模板机进行克隆即可，但是需要注意，三台服务器的的 静态ip地址和主机名都要修改一下，以便区分！</p>
<h5 id="2-1-1-克隆第一台"><a href="#2-1-1-克隆第一台" class="headerlink" title="2.1.1 克隆第一台"></a>2.1.1 克隆第一台</h5><p>修改主机名为hadoop102</p>
<p>修改ip地址为：192.168.2.102</p>
<h5 id="2-1-2-克隆第二台"><a href="#2-1-2-克隆第二台" class="headerlink" title="2.1.2 克隆第二台"></a>2.1.2 克隆第二台</h5><p>修改主机名为hadoop103</p>
<p>修改ip地址为：192.168.2.103</p>
<h5 id="2-1-3-克隆第三台"><a href="#2-1-3-克隆第三台" class="headerlink" title="2.1.3 克隆第三台"></a>2.1.3 克隆第三台</h5><p>修改主机名为hadoop104</p>
<p>修改ip地址为：192.168.2.104</p>
<h4 id="2-2-集群分发脚本的应用场景"><a href="#2-2-集群分发脚本的应用场景" class="headerlink" title="2.2 集群分发脚本的应用场景"></a>2.2 集群分发脚本的应用场景</h4><p><strong>场景介绍：</strong></p>
<p>​        上面我们已经准备好了三台服务器，并且都各自修改了主机名和ip地址。但是我们知道 需要额必备软件以及环境变量还没有配置，如果机械的一台一台配置也可以但是这样会引发大量的重复性工作，没有必要。如何能避免重复配置呢，最好是值在一台机器进行修改 然后将修改的配置信息同步到集群的所有机器那就完美了！这时候就要用到 分发脚本 的方案！</p>
<h5 id="2-2-1-scp-安全拷贝"><a href="#2-2-1-scp-安全拷贝" class="headerlink" title="2.2.1 scp 安全拷贝"></a>2.2.1 scp 安全拷贝</h5><p><strong>scp含义：</strong></p>
<p>​    scp命令可以实现服务器与服务器之间的数据拷贝</p>
<p><strong>基本语法：</strong></p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">scp</span>	-r	<span class="token variable">$pdir</span>/<span class="token variable">$fname</span> <span class="token variable">$user</span>@hadoop<span class="token variable">$host</span><span class="token builtin class-name">:</span><span class="token variable">$pdir</span>/<span class="token variable">$fname</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>​    命令  递归    要拷贝的文件路径/名称   目的用户@主机:目的路径/名称</p>
<p><strong>案例实操：</strong></p>
<p>前提：在 hadoop102 hadoop103 hadoop104 都已经创建好的 /opt/module</p>
<p>​      /opt/software 两个目录， 并且已经把这两个目录修改为atguigu:atguigu</p>
<p>​      sudo chown atguigu:atguigu -R /opt/module</p>
<p>1).在hadoop101上，将hadoop101中/opt/module/目录下所有内容拷贝到hadoop102上的/opt/module/目录下。</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">scp</span> -r /opt/module/* atguigu@hadoop102:/opt/module/<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>2).在hadoop103上，将hadoop101中/opt/module/目录下的所有内容拷贝到hadoop103的/opt/module/目录下。</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">scp</span> -r atguigu@hadoop101:/opt/module/* /opt/module/<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>3).在hadoop103上，将hadoop101中/opt/module/目录下的所有内容拷贝到hadoop104的/opt/module/目录下。</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"><span class="token function">scp</span> -r atguigu@hadoop101:/opt/module/* atguigu@hadoop104:/opt/module/<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>4).在任意一台机器上，将hadoop101中的/etc/profile.d目录下的my_env.sh配置文件分别复制到hadoop102、hadoop103、hadoop104上</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"><span class="token number">1</span>. <span class="token function">scp</span> -r /etc/profile.d/my_env.sh root@hadoop102:/etc/profile.d/
<span class="token number">2</span>. <span class="token function">scp</span> -r /etc/profile.d/my_env.sh root@hadoop103:/etc/profile.d/
<span class="token number">3</span>. <span class="token function">scp</span> -r /etc/profile.d/my_env.sh root@hadoop104:/etc/profile.d/<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<h5 id="2-2-2-rsync远程同步工具"><a href="#2-2-2-rsync远程同步工具" class="headerlink" title="2.2.2 rsync远程同步工具"></a>2.2.2 rsync远程同步工具</h5><p><strong>功能描述：</strong></p>
<p>​        rsync主要用于备份和镜像。具有速度快、避免复制相同内容和支持符号链接的优点。</p>
<p><strong>rsync和scp区别：</strong></p>
<p>​        用rsync做文件的复制要比scp的速度快，rsync只对差异文件做更新。scp是把所有文件都复制过去。</p>
<p><strong>基本语法：</strong></p>
<p>rsync   -av    $pdir/$fname        $user@hadoop$host:$pdir/$fname</p>
<p>命令  选项参数  要拷贝的文件路径/名称  目的用户@主机:目的路径/名称</p>
<p>​     选项参数说明</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-a</td>
<td>归档拷贝</td>
</tr>
<tr>
<td>-v</td>
<td>显示复制过程</td>
</tr>
</tbody></table>
<p><strong>案例实操:</strong></p>
<p>把hadoop102机器上的/opt/software目录同步到hadoop103服务器的/opt/software目录下（没有实际意义的操作只是为了练手）</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"><span class="token function">rsync</span> -av /opt/software/* atguigu@hadoop103:/opt/software/<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<h5 id="2-2-3-分发脚本的应用"><a href="#2-2-3-分发脚本的应用" class="headerlink" title="2.2.3 分发脚本的应用"></a>2.2.3 分发脚本的应用</h5><p><strong>概述：</strong>前面其实我们已经是实现了服务器之间的文件目录拷贝传递了，但是每次都得执行命令来实现，还是比较麻烦的，干脆一步到位，通过编写一个脚本 通过执行脚本来实现信息拷贝。</p>
<p><strong>前提：</strong> 在/home/atguigu/bin这个目录下存放的脚本，atguigu用户可以在系统任何地方直接执行。</p>
<p><strong>脚本实现：</strong></p>
<p>1). 在/home/atguigu/bin目录下创建xsync文件</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"><span class="token punctuation">[</span>atguigu@hadoop102 opt<span class="token punctuation">]</span>$ <span class="token builtin class-name">cd</span> /home/atguigu
<span class="token punctuation">[</span>atguigu@hadoop102 ~<span class="token punctuation">]</span>$ <span class="token function">mkdir</span> bin
<span class="token punctuation">[</span>atguigu@hadoop102 ~<span class="token punctuation">]</span>$ <span class="token builtin class-name">cd</span> bin
<span class="token punctuation">[</span>atguigu@hadoop102 bin<span class="token punctuation">]</span>$ <span class="token function">vim</span> xsync<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<p>2). 在该文件中编写如下代码</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"><span class="token shebang important">#!/bin/bash</span>
<span class="token comment">#1. 判断参数个数</span>
<span class="token keyword">if</span> <span class="token punctuation">[</span> <span class="token variable">$#</span> -lt <span class="token number">1</span> <span class="token punctuation">]</span>
<span class="token keyword">then</span>
  <span class="token builtin class-name">echo</span> Not Enough Arguement<span class="token operator">!</span>
  <span class="token builtin class-name">exit</span><span class="token punctuation">;</span>
<span class="token keyword">fi</span>
<span class="token comment">#2. 遍历集群所有机器</span>
<span class="token keyword">for</span> <span class="token for-or-select variable">host</span> <span class="token keyword">in</span> hadoop102 hadoop103 hadoop104
<span class="token keyword">do</span>
  <span class="token builtin class-name">echo</span> <span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span>  <span class="token variable">$host</span>  <span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span>
  <span class="token comment">#3. 遍历所有目录，挨个发送</span>
  <span class="token keyword">for</span> <span class="token for-or-select variable">file</span> <span class="token keyword">in</span> <span class="token variable">$@</span>
  <span class="token keyword">do</span>
    <span class="token comment">#4. 判断文件是否存在</span>
    <span class="token keyword">if</span> <span class="token punctuation">[</span> -e <span class="token variable">$file</span> <span class="token punctuation">]</span>
    <span class="token keyword">then</span>
      <span class="token comment">#5. 获取父目录</span>
      <span class="token assign-left variable">pdir</span><span class="token operator">=</span><span class="token variable"><span class="token variable">$(</span><span class="token builtin class-name">cd</span> -P <span class="token punctuation">$(</span>dirname $file<span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token builtin class-name">pwd</span><span class="token variable">)</span></span>
      <span class="token comment">#6. 获取当前文件的名称</span>
      <span class="token assign-left variable">fname</span><span class="token operator">=</span><span class="token variable"><span class="token variable">$(</span><span class="token function">basename</span> $file<span class="token variable">)</span></span>
      <span class="token function">ssh</span> <span class="token variable">$host</span> <span class="token string">"mkdir -p <span class="token variable">$pdir</span>"</span>
      <span class="token function">rsync</span> -av <span class="token variable">$pdir</span>/<span class="token variable">$fname</span> <span class="token variable">$host</span><span class="token builtin class-name">:</span><span class="token variable">$pdir</span>
    <span class="token keyword">else</span>
      <span class="token builtin class-name">echo</span> <span class="token variable">$file</span> does not exists<span class="token operator">!</span>
    <span class="token keyword">fi</span>
  <span class="token keyword">done</span>
<span class="token keyword">done</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>3). 修改文件的执行权限</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"><span class="token function">chmod</span> <span class="token number">777</span> xsync<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>4). 将脚本复制到/bin中，以便全局调用</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"><span class="token function">sudo</span> <span class="token function">cp</span> xsync /bin/<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>5). 测试脚本</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">xsync test.txt<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<h4 id="2-3-分布式集群规划"><a href="#2-3-分布式集群规划" class="headerlink" title="2.3 分布式集群规划"></a>2.3 分布式集群规划</h4><p>​    <strong>概述：</strong>接下来我们就要搭建Hadoop集群了，在操作之前一定要有具体的集群规划，集群规划其实就是把Hadoop中的核心组件如何安排到每台机器上。</p>
<p>​    <strong>分析：</strong> 通过前面的介绍我们知道 在Hadoop集群当中先要考虑数据的存储以及资源调度的安排。那就会涉及到NameNode 、ResourceManager 、SecondaryNameNode 、DataNode 、 NodeManager。如何把这些组件分布到每一台机器上，就得合理分析一下。</p>
<p>NameNode 、ResourceManager 、SecondaryNameNode 这三个组件相对来说比较耗费资源，我们通常把他们分布到不同的机器上。所以三台机器每一台分布一个。</p>
<p>DataNode是具体存储数据的，因为三台机器都具备存储空间，那每一台都分布一个DataNode</p>
<p>NodeManager是负责每一台机器的资源的管理，因此三台机器每一台也分布一个NodeManager</p>
<p><strong>hadoop102            NameNode                         DataNode              NodeManager</strong></p>
<p><strong>hadoop103            ResourceManager              DataNode              NodeManager</strong></p>
<p><strong>hadoop104            SecondaryNameNode      DataNode                NodeManager</strong></p>
<h4 id="2-4-搭建完全集群"><a href="#2-4-搭建完全集群" class="headerlink" title="2.4 搭建完全集群"></a>2.4 搭建完全集群</h4><h5 id="1-先删除每个节点中hadoop安装目录下的-data-和-logs目录，如果是最新解压配置的hadoop集群，并没有这两个目录就不需要进行删除这步。"><a href="#1-先删除每个节点中hadoop安装目录下的-data-和-logs目录，如果是最新解压配置的hadoop集群，并没有这两个目录就不需要进行删除这步。" class="headerlink" title="1.先删除每个节点中hadoop安装目录下的 data 和 logs目录，如果是最新解压配置的hadoop集群，并没有这两个目录就不需要进行删除这步。"></a><strong>1.先删除每个节点中hadoop安装目录下的 data 和 logs目录，如果是最新解压配置的hadoop集群，并没有这两个目录就不需要进行删除这步。</strong></h5><h5 id="2-在hadoop-env-sh文件中，配置JAVA-HOME-的环境变量，这是因为Hadoop运行的时候需要java的环境变量。"><a href="#2-在hadoop-env-sh文件中，配置JAVA-HOME-的环境变量，这是因为Hadoop运行的时候需要java的环境变量。" class="headerlink" title="2.在hadoop-env.sh文件中，配置JAVA_HOME 的环境变量，这是因为Hadoop运行的时候需要java的环境变量。"></a><strong>2.在hadoop-env.sh文件中，配置JAVA_HOME 的环境变量，这是因为Hadoop运行的时候需要java的环境变量。</strong></h5><h5 id="3-配置Hadoop的4大核心配置文件"><a href="#3-配置Hadoop的4大核心配置文件" class="headerlink" title="3.配置Hadoop的4大核心配置文件"></a><strong>3.配置Hadoop的4大核心配置文件</strong></h5><ul>
<li><p><strong>core-site.xml</strong>  这个是hadoop总的核心配置文件，集群加载启动的时候首先会加载解析此配置文件，具体配置内容如下：</p>
 <pre class="line-numbers language-xml" data-language="xml"><code class="language-xml"><span class="token prolog">&lt;?xml version="1.0" encoding="UTF-8"?></span>
<span class="token prolog">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?></span>
<span class="token comment">&lt;!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
--></span>

<span class="token comment">&lt;!-- Put site-specific property overrides in this file. --></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>
    <span class="token comment">&lt;!--cmeNode的地址 --></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>fs.defaultFS<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>hdfs://hadoop102:9820<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
    <span class="token comment">&lt;!-- 指定hadoop数据的存储目录 --></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hadoop.data.dir<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>/opt/module/hadoop-3.1.3/data<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>

    
    <span class="token comment">&lt;!-- 配置该atguigu(superUser)允许通过代理访问的主机节点 --></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hadoop.proxyuser.atguigu.hosts<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>*<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
    <span class="token comment">&lt;!-- 配置该atguigu(superUser)允许通过代理用户所属组 --></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hadoop.proxyuser.atguigu.groups<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>*<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
    <span class="token comment">&lt;!-- 配置该atguigu(superUser)允许通过代理的用户--></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hadoop.proxyuser.atguigu.users<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>*<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span>
   <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li>
<li><p><strong>hdfs-site.xml</strong> 这个是hdfs的核心配置文件，具体配置内容如下：</p>
 <pre class="line-numbers language-xml" data-language="xml"><code class="language-xml"> <span class="token prolog">&lt;?xml version="1.0" encoding="UTF-8"?></span>
 <span class="token prolog">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?></span>
 <span class="token comment">&lt;!--
   Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
--></span>

<span class="token comment">&lt;!-- Put site-specific property overrides in this file. --></span>

<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>
    <span class="token comment">&lt;!--指定NameNode数据的存储目录--></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.namenode.name.dir<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>file://$&#123;hadoop.data.dir&#125;/name<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
    <span class="token comment">&lt;!--指定DataNode数据的存储目录--></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.datanode.data.dir<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>file://$&#123;hadoop.data.dir&#125;/data<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
    <span class="token comment">&lt;!--指定SecondaryNameNode数据的存储目录--></span>
     <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.namenode.checkpoint.dir<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>file://$&#123;hadoop.data.dir&#125;/namesecondary<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
    <span class="token comment">&lt;!-- nn web端访问地址--></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.namenode.http-address<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>hadoop102:9870<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
    <span class="token comment">&lt;!-- 2nn web端访问地址--></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.namenode.secondary.http-address<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>hadoop104:9868<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li>
<li><p>yarn-site.xml 这个是Yarn的核心配置文件,具体内容如下：</p>
 <pre class="line-numbers language-xml" data-language="xml"><code class="language-xml"><span class="token prolog">&lt;?xml version="1.0"?></span>
<span class="token comment">&lt;!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
--></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>

<span class="token comment">&lt;!-- Site specific YARN configuration properties --></span>
        <span class="token comment">&lt;!-- 指定MR走shuffle --></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.nodemanager.aux-services<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>mapreduce_shuffle<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
<span class="token comment">&lt;!-- 指定ResourceManager的地址--></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.resourcemanager.hostname<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>hadoop103<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
<span class="token comment">&lt;!-- 环境变量的继承 --></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.nodemanager.env-whitelist<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>                         <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
<span class="token comment">&lt;!-- yarn容器允许分配的最大最小内存 --></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.minimum-allocation-mb<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>512<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.maximum-allocation-mb<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>4096<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
<span class="token comment">&lt;!-- yarn容器允许管理的物理内存大小 --></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.nodemanager.resource.memory-mb<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>4096<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
<span class="token comment">&lt;!-- 关闭yarn对物理内存和虚拟内存的限制检查 --></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.nodemanager.pmem-check-enabled<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>false<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.nodemanager.vmem-check-enabled<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>false<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>	
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li>
<li><p>mapred-site.xml  这是MapReduce配置文件，配置内容如下：</p>
 <pre class="line-numbers language-xml" data-language="xml"><code class="language-xml"><span class="token prolog">&lt;?xml version="1.0"?></span>
<span class="token prolog">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?></span>
<span class="token comment">&lt;!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
--></span>

<span class="token comment">&lt;!-- Put site-specific property overrides in this file. --></span>

<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>
<span class="token comment">&lt;!-- 指定MapReduce程序运行在Yarn上 --></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>mapreduce.framework.name<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>yarn<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h5 id="4-启动HDFS，单独启动每一台机器上的组件（重点）"><a href="#4-启动HDFS，单独启动每一台机器上的组件（重点）" class="headerlink" title="4. 启动HDFS，单独启动每一台机器上的组件（重点）"></a><strong>4. 启动HDFS，单独启动每一台机器上的组件（重点）</strong></h5></li>
<li><ol>
<li><p>因为hdfs分布式文件系统本质是一个文件系统，固然在使用之前要进行格式化，那么在哪台机器格式化呢，就是hdfs的大哥NameNode所在的节点进行格式化，格式化命令如下：</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"><span class="token punctuation">[</span>atguigu@hadoop102 hadoop<span class="token punctuation">]</span>$ hdfs namenode -format<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li>
</ol>
</li>
<li><ol start="2">
<li>启动HDFS文件系统，注意：我们现在是每台机器逐个启动所以一定要清晰之前定的集群规划的方案，现在要启动HDFS文件系统，而HDFS系统又包含 NameNode、SecondaryNameNode、DataNode，这三大组件有分别被规划在 NameNode在hadoop102、SecondaryNameNode在hadoop104、以及每一台机器上都有DataNode，所以启动流程如下：</li>
</ol>
<ul>
<li><p>在hadoop102上 启动NameNode 命令如下：</p>
 <pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"><span class="token punctuation">[</span>atguigu@hadoop102 hadoop<span class="token punctuation">]</span>$ hdfs --daemon start namenode<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li>
<li><p>在hadoop104上 启动SecondaryNameNode 命令如下：</p>
 <pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"><span class="token punctuation">[</span>atguigu@hadoop104 ~<span class="token punctuation">]</span>$ hdfs --daemon start secondarynamenode<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li>
<li><p>在hadoop102 hadoop103 hadoop104 都启动DataNode 命令如下：</p>
 <pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"><span class="token punctuation">[</span>atguigu@hadoop102 hadoop<span class="token punctuation">]</span>$ hdfs --daemon start datanode
<span class="token punctuation">[</span>atguigu@hadoop103 hadoop<span class="token punctuation">]</span>$ hdfs --daemon start namenode
<span class="token punctuation">[</span>atguigu@hadoop104 hadoop<span class="token punctuation">]</span>$ hdfs --daemon start namenode<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li>
</ul>
</li>
<li><ol start="3">
<li>检测hdfs是否启动成功 Web端查看HDFS的NameNode</li>
</ol>
</li>
</ul>
<p>（a）浏览器中输入：<a target="_blank" rel="noopener" href="http://hadoop102:9870/">http://hadoop102:9870</a></p>
<p>（b）查看HDFS上存储的数据信息</p>
<p><img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_21_01/hdfs%E9%A1%B5%E9%9D%A2%E5%9B%BE.png" alt="hdfs的页面图"></p>
<h5 id="5-启动Yarn"><a href="#5-启动Yarn" class="headerlink" title="5. 启动Yarn"></a><strong>5. 启动Yarn</strong></h5><p>​        根据集群规划，Yarn的ResourceManager我们分布在hadoop103上，NodeManager每一台机器上都存在所以启动流程如下：</p>
<ul>
<li><p>1). 在hadoop103 启动resourcemanager 命令如下：</p>
 <pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"><span class="token punctuation">[</span>atguigu@hadoop103 hadoop<span class="token punctuation">]</span>$ <span class="token function">yarn</span> --daemon start resourcemanager<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li>
<li><p>2). 分别在hadoop102、hadoop103、hadoop104 启动nodemanager 命令如下：</p>
 <pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"><span class="token punctuation">[</span>atguigu@hadoop102 hadoop<span class="token punctuation">]</span>$ hdfs --daemon start nodemanager
<span class="token punctuation">[</span>atguigu@hadoop103 hadoop<span class="token punctuation">]</span>$ hdfs --daemon start nodemanager
<span class="token punctuation">[</span>atguigu@hadoop104 hadoop<span class="token punctuation">]</span>$ hdfs --daemon start nodemanager<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li>
<li><p>3). 检测Yarn是否启动成功 Web端查看YARN的ResourceManager</p>
<p> （a）浏览器中输入：<a target="_blank" rel="noopener" href="http://hadoop103:8088/">http://hadoop103:8088</a></p>
<p> （b）查看YARN上运行的Job信息</p>
<p> <img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_21_01/Yarn%E9%A1%B5%E9%9D%A2%E4%BF%A1%E6%81%AF.png" alt="yarn页面信息"></p>
</li>
</ul>
<h5 id="6-简单测试使用集群"><a href="#6-简单测试使用集群" class="headerlink" title="6.简单测试使用集群"></a>6.简单测试使用集群</h5><p>​    <strong>前言：</strong> 接下来简单测试试用一下我们搭建好的集群环境，操作的目标就是在HDFS 文件系统上上传文件以及运行一下简单的MapReduce程序即可！但是这里需要我们注意的一个 <strong>问题就是 HDFS系统所指向的物理路径究竟是哪 一会应该往哪个路径下上传文件！</strong></p>
<p>​    <strong>问题一：HDFS文件系统怎么定位？</strong></p>
<p>​    首先我们清楚，当前集群是运行在Linux上的，而Linux又是在Windows系统中的通过虚拟机的方式运行的，所以HDFS文件系统本质上也是占用了我们当前电脑硬盘的一部分，通过hadoop体系为HDFS分配出的一块存储空间。但是一定要注意它具有独立性，是由Hadoop独立来管理的。</p>
<p>​    <strong>问题二：在操作HDFS文件系统的时候如何理解它的输入路径和输出路径？</strong></p>
<p>​    Hadoop如何识别是Linux路径还是HDFS路径呢？本质上还得看 Hadoop的核心配置文件的fs.defaultFS的配置信息。</p>
<p>当前我们搭建的集群配置如下：</p>
<pre class="line-numbers language-xml" data-language="xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>fs.defaultFS<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>hdfs://hadoop102:9820<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<p>参考官网默认配置如下：</p>
<pre class="line-numbers language-xml" data-language="xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>fs.defaultFS<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>file:///<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<p><strong>对比分析：</strong></p>
<p>1). Hadoop的fs.defaultFS的默认配置是file:///  如果解析的是这个配置，file:/// 本质上所表示的就是Linux本地路径，那么在操作中写输入输出就按照Linux的规则正常写就行，例如编写执行wordcount程序的命令如下：</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"><span class="token punctuation">[</span>atguigu@hadoop102 hadoop<span class="token punctuation">]</span>$ hadoop jar share/hadoop/mapredece/hadoop-mapreduce-ecanples.jar wordcount wcinput/wc.input wcoutput<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>2). 如果我们自己修改了core-site.xml 核心配置文件配置 fs.defaultFS 的值为hdfs://hadoop102:9820 那么意味着在解析输入输出路径的时候指向的是HDFS系统维护的目录结构 在HDFS系统底层维护的路径是  <strong>/user/atguigu/wcinput</strong> 所以如果是在这个情况下我们要操作wordcount程序就应该这么写了 命令如下：</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"><span class="token punctuation">[</span>atguigu@hadoop102 hadoop<span class="token punctuation">]</span>$ hadoop jar share/hadoop/mapredece/hadoop-mapreduce-ecanples.jar wordcount /user/atguigu/wcinput/wc.input /user/atguigu/wcoutput<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p><strong>OK! 有了上面的内容作为支撑，下面我们就正式对Hadoop集群进行简单测试操作！！！</strong></p>
<h6 id="6-1-在HDFS中创建一个目录-user-atguigu-input-目录"><a href="#6-1-在HDFS中创建一个目录-user-atguigu-input-目录" class="headerlink" title="6.1 在HDFS中创建一个目录 /user/atguigu/input 目录"></a>6.1 在HDFS中创建一个目录 /user/atguigu/input 目录</h6><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"><span class="token punctuation">[</span>atguigu@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ hdfs dfs -rm -R /user/atguigu/input<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<h6 id="6-2-将hadoop安装目下的wcinput-wc-input-文件上传到HDFS文件系统上的-user-atguigu-input-目录下"><a href="#6-2-将hadoop安装目下的wcinput-wc-input-文件上传到HDFS文件系统上的-user-atguigu-input-目录下" class="headerlink" title="6.2 将hadoop安装目下的wcinput/wc.input 文件上传到HDFS文件系统上的 /user/atguigu/input 目录下"></a>6.2 将hadoop安装目下的wcinput/wc.input 文件上传到HDFS文件系统上的 /user/atguigu/input 目录下</h6><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"><span class="token punctuation">[</span>atguigu@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ hdfs dfs -put wciput/wc.input /user/atguigu/input<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<h6 id="6-3-如何在HDFS上查看具体存储的文件"><a href="#6-3-如何在HDFS上查看具体存储的文件" class="headerlink" title="6.3 如何在HDFS上查看具体存储的文件"></a>6.3 如何在HDFS上查看具体存储的文件</h6><p>DataNode的存储目录：</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"><span class="token punctuation">[</span>atguigu@hadoop103 hadoop-3.1.3<span class="token punctuation">]</span>$ <span class="token builtin class-name">cd</span> data/data/current/BP-1528516923-192.168.2.102-1597943910514/current/finalized/subdir0/subdir0/<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<h6 id="6-4-测试Yarn是否能正常使用-还是以Mapreduce的wordcount程序为例"><a href="#6-4-测试Yarn是否能正常使用-还是以Mapreduce的wordcount程序为例" class="headerlink" title="6.4 测试Yarn是否能正常使用 还是以Mapreduce的wordcount程序为例"></a>6.4 测试Yarn是否能正常使用 还是以Mapreduce的wordcount程序为例</h6><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"><span class="token punctuation">[</span>atguigu@hadoop104 hadoop-3.1.3<span class="token punctuation">]</span>$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /user/atguigu/input /user/atguigu/output<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>6.5 在hdfs上面查看执行后的结果</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"><span class="token punctuation">[</span>atguigu@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ hdfs dfs -cat /user/atguigu/output/part-r-00000<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<h5 id="7-SSH免密登录"><a href="#7-SSH免密登录" class="headerlink" title="7. SSH免密登录"></a>7. SSH免密登录</h5><p><strong>存在的问题：</strong> 集群启动和关闭，目前我们都是通过单点操作完成的，这样很不方便，于是就考虑能不能在一台机器上就能搞定集群的启动和关闭？</p>
<p><strong>分析：</strong></p>
<p>参照之前的脚本分发的思路，我们可以编写一个集群启动和关闭的脚本，就是把哪些在每一台机器上输入的命令封装到一个脚本中，然后通过执行脚本来实现集群启动关闭的目的。</p>
<p><strong>脚本的大概思路：</strong></p>
<p>​    登录到hadoop102  启动/关闭 namenode</p>
<p>​    登录到hadoop104  启动/关闭 secondarynamenode</p>
<p>​    登录到hadoop102   hadoop103   hadoop104  启动/关闭 datanode</p>
<p>​    登录到hadoop103 启动/关闭 resourcemanager </p>
<p>​    登录到hadoop102 hadoop103 hadoop104  启动/关闭 nodemanager</p>
<p><strong>如何登录远程的机器：</strong></p>
<p>语法：ssh ip/主机名 </p>
<p><strong>无密钥配置：</strong> 单纯的 ssh 命令操作，虽然可以只在一台机器操作了但是操作步骤较多，而且登录的时候每次都需要输入密码，我们接下来要做到免密登录+脚本控制</p>
<p><strong>免密登录的原理：</strong></p>
<p><img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_21_01/%E5%85%8D%E5%AF%86%E7%99%BB%E5%BD%95%E5%8E%9F%E7%90%86.png" alt="免密登录原理"></p>
<p><strong>实现步骤：</strong></p>
<p>1). 生成公钥和私钥：</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"><span class="token punctuation">[</span>atguigu@hadoop102 .ssh<span class="token punctuation">]</span>$ ssh-keygen -t rsa<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>然后敲（四次回车），就会生成两个文件id_rsa（私钥）、id_rsa.pub（公钥）</p>
<p>2). 将公钥拷贝到要免密登录的目标机器上</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"><span class="token punctuation">[</span>atguigu@hadoop102 .ssh<span class="token punctuation">]</span>$ ssh-copy-id hadoop102
<span class="token punctuation">[</span>atguigu@hadoop102 .ssh<span class="token punctuation">]</span>$ ssh-copy-id hadoop103
<span class="token punctuation">[</span>atguigu@hadoop102 .ssh<span class="token punctuation">]</span>$ ssh-copy-id hadoop104<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<p>3). 注意，集群机器的配置</p>
<ul>
<li><p> 还需要在hadoop103上采用atguigu账号配置一下无密登录到hadoop102、hadoop103、hadoop104服务器上。</p>
</li>
<li><p> 还需要在hadoop104上采用atguigu账号配置一下无密登录到hadoop102、hadoop103、hadoop104服务器上。</p>
</li>
<li><p> 还需要在hadoop102上采用atguigu账号，配置一下无密登录到hadoop102、hadoop103、hadoop104；</p>
</li>
</ul>
<p>4).  .ssh文件夹下（~/.ssh）的文件功能解释</p>
<table>
<thead>
<tr>
<th>known_hosts</th>
<th>记录ssh访问过计算机的公钥(public  key)</th>
</tr>
</thead>
<tbody><tr>
<td>id_rsa</td>
<td>生成的私钥</td>
</tr>
<tr>
<td>id_rsa.pub</td>
<td>生成的公钥</td>
</tr>
<tr>
<td>authorized_keys</td>
<td>存放授权过的无密登录服务器公钥</td>
</tr>
</tbody></table>
<h5 id="8-集群的群起操作"><a href="#8-集群的群起操作" class="headerlink" title="8.集群的群起操作"></a>8.集群的群起操作</h5><p>​    当配置过了ssh免密登录，就可以对hadoop进行群起了（多台机器通过脚本一起启动），群起的脚本hadoop已经帮我们内置好了直接使用即可！但是要最终完成群起操作我们必须让启动/关闭脚本知道 NameNode  SecondaryNameNode  DataNode ResourceManager  NodeManager都在哪一台机器上分配，这个怎么做到呢？这个是由  hadoop安装目录下的 etc/hadoop/workers 配置文件来控制。</p>
<ul>
<li> 配置 workers 文件，内容如下：</li>
</ul>
<pre class="line-numbers language-mark" data-language="mark"><code class="language-mark">hadoop102
hadoop103
hadoop104<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<p><strong>注意：</strong>该文件中添加的内容结尾不允许有空格，文件中不允许有空行。</p>
<ul>
<li><p>启动集群</p>
<p>1). <strong>如果集群是第一次启动</strong>，需要在hadoop102节点格式化NameNode（注意格式化NameNode，会产生新的集群id，导致NameNode和DataNode的集群id不一致，集群找不到已往数据。如果集群在运行过程中报错，需要重新格式化NameNode的话，一定要先停止namenode和datanode进程，并且要删除所有机器的data和logs目录，然后再进行格式化。</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"><span class="token punctuation">[</span>atguigu@hadoop102 ~<span class="token punctuation">]</span>$ hdfs namenode -format<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>2). 启动HDFS</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"><span class="token punctuation">[</span>atguigu@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ sbin/start-dfs.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>3). 在配置了ResourceManager的节点（hadoop103）启动YARN</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"><span class="token punctuation">[</span>atguigu@hadoop103 hadoop-3.1.3<span class="token punctuation">]</span>$ sbin/start-yarn.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<h5 id="9-群起群停脚本的编写"><a href="#9-群起群停脚本的编写" class="headerlink" title="9.群起群停脚本的编写"></a>9.群起群停脚本的编写</h5></li>
</ul>
<p>​    上面我们已经完成对集群的群起，但是还不够完美，我们操作执行了两个脚本才启动了hdfs和yarn，虽然hadoop也给我们提供了start-all.sh 脚本，但是通常开发中不建议使用，因为start-all.sh脚本启动的话会默认启动一些不必要的组件。我们想更加完美的群起 只执行一个脚本就能把hdfs和yarn都启动或者停止。接下来我们自己封装一个脚本来实现，步骤如下：</p>
<p>1). 进入到/home/atguigu/bin目录下创建一个<strong>群起/群停</strong>脚本，这样操作为了在任何位置都能执行脚本</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"><span class="token punctuation">[</span>atguigu@hadoop102 ~<span class="token punctuation">]</span>$ <span class="token builtin class-name">cd</span> /home/atguigu/bin
<span class="token punctuation">[</span>atguigu@hadoop102 ~<span class="token punctuation">]</span>$ <span class="token function">vim</span> mycluster.sh<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p>2). 编写脚本内容：</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"><span class="token shebang important">#!/bin/bash</span>
<span class="token keyword">if</span> <span class="token punctuation">[</span> <span class="token variable">$#</span> -lt <span class="token number">1</span> <span class="token punctuation">]</span>
<span class="token keyword">then</span>
         <span class="token builtin class-name">echo</span> <span class="token string">"No Args Input..."</span>
         <span class="token builtin class-name">exit</span>
<span class="token keyword">fi</span>
<span class="token keyword">case</span> <span class="token variable">$1</span> <span class="token keyword">in</span>
<span class="token string">"start"</span><span class="token punctuation">)</span>
         <span class="token builtin class-name">echo</span> <span class="token string">"==================START HDFS==================="</span> 
         <span class="token function">ssh</span> hadoop102 /opt/module/hadoop-3.1.3/sbin/start-dfs.sh
         <span class="token builtin class-name">echo</span> <span class="token string">"==================START YARN==================="</span>
         <span class="token function">ssh</span> hadoop103 /opt/module/hadoop-3.1.3/sbin/start-yarn.sh
<span class="token punctuation">;</span><span class="token punctuation">;</span>
<span class="token string">"stop"</span><span class="token punctuation">)</span>
         <span class="token builtin class-name">echo</span> <span class="token string">"==================STOP YARN==================="</span>
         <span class="token function">ssh</span> hadoop103 /opt/module/hadoop-3.1.3/sbin/stop-yarn.sh
         <span class="token builtin class-name">echo</span> <span class="token string">"==================STOP HDFS==================="</span> 
         <span class="token function">ssh</span> hadoop102 /opt/module/hadoop-3.1.3/sbin/stop-dfs.sh

<span class="token punctuation">;</span><span class="token punctuation">;</span>
*<span class="token punctuation">)</span>
  <span class="token builtin class-name">echo</span> <span class="token string">"Input Args Error!!!!"</span>
<span class="token punctuation">;</span><span class="token punctuation">;</span>
<span class="token keyword">esac</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>3). 保存后退出，然后赋予脚本执行权限</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"><span class="token punctuation">[</span>atguigu@hadoop102 bin<span class="token punctuation">]</span>$ <span class="token function">chmod</span> <span class="token number">777</span> myhadoop.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>4). 分发/home/atguigu/bin目录，保证自定义脚本在三台机器上都可以使用</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"><span class="token punctuation">[</span>atguigu@hadoop102 ~<span class="token punctuation">]</span>$ xsync /home/atguigu/bin/<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<h5 id="10-编写统一查看jps的脚本"><a href="#10-编写统一查看jps的脚本" class="headerlink" title="10.编写统一查看jps的脚本"></a>10.编写统一查看jps的脚本</h5><p>​    上面我们做了一个频繁的操作，就是总是在每一机器上输入 jps 命令，来查看当前机器的java进程，而且每次输入都是切换到服务器上输入，很麻烦，接下来我们要实现在一台机器就能查看整个集群的java进程。</p>
<p>1). 进入到/home/atguigu/bin目录下创建一个查看jps的脚本</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"><span class="token punctuation">[</span>atguigu@hadoop102 ~<span class="token punctuation">]</span>$ <span class="token builtin class-name">cd</span> /home/atguigu/bin
<span class="token punctuation">[</span>atguigu@hadoop102 ~<span class="token punctuation">]</span>$ <span class="token function">vim</span> jpsall.sh<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p>2). 编辑脚本内容如下：</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"><span class="token shebang important">#!/bin/bash</span>
<span class="token keyword">for</span> <span class="token for-or-select variable">i</span> <span class="token keyword">in</span> hadoop102 hadoop103 hadoop104
<span class="token keyword">do</span>
	<span class="token builtin class-name">echo</span> <span class="token string">"***************<span class="token variable">$i</span> JPS****************"</span>
	<span class="token function">ssh</span> <span class="token variable">$i</span> /opt/module/jkd1.8.0_212/bin/jps
<span class="token keyword">done</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>3).  保存后退出，然后赋予脚本执行权限</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"><span class="token punctuation">[</span>atguigu@hadoop102 bin<span class="token punctuation">]</span>$ <span class="token function">chmod</span> <span class="token number">777</span> jpsall.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>4). 测试</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"><span class="token punctuation">[</span>atguigu@hadoop102 bin<span class="token punctuation">]</span>$ jpsall.sh <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>结果如下：</p>
<p><img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_21_01/hadoop%E5%90%AF%E5%8A%A8%E7%A8%8B%E5%BA%8F%E5%B1%95%E7%A4%BA.png" alt="hadoop进程展示"></p>
<h5 id="11-历史服务器的使用"><a href="#11-历史服务器的使用" class="headerlink" title="11.历史服务器的使用"></a>11.历史服务器的使用</h5><p>​    这一小节主要介绍hadoop的历史服务器的使用！什么是历史服务器呢？举个例子就是我们在YARN上跑的一些job的历史记录，当重启YARN后之前执行过的job任务记录就会消失，hadoop为了更好的追溯和记录这些job执行记录专门提供了一个历史服务器，只要我们在Hadoop中配置了历史服务器那么以后就可以很方便查看执行过的所有job。</p>
<p>1).配置mapred-site.xml</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"><span class="token punctuation">[</span>atguigu@hadoop102 hadoop<span class="token punctuation">]</span>$ <span class="token function">vim</span> mapred-site.xml<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>在该文件里面增加如下配置:</p>
<pre class="line-numbers language-xml" data-language="xml"><code class="language-xml"><span class="token comment">&lt;!-- 历史服务器端地址 --></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>mapreduce.jobhistory.address<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>hadoop102:10020<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>

<span class="token comment">&lt;!-- 历史服务器web端地址 --></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>mapreduce.jobhistory.webapp.address<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>hadoop102:19888<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>2). 分发配置</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"><span class="token punctuation">[</span>atguigu@hadoop102 hadoop<span class="token punctuation">]</span>$ xsync <span class="token variable">$HADOOP_HOME</span>/etc/hadoop/mapred-site.xml<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>3). 在hadoop102启动历史服务器</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"><span class="token punctuation">[</span>atguigu@hadoop102 hadoop<span class="token punctuation">]</span>$ mapred --daemon start historyserver<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>4). 查看历史服务器是否启动</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"><span class="token punctuation">[</span>atguigu@hadoop102 hadoop<span class="token punctuation">]</span>$ jps<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<ol start="5">
<li>web端查看历史服务器的图形化界面</li>
</ol>
<p><a target="_blank" rel="noopener" href="http://hadoop102:19888/jobhistory">http://hadoop102:19888/jobhistory</a></p>
<h5 id="12-配置日志的聚集"><a href="#12-配置日志的聚集" class="headerlink" title="12.配置日志的聚集"></a>12.配置日志的聚集</h5><p>​    本小节主要对hadoop中的日志进行合理性的管理，方便我们更好的查阅。默认情况下 Hadoop作业执行的日志保存在hadoop的安装目录下logs下面。我们可以在linux上直接查看，但是这样操作不够人性化，查阅起来也比较麻烦。所以我们可以在执行job任务的时候产生日志后，让它自动的保存到hdfs系统中，这样就可以在网页中通过访问HDFS系统的web端地址来查看日志了！如果想完成上述操作需要我们进行以下几步配置和操作。</p>
<p>1）配置yarn-site.xml</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"><span class="token punctuation">[</span>atguigu@hadoop102 hadoop<span class="token punctuation">]</span>$ <span class="token function">vim</span> yarn-site.xml<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>内容如下：</p>
<pre class="line-numbers language-xml" data-language="xml"><code class="language-xml"><span class="token comment">&lt;!-- 开启日志聚集功能 --></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.log-aggregation-enable<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>true<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
<span class="token comment">&lt;!-- 设置日志聚集服务器地址 --></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>  
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.log.server.url<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>  
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>http://hadoop102:19888/jobhistory/logs<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
<span class="token comment">&lt;!-- 设置日志保留时间为7天 --></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.log-aggregation.retain-seconds<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>604800<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>2）分发配置</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"><span class="token punctuation">[</span>atguigu@hadoop102 hadoop<span class="token punctuation">]</span>$ xsync <span class="token variable">$HADOOP_HOME</span>/etc/hadoop/yarn-site.xml<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>3）关闭NodeManager、ResourceManager和HistoryServer</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"><span class="token punctuation">[</span>atguigu@hadoop103 ~<span class="token punctuation">]</span>$ stop-yarn.sh
<span class="token punctuation">[</span>atguigu@hadoop102 ~<span class="token punctuation">]</span>$ mapred --daemon stop historyserver<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p>4）启动NodeManager 、ResourceManage和HistoryServer </p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"><span class="token punctuation">[</span>atguigu@hadoop103 ~<span class="token punctuation">]</span>$ start-yarn.sh
<span class="token punctuation">[</span>atguigu@hadoop102 ~<span class="token punctuation">]</span>$ mapred --daemon start historyserver<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p>5）执行wordcount程序</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"><span class="token punctuation">[</span>atguigu@hadoop102 ~<span class="token punctuation">]</span>$ hadoop jar  <span class="token variable">$HADOOP_HOME</span>/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /input /output<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>6）Web端查看日志</p>
<p>​    <a target="_blank" rel="noopener" href="http://hadoop102:19888/jobhistory">http://hadoop102:19888/jobhistory</a></p>
<p><img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_21_01/web%E6%9F%A5%E7%9C%8B%E5%8E%86%E5%8F%B2%E6%9C%8D%E5%8A%A1%E5%99%A8.png" alt="web端查看历史服务器"></p>
<p><img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_21_01/jobManager.png" alt="job页面"></p>
<p><img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_21_01/hadoop%E5%8E%86%E5%8F%B2%E6%9C%8D%E5%8A%A1%E5%99%A8.png" alt="历史服务器"></p>
<h5 id="13-集群时间同步"><a href="#13-集群时间同步" class="headerlink" title="13. 集群时间同步"></a>13. 集群时间同步</h5><p>​    本小节主要操作在集群环境下，每一台服务器之间的时间同步。时间同步是很有必要的，因为在多台机器协同工作的时候，必然要求时间统一 要不然就会出问题。以下内容只要求大致了解 这项工作一般在运维的范畴。</p>
<p><strong>1）时间服务器配置(必须root用户</strong>)</p>
<p>（0）查看所有节点ntpd服务状态和开机自启动状态</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"><span class="token punctuation">[</span>atguigu@hadoop102 ~<span class="token punctuation">]</span>$ <span class="token function">sudo</span> systemctl status ntpd
<span class="token punctuation">[</span>atguigu@hadoop102 ~<span class="token punctuation">]</span>$ <span class="token function">sudo</span> systemctl is-enabled ntpd<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p>（1）在所有节点关闭ntpd服务和自启动</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"><span class="token punctuation">[</span>atguigu@hadoop102 ~<span class="token punctuation">]</span>$ <span class="token function">sudo</span> systemctl stop ntpd
<span class="token punctuation">[</span>atguigu@hadoop102 ~<span class="token punctuation">]</span>$ <span class="token function">sudo</span> systemctl disable ntpd<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p>（2）修改hadoop102的ntp.conf配置文件</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"><span class="token punctuation">[</span>atguigu@hadoop102 ~<span class="token punctuation">]</span>$ <span class="token function">sudo</span> <span class="token function">vim</span> /etc/ntp.conf<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>修改内容如下:</p>
<p>​    a）修改1（授权192.168.1.0-192.168.1.255网段上的所有机器可以从这台机器上查询和同步时间）</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"><span class="token comment">#restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap</span>
改为（就是把注释去掉）：
restrict <span class="token number">192.168</span>.1.0 mask <span class="token number">255.255</span>.255.0 nomodify notrap<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<p>​    b）修改2（集群在局域网中，不使用其他互联网上的时间）</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">server <span class="token number">0</span>.centos.pool.ntp.org iburst
server <span class="token number">1</span>.centos.pool.ntp.org iburst
server <span class="token number">2</span>.centos.pool.ntp.org iburst
server <span class="token number">3</span>.centos.pool.ntp.org iburst
改为（都加上注释）：
<span class="token comment">#server 0.centos.pool.ntp.org iburst</span>
<span class="token comment">#server 1.centos.pool.ntp.org iburst</span>
<span class="token comment">#server 2.centos.pool.ntp.org iburst</span>
<span class="token comment">#server 3.centos.pool.ntp.org iburst</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>​    c）添加3（当该节点丢失网络连接，依然可以采用本地时间作为时间服务器为集群中的其他节点提供时间同步）</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">server <span class="token number">127.127</span>.1.0
fudge <span class="token number">127.127</span>.1.0 stratum <span class="token number">10</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p>（3）修改hadoop102的/etc/sysconfig/ntpd 文件</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"><span class="token punctuation">[</span>atguigu@hadoop102 ~<span class="token punctuation">]</span>$ <span class="token function">sudo</span> <span class="token function">vim</span> /etc/sysconfig/ntpd<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>增加内容如下（让硬件时间与系统时间一起同步）</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"><span class="token assign-left variable">SYNC_HWCLOCK</span><span class="token operator">=</span>yes<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>（4）重新启动ntpd服务</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"><span class="token punctuation">[</span>atguigu@hadoop102 ~<span class="token punctuation">]</span>$ <span class="token function">sudo</span> systemctl start ntpd<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>（5）设置ntpd服务开机启动</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"><span class="token punctuation">[</span>atguigu@hadoop102 ~<span class="token punctuation">]</span>$ <span class="token function">sudo</span> systemctl <span class="token builtin class-name">enable</span> ntpd<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>


<p><strong>2）在其他机器进行时间同步操作（必须root用户）</strong></p>
<p>（1）在其他机器配置1分钟与时间服务器同步一次</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"><span class="token punctuation">[</span>atguigu@hadoop103 ~<span class="token punctuation">]</span>$ <span class="token function">sudo</span> <span class="token function">crontab</span> -e<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>编写定时任务如下：</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">*/1 * * * * /usr/sbin/ntpdate hadoop102<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>（2）修改任意机器时间</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"><span class="token punctuation">[</span>atguigu@hadoop103 ~<span class="token punctuation">]</span>$ <span class="token function">sudo</span> <span class="token function">date</span> -s <span class="token string">"2018-8-08 08:08:08"</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>（3）一分钟后查看机器是否与时间服务器同步</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"><span class="token punctuation">[</span>atguigu@hadoop103 ~<span class="token punctuation">]</span>$ <span class="token function">sudo</span> <span class="token function">date</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>


<h2 id="五、Hadoop编译源码"><a href="#五、Hadoop编译源码" class="headerlink" title="五、Hadoop编译源码"></a>五、Hadoop编译源码</h2><h2 id="六、常见错误及解决方案"><a href="#六、常见错误及解决方案" class="headerlink" title="六、常见错误及解决方案"></a>六、常见错误及解决方案</h2>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/about" rel="external nofollow noreferrer">Jinxin Li</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="http://fourgold.github.io/2019/04/16/Hadoop%E7%AC%94%E8%AE%B0%E6%80%BB%E7%BB%93/">http://fourgold.github.io/2019/04/16/Hadoop%E7%AC%94%E8%AE%B0%E6%80%BB%E7%BB%93/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/about" target="_blank">Jinxin Li</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            <span class="chip bg-color">无标签</span>
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <style>
    #reward {
        margin: 40px 0;
        text-align: center;
    }

    #reward .reward-link {
        font-size: 1.4rem;
        line-height: 38px;
    }

    #reward .btn-floating:hover {
        box-shadow: 0 6px 12px rgba(0, 0, 0, 0.2), 0 5px 15px rgba(0, 0, 0, 0.2);
    }

    #rewardModal {
        width: 320px;
        height: 350px;
    }

    #rewardModal .reward-title {
        margin: 15px auto;
        padding-bottom: 5px;
    }

    #rewardModal .modal-content {
        padding: 10px;
    }

    #rewardModal .close {
        position: absolute;
        right: 15px;
        top: 15px;
        color: rgba(0, 0, 0, 0.5);
        font-size: 1.3rem;
        line-height: 20px;
        cursor: pointer;
    }

    #rewardModal .close:hover {
        color: #ef5350;
        transform: scale(1.3);
        -moz-transform:scale(1.3);
        -webkit-transform:scale(1.3);
        -o-transform:scale(1.3);
    }

    #rewardModal .reward-tabs {
        margin: 0 auto;
        width: 210px;
    }

    .reward-tabs .tabs {
        height: 38px;
        margin: 10px auto;
        padding-left: 0;
    }

    .reward-content ul {
        padding-left: 0 !important;
    }

    .reward-tabs .tabs .tab {
        height: 38px;
        line-height: 38px;
    }

    .reward-tabs .tab a {
        color: #fff;
        background-color: #ccc;
    }

    .reward-tabs .tab a:hover {
        background-color: #ccc;
        color: #fff;
    }

    .reward-tabs .wechat-tab .active {
        color: #fff !important;
        background-color: #22AB38 !important;
    }

    .reward-tabs .alipay-tab .active {
        color: #fff !important;
        background-color: #019FE8 !important;
    }

    .reward-tabs .reward-img {
        width: 210px;
        height: 210px;
    }
</style>

<div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">你的赏识是我前进的动力</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="/medias/reward/alipay.jpg" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                    <div id="wechat">
                        <img src="/medias/reward/wechat.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>

            
        </div>
    </div>

    

    

    

    
    <div class="livere-card card" data-aos="fade-up">
    <!-- 来必力City版安装代码 -->
    <div id="lv-container" class="card-content" data-id="city" data-uid="">
        <script type="text/javascript">
            (function (d, s) {
                let j, e = d.getElementsByTagName(s)[0];
                if (typeof LivereTower === 'function') {
                    return;
                }

                j = d.createElement(s);
                j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
                j.async = true;

                e.parentNode.insertBefore(j, e);
            })(document, 'script');
        </script>
        <noscript>为正常使用来必力评论功能请激活JavaScript。</noscript>
    </div>
    <!-- City版安装代码已完成 -->
</div>
    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/2019/06/07/Flink09_CEP/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/8.jpg" class="responsive-img" alt="Flink09_CEP">
                        
                        <span class="card-title">Flink09_CEP</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            对CEP库的API与CEP的时间算法介绍,同时进行简单的CEP尝试
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2019-06-07
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/flink/" class="post-category">
                                    flink
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/flink/">
                        <span class="chip bg-color">flink</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2019/03/07/Flink04_Timestamp%E4%B8%8EWindow/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/10.jpg" class="responsive-img" alt="Flink04_Timestamp与window的实战与源码分析">
                        
                        <span class="card-title">Flink04_Timestamp与window的实战与源码分析</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            里面除了基础对于WaterMark与window的解释外,还有源码架构与实战理解watermark,让你肉眼可见watermark,不过要想看懂还是得下功夫
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2019-03-07
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-user fa-fw"></i>
                            Jinxin Li
                            
                        </span>
                    </div>
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>

<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>


    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('1'),
            headingSelector: 'h1,h2, h3, h4'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h1,h2, h3, h4').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/libs/aplayer/APlayer.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js"></script>

    
    <div class="container row center-align" style="margin-bottom: 0px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2019-2021</span>
            
            <span id="year">2019</span>
            <a href="/about" target="_blank">Jinxin Li</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            <br>
            
            &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                class="white-color">205.6k</span>&nbsp;字
            
            
            
            
            
            
            <span id="busuanzi_container_site_pv">
                |&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv"
                    class="white-color"></span>&nbsp;次
            </span>
            
            
            <span id="busuanzi_container_site_uv">
                |&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv"
                    class="white-color"></span>&nbsp;人
            </span>
            
            <br>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/fourgold" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:799392914@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>







    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=799392914" class="tooltipped" target="_blank" data-tooltip="QQ联系我: 799392914" data-position="top" data-delay="50">
        <i class="fab fa-qq"></i>
    </a>







    <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50">
        <i class="fas fa-rss"></i>
    </a>

</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

	
    

    

    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    

</body>

</html>
