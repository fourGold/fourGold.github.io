<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Jetbrains系列产品重置试用方法</title>
      <link href="2020/12/28/Jetbrains%E7%B3%BB%E5%88%97%E4%BA%A7%E5%93%81%E9%87%8D%E7%BD%AE%E8%AF%95%E7%94%A8%E6%96%B9%E6%B3%95/"/>
      <url>2020/12/28/Jetbrains%E7%B3%BB%E5%88%97%E4%BA%A7%E5%93%81%E9%87%8D%E7%BD%AE%E8%AF%95%E7%94%A8%E6%96%B9%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h2 id="0x0-项目背景"><a href="#0x0-项目背景" class="headerlink" title="0x0. 项目背景"></a>0x0. 项目背景</h2><p>Jetbrains家的产品有一个很良心的地方，他会允许你试用<code>30</code>天（这个数字写死在代码里了）以评估是否你真的需要为它而付费。<br>但很多时候会出现一种情况：<strong>IDE并不能按照我们实际的试用时间来计算。</strong><br>我举个例子：如果我们开始了试用，然后媳妇生孩子要你回去陪产！陪产时我们并无空闲对IDE试用评估，它依旧算试用时间。（只是举个例子，或许你并没有女朋友）<br>发现了吗？你未能真的有<code>30</code>天来对它进行全面的试用评估，你甚至无法作出是否付费的决定。此时你会想要延长试用时间，然而Jetbrains并未提供相关功能，该怎么办？</p><p>事实上有一款插件可以实现这个功能，你或许可以用它来重置一下试用时间。<strong>但切记不要无休止的一直试用，这并不是这个插件的本意！</strong></p><h2 id="0x1-如何安装"><a href="#0x1-如何安装" class="headerlink" title="0x1. 如何安装"></a>0x1. 如何安装</h2><h3 id="1-插件市场安装："><a href="#1-插件市场安装：" class="headerlink" title="1). 插件市场安装："></a>1). 插件市场安装：</h3><ul><li>在<code>Settings/Preferences...</code> -&gt; <code>Plugins</code> 内手动添加第三方插件仓库地址：<code>https://plugins.zhile.io</code></li><li>搜索：<code>IDE Eval Reset</code>插件进行安装。如果搜索不到请注意是否做好了上一步？网络是否通畅？</li><li>插件会提示安装成功。</li></ul><h3 id="2-下载安装："><a href="#2-下载安装：" class="headerlink" title="2). 下载安装："></a>2). 下载安装：</h3><ul><li>点击这个<a href="https://plugins.zhile.io/files/ide-eval-resetter-2.1.6.zip">链接(v2.1.6)</a>下载插件的<code>zip</code>包（macOS可能会自动解压，然后把<code>zip</code>包丢进回收站）</li><li>通常可以直接把<code>zip</code>包拖进IDE的窗口来进行插件的安装。如果无法拖动安装，你可以在<code>Settings/Preferences...</code> -&gt; <code>Plugins</code> 里手动安装插件（<code>Install Plugin From Disk...</code>）</li><li>插件会提示安装成功。</li></ul><h2 id="0x2-如何使用"><a href="#0x2-如何使用" class="headerlink" title="0x2. 如何使用"></a>0x2. 如何使用</h2><ul><li>一般来说，在IDE窗口切出去或切回来时（窗口失去/得到焦点）会触发事件，检测是否长时间（<code>25</code>天）没有重置，给通知让你选择。（初次安装因为无法获取上次重置时间，会直接给予提示）</li><li>也可以手动唤出插件的主界面：<ul><li>如果IDE没有打开项目，在<code>Welcome</code>界面点击菜单：<code>Get Help</code> -&gt; <code>Eval Reset</code></li><li>如果IDE打开了项目，点击菜单：<code>Help</code> -&gt; <code>Eval Reset</code></li></ul></li><li>唤出的插件主界面中包含了一些显示信息，<code>2</code>个按钮，<code>1</code>个勾选项：<ul><li>按钮：<code>Reload</code> 用来刷新界面上的显示信息。</li><li>按钮：<code>Reset</code> 点击会询问是否重置试用信息并<strong>重启IDE</strong>。选择<code>Yes</code>则执行重置操作并<strong>重启IDE生效</strong>，选择<code>No</code>则什么也不做。（此为手动重置方式）</li><li>勾选项：<code>Auto reset before per restart</code> 如果勾选了，则自勾选后<strong>每次重启/退出IDE时会自动重置试用信息</strong>，你无需做额外的事情。（此为自动重置方式）</li></ul></li></ul><h2 id="0x3-如何更新"><a href="#0x3-如何更新" class="headerlink" title="0x3. 如何更新"></a>0x3. 如何更新</h2><h3 id="1-插件更新机制（推荐）："><a href="#1-插件更新机制（推荐）：" class="headerlink" title="1). 插件更新机制（推荐）："></a>1). 插件更新机制（推荐）：</h3><ul><li>IDE会自行检测其自身和所安装插件的更新并给予提示。如果本插件有更新，你会收到提示看到更新日志，自行选择是否更新。</li><li>点击IDE的<code>Check for Updates...</code> 菜单手动检测IDE和所安装插件的更新。如果本插件有更新，你会收到提示看到更新日志，自行选择是否更新。</li><li>插件更新可能会需要<strong>重启IDE</strong>。</li></ul><h3 id="2-手动更新："><a href="#2-手动更新：" class="headerlink" title="2). 手动更新："></a>2). 手动更新：</h3><ul><li>从本页面下载最新的插件<code>zip</code>包安装更新。参考本文：<code>下载安装</code>小节。</li><li>插件更新需要<strong>重启IDE</strong>。</li></ul><h2 id="0x4-一些说明"><a href="#0x4-一些说明" class="headerlink" title="0x4. 一些说明"></a>0x4. 一些说明</h2><ul><li><p>本插件默认不会显示其主界面，如果你需要，参考本文：<code>如何使用</code>小节。</p></li><li><p>市场付费插件的试用信息也会<strong>一并重置</strong>。</p></li><li><p>对于某些付费插件（如:<code>Iedis 2</code>,<code>MinBatis</code>）来说，你可能需要去取掉<code>javaagent</code></p><p>配置（如果有）后重启IDE：</p><ul><li>如果IDE没有打开项目，在<code>Welcome</code>界面点击菜单：<code>Configure</code> -&gt; <code>Edit Custom VM Options...</code> -&gt; 移除 <code>-javaagent:</code> 开头的行。</li><li>如果IDE打开了项目，点击菜单：<code>Help</code> -&gt; <code>Edit Custom VM Options...</code> -&gt; 移除 <code>-javaagent:</code> 开头的行。</li></ul></li><li><p>重置需要<strong>重启IDE生效</strong>！</p></li><li><p>重置后并不弹出<code>Licenses</code>对话框让你选择输入License或试用，这和之前的重置脚本/插件不同（省去这烦人的一步）。</p></li><li><p>如果长达<code>25</code>天不曾有任何重置动作，IDE会有<strong>通知询问</strong>你是否进行重置。</p></li><li><p>如果勾选：<code>Auto reset before per restart</code> ，重置是静默无感知的。</p></li><li><p>简单来说：勾选了<code>Auto reset before per restart</code>则无需再管，一劳永逸。</p></li></ul><h2 id="0x5-开源信息"><a href="#0x5-开源信息" class="headerlink" title="0x5. 开源信息"></a>0x5. 开源信息</h2><ul><li>插件是学习研究项目，源代码是开放的。源码仓库地址：<a href="https://gitee.com/pengzhile/ide-eval-resetter">Gitee</a>。</li><li>如果你有更好的想法，欢迎给我提<code>Pull Request</code>来共同研究完善。</li><li>插件源码使用：<code>GPL-2.0</code>开源协议发布。</li><li>插件使用<code>PHP</code>编写，毕竟<code>PHP</code>是世界上最好的编程语言！</li></ul><h2 id="0x6-支持的产品"><a href="#0x6-支持的产品" class="headerlink" title="0x6. 支持的产品"></a>0x6. 支持的产品</h2><ul><li><strong>IntelliJ IDEA</strong></li><li><strong>AppCode</strong></li><li><strong>CLion</strong></li><li><strong>DataGrip</strong></li><li><strong>GoLand</strong></li><li><strong>PhpStorm</strong></li><li><strong>PyCharm</strong></li><li><strong>Rider</strong></li><li><strong>RubyMine</strong></li><li><strong>WebStorm</strong></li></ul><p><strong>转载于：</strong> <a href="https://zhile.io/2020/11/18/jetbrains-eval-reset.html">https://zhile.io/2020/11/18/jetbrains-eval-reset.html</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> tip </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>article title</title>
      <link href="2020/12/27/article-title/"/>
      <url>2020/12/27/article-title/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="2020/12/27/hello-world/"/>
      <url>2020/12/27/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Spark_SparkSQL</title>
      <link href="2020/11/22/Spark_SparkSQL/"/>
      <url>2020/11/22/Spark_SparkSQL/</url>
      
        <content type="html"><![CDATA[<h1 id="SparkSQL-Abstract"><a href="#SparkSQL-Abstract" class="headerlink" title="SparkSQL_Abstract"></a>SparkSQL_Abstract</h1><h2 id="什么是Spark-SQL"><a href="#什么是Spark-SQL" class="headerlink" title="什么是Spark SQL"></a>什么是Spark SQL</h2><p>Spark SQL是Spark用于结构化数据(Structured Data) 处理的Spark模块</p><p>Spark SQL的底层实现方式是<code>DataFrame API</code> 和 <code>DataSets API</code> </p><p>Spark SQL 运行在 Spark Core 之上。它允许开发人员从 Hive 表和 Parquet 文件中导入关系数据，在导入的数据和现有 rdd 上运行 SQL 查询，并轻松地将 rdd 写到 Hive 表或 Parquet 文件中。</p><p>Spark SQL 引入了称为 <code>Catalyst</code> 的可扩展优化器，因为它有助于在 Bigdata 支持广泛的数据源和算法。</p><h2 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h2><p>DataFrame是一种以RDD为基础的分布式数据集，类似于传统数据库中的二维表格</p><h3 id="0x0-主要区别"><a href="#0x0-主要区别" class="headerlink" title="0x0 主要区别"></a>0x0 主要区别</h3><p>DataFrame也是懒执行的</p><p>DataFrame与RDD的<strong>主要区别</strong>在于，前者带有schema元信息，即DataFrame所表示的二维表数据集的每一列都带有名称和类型。</p><p><img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_20_12/DataFrame%E4%B8%8ERDD%E7%9A%84%E5%8C%BA%E5%88%AB.png" alt="DataFrame与RDD的区别"></p><h3 id="0x1-DataFrame优势"><a href="#0x1-DataFrame优势" class="headerlink" title="0x1 DataFrame优势"></a>0x1 DataFrame优势</h3><p>提供内存管理和执行优化。</p><p>自定义内存管理: 这也被称为项目 <strong>Tungsten</strong>钨。由于数据以二进制格式存储在off-heap memory中，因此节省了大量内存。除此之外，没有垃圾收集开销。</p><p>优化执行计划: 这也称为查询优化器。使用这个，可以为查询的执行创建一个优化的执行计划。一旦创建了优化的计划，最终在 Spark 的 RDDs 上执行。</p><h3 id="0x2-优化器Catalyst"><a href="#0x2-优化器Catalyst" class="headerlink" title="0x2 优化器Catalyst"></a>0x2 优化器Catalyst</h3><ul><li>Analyze logical plan to solve references 分析逻辑计划以解决引用</li><li>Logical plan optimization 逻辑计划优化</li><li>Physical planning 物理规划</li><li>Code generation to compile part of a query to Java bytecode.把一部分代码转换为字节码文件</li></ul><p><img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_20_12/SparkSQL%E4%BC%98%E5%8C%96%E6%A1%88%E4%BE%8B.png" alt="SparkSQL优化案例"></p><p>如图所示,底层执行了一些优化策略,举个最简单的例子,两个RDD的数据源想做个连接,连接之后对数据进行过滤,这个时候可能会有产生笛卡尔乘积,而且再数据量大的情况下存在shuffle来说,性能会大大下降,甚至超过内存无法进行计算</p><p>DF底层会根据逻辑先进行filter然后再进行join,大大减少了数据量</p><h2 id="DataSet"><a href="#DataSet" class="headerlink" title="DataSet"></a>DataSet</h2><p>1.6版本后新的抽象</p><p>DataSet是分布式数据集合。</p><p>DataSet是强类型的。比如可以有DataSet[Car]，DataSet[User]。具有类型安全检查</p><p><code>DataFrame</code>是<code>DataSet</code>的特例,<code>type DataFrame = DataSet[Row]</code> ，<code>Row</code>是一个类型，跟Car、User这些的类型一样，所有的表结构信息都用Row来表示。</p><h3 id="0x0-为什么是DataSet"><a href="#0x0-为什么是DataSet" class="headerlink" title="0x0 为什么是DataSet"></a>0x0 为什么是DataSet</h3><p>DataFrame虽然定义了保存了表结构的原信息</p><pre class="line-numbers language-java" data-language="java"><code class="language-java">ResultSet rs &#x3D; pstat.executeQuery(&quot;select id,name from user&quot;);while (rs.next())&#123;    rs.getInt(1)&#x2F;&#x2F;我们知道是id,但是如果我们sql改变顺序,        &#x2F;&#x2F;比如name,id        &#x2F;&#x2F;调用结果就得修改&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>所以spark就对DataFrame的里面的表结构封装成一个对象,直接使用对象点的方式进行调用,对象的类型就设置为Row类型</p><p>DataSet(row) = DataFrame</p><h3 id="0x1-Encoder"><a href="#0x1-Encoder" class="headerlink" title="0x1 Encoder"></a>0x1 Encoder</h3><p>Encoder编码器是 Spark SQL 中序列化和反序列化(SerDes)框架的基本概念。编码器在<strong>对象</strong>和 Spark 的内部<strong>二进制格式</strong>之间进行转换</p><h2 id="SparkSession"><a href="#SparkSession" class="headerlink" title="SparkSession"></a>SparkSession</h2><p>在老的版本中，SparkSQL提供两种SQL查询起始点：</p><p>一个叫SQLContext，用于Spark自己提供的SQL查询；</p><p>一个叫HiveContext，用于连接Hive的查询。</p><p>SparkSession是Spark最新的SQL查询起始点，实质上是SQLContext和HiveContext的组合，所以在SQLContext和HiveContext上可用的API在SparkSession上同样是可以使用的。</p><p>SparkSession内部封装了SparkContext，所以计算实际上是由SparkContext完成的。当我们使用spark-shell的时候，Spark框架会自动的创建一个名称叫做Spark的SparkSession，就像我们以前可以自动获取到一个sc来表示SparkContext。</p><h2 id="DS-DF-RDD转换"><a href="#DS-DF-RDD转换" class="headerlink" title="DS-DF-RDD转换"></a>DS-DF-RDD转换</h2><p><img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_21_01/DataFrame%E4%B8%8ERDD%E7%9A%84%E8%BD%AC%E6%8D%A2%E5%85%B3%E7%B3%BB%E5%9B%BE.png" alt="DataFrame与RDD的转换关系图"></p><p>￼</p><p>由DataFrame转换过来的RDD的是Row类型</p><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql">--1.RDD &lt;&#x3D;&gt; DF--a. RDD --&gt; DF rdd.toDF(&quot;列名1&quot;，&quot;列名2&quot;，...)--b. DF --&gt; RDDdf.rdd--2.RDD &lt;&#x3D;&gt; DS--a、RDD --&gt; DS--将rdd的数据转换为样例类的格式。    rdd.toDS --这里声明一点--rdd如果是字符串创建来的,是没有能力toDS的--这里要实现把对象准备好val rdd &#x3D; sc.makeRDD(List(Emp(30,&quot;张三&quot;),Emp(40,&quot;李四&quot;))rdd.toDS--这样才能真正的转变--b. DS --&gt; RDDds.rdd-- 3.DF &lt;&#x3D;&gt; DS--a. DF --&gt; DS     df.as[样例类]--该样例类必须存在，而且df中的数据个样例类对应--b. DS --&gt; DSds.toDF<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_21_01/%E4%B8%89%E8%80%85%E8%BD%AC%E6%8D%A2%E5%9B%BE.png" alt="DS与DF,RDD转换图"></p><h1 id="SparkSQL-API"><a href="#SparkSQL-API" class="headerlink" title="SparkSQL_API"></a>SparkSQL_API</h1><h2 id="IDEA中使用SparkSQL"><a href="#IDEA中使用SparkSQL" class="headerlink" title="IDEA中使用SparkSQL"></a>IDEA中使用SparkSQL</h2><h3 id="0x0添加依赖"><a href="#0x0添加依赖" class="headerlink" title="0x0添加依赖"></a>0x0添加依赖</h3><pre class="line-numbers language-xml" data-language="xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>org.apache.spark<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>spark-sql_2.12<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>2.4.5<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="0x1构建sparkSession对象"><a href="#0x1构建sparkSession对象" class="headerlink" title="0x1构建sparkSession对象"></a>0x1构建sparkSession对象</h3><ol><li>重要：连接SparkSQL</li></ol><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">&#x2F;&#x2F; 1. 创建环境val sparkConf: SparkConf &#x3D; new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;sparksql&quot;)&#x2F;&#x2F; 2. 创建SparkSession对象val spark: SparkSession &#x3D; SparkSession.builder().config(sparkConf).getOrCreate()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><ol start="2"><li>添加隐式转换，每次构建完对象以后都需要增加这个 隐式转换的代码</li></ol><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala"> &#x2F;&#x2F; 3. 增加隐式转换    import spark.implicits._&quot;1. 这里的spark不是Scala中的包名，而是创建的sparkSession对象的变量名称 2. spark对象不能使用var声明，因为Scala只支持val修饰的对象的引入&quot;&#x2F;&#x2F;为什么要导入这个对象的隐式转换呢,为了方便调用 $age等<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ol start="3"><li>说明</li></ol><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql">-- 为啥要导入隐式转换sparkSQL是在spark的基础上进行延伸，属于功能的扩展，使用隐式转换，体现了OCP开发原则。--构建对象为什么不直接new呢？因为sparkSession是对sparkContext的包装，创建这个对象时，需要很多步骤，将这些过程进行封装，让开发更容易，使用一个构建器来创建对象。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="0x3代码实现"><a href="#0x3代码实现" class="headerlink" title="0x3代码实现"></a>0x3代码实现</h3><p>直接从SparkSQL里看</p><p>这里仅仅展示一个示例</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">git clone git@github.com:fourgold&#x2F;Spark.git<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">object SparkSQL01_DSL &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    &#x2F;&#x2F;创建SparkSQL的运行环境    val sparkConf: SparkConf &#x3D; new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;BASIC&quot;)    val sparkSession: SparkSession &#x3D; SparkSession.builder().config(sparkConf).getOrCreate()    &#x2F;&#x2F;DataFrame    val dataFrame: DataFrame &#x3D; sparkSession.read.json(&quot;.&#x2F;input&#x2F;user.json&quot;)&#x2F;&#x2F;    dataFrame.show()    &#x2F;&#x2F;DSL语句使用调用方法,类似于Flink总的TABLE API与SQL API    dataFrame.select(&quot;name&quot;,&quot;age&quot;).show()    import sparkSession.implicits._    dataFrame.select($&quot;age&quot;+1).as(&quot;age&quot;).show()    &#x2F;&#x2F;怎么选择两列    &#x2F;&#x2F;可以使用单引号代表引用    dataFrame.select(&#39;age+1).as(&quot;age&quot;).show()    &#x2F;&#x2F;关闭环境    sparkSession.close()  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="自定义UDF函数"><a href="#自定义UDF函数" class="headerlink" title="自定义UDF函数"></a>自定义UDF函数</h2><p>需求:将一个字段的string,加一个前缀</p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">object Demo02_Practice &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    val spark: SparkSession &#x3D; SparkSession.builder().config(new SparkConf().setMaster(&quot;local[1]&quot;).setAppName(&quot;SparkSQL&quot;)).getOrCreate()    &#x2F;&#x2F;用户自定函数    import spark.implicits._    val df: DataFrame &#x3D; spark.read.json(&quot;Day08&#x2F;input&#x2F;person.json&quot;)    df.createOrReplaceTempView(&quot;user&quot;)    df.show()    &#x2F;&#x2F;自定义udf函数    spark.udf.register(&quot;addName&quot;,(name:String)&#x3D;&gt;&quot;name:&quot;+name)    spark.sql(&quot;select age,addName(username) from user&quot;).show()  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="自定义UDAF函数"><a href="#自定义UDAF函数" class="headerlink" title="自定义UDAF函数"></a>自定义UDAF函数</h2><p>需求:</p><p><img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_21_01/%E8%87%AA%E5%AE%9AUDAF%E5%87%BD%E6%95%B0%E5%AE%9E%E7%8E%B0%E9%9C%80%E6%B1%82.png" alt="自定义UDAF函数实现需求"></p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">package com.SparkSqlimport org.apache.spark.sql.Rowimport org.apache.spark.sql.expressions.&#123;MutableAggregationBuffer, UserDefinedAggregateFunction&#125;import org.apache.spark.sql.types.&#123;DataType, DoubleType, LongType, StructField, StructType&#125;class MyAvg extends UserDefinedAggregateFunction&#123;  &#x2F;&#x2F;输入类型  override def inputSchema: StructType &#x3D; StructType(Array(StructField(&quot;age&quot;,LongType)))  &#x2F;&#x2F;缓冲区  override def bufferSchema: StructType &#x3D;StructType(Array(StructField(&quot;sum&quot;,LongType),StructField(&quot;count&quot;,LongType)))  &#x2F;&#x2F;返回值的数据类型  override def dataType: DataType &#x3D; DoubleType  &#x2F;&#x2F;稳定性：对于相同的输入是否一直返回相同的输出。  override def deterministic: Boolean &#x3D; true  &#x2F;&#x2F;缓冲区的初始化  override def initialize(buffer: MutableAggregationBuffer): Unit &#x3D; &#123;    buffer(0) &#x3D; 0L&#x2F;&#x2F;sum    buffer(1) &#x3D; 0L&#x2F;&#x2F;count  &#125;  &#x2F;&#x2F; 更新缓冲区中的数据  override def update(buffer: MutableAggregationBuffer, input: Row): Unit &#x3D; &#123;    if (!input.isNullAt(0)) &#123;      buffer(0) &#x3D; buffer.getLong(0) + input.getLong(0)      buffer(1) &#x3D; buffer.getLong(1) + 1    &#125;  &#125;  &#x2F;&#x2F; 合并缓冲区  override def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit &#x3D; &#123;    buffer1(0) &#x3D; buffer1.getLong(0) + buffer2.getLong(0)    buffer1(1) &#x3D; buffer1.getLong(1) + buffer2.getLong(1)  &#125;  override def evaluate(buffer: Row): Any &#x3D; buffer.getLong(0).toDouble&#x2F;buffer.getLong(1).toDouble&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>聚合函数的使用</p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">&#x2F;&#x2F;使用package com.SparkSqlimport org.apache.spark.SparkConfimport org.apache.spark.sql.expressions.UserDefinedAggregateFunctionimport org.apache.spark.sql.&#123;DataFrame, SparkSession&#125;&#x2F;** * @author Jinxin Li * @create 2020-11-03 16:55 *&#x2F;object Demo02_Practice &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    val spark: SparkSession &#x3D; SparkSession.builder().config(new SparkConf().setMaster(&quot;local[1]&quot;).setAppName(&quot;SparkSQL&quot;)).getOrCreate()    &#x2F;&#x2F;用户自定函数    import spark.implicits._    val df: DataFrame &#x3D; spark.read.json(&quot;Day08&#x2F;input&#x2F;person.json&quot;)    df.createOrReplaceTempView(&quot;user&quot;)    df.show()    &#x2F;&#x2F;自定义udf函数    spark.udf.register(&quot;addName&quot;,(name:String)&#x3D;&gt;&quot;name:&quot;+name)    spark.sql(&quot;select age,addName(username) from user&quot;).show()    &#x2F;&#x2F;在spark中注册聚合函数&#x3D;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;    var MyAvg &#x3D; new MyAvg    spark.udf.register(&quot;avgAge&quot;,MyAvg)    spark.sql(&quot;select avgAge(age) from user&quot;).show()    spark.close()  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="自定义强类型AggregateUDAF函数"><a href="#自定义强类型AggregateUDAF函数" class="headerlink" title="自定义强类型AggregateUDAF函数"></a>自定义强类型AggregateUDAF函数</h2><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">package com.SparkSqlimport org.apache.spark.sql.&#123;Encoder, Encoders&#125;import org.apache.spark.sql.expressions.Aggregator&#x2F;** * @author Jinxin Li * @create 2020-11-03 19:16 * 求user的平均年龄 * 1.继承 * 2.定义泛型 * In 输入数据类型 * Buf * out Double输出的数据类型 * 3.重写方法(6) *&#x2F;case class Buff(var total:Long,var count:Long)class MyAvg2 extends Aggregator[Long,Buff,Double]&#123;  &#x2F;&#x2F;scala用zero,初始值,零值  &#x2F;&#x2F;缓冲区的初始化  override def zero: Buff &#x3D; &#123;    Buff(0L,0L)  &#125;  &#x2F;&#x2F;根据输入的数据更新缓冲区的数据  override def reduce(b: Buff, a: Long): Buff &#x3D; &#123;    b.total +&#x3D; a    b.count +&#x3D; 1    b  &#125;  override def merge(b1: Buff, b2: Buff): Buff &#x3D; &#123;    b1.count +&#x3D; b2.count    b1.total +&#x3D; b2.total    b1  &#125;  override def finish(reduction: Buff): Double &#x3D; reduction.total.toDouble&#x2F;reduction.count.toDouble  override def bufferEncoder: Encoder[Buff] &#x3D; Encoders.product  override def outputEncoder: Encoder[Double] &#x3D; Encoders.scalaDouble&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="0x0注册"><a href="#0x0注册" class="headerlink" title="0x0注册"></a>0x0注册</h3><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">val MyAvg2 &#x3D; new MyAvg2    spark.udf.register(&quot;avgAge1&quot;, functions.udaf(MyAvg2))    spark.sql(&quot;select avgAge1(age) from user&quot;).show()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="0x1使用"><a href="#0x1使用" class="headerlink" title="0x1使用"></a>0x1使用</h3><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">&#x2F;** * 弱类型操作,只有0,1 没有类型的概念 * 没有类型的概念 * 强类型通过属性操作,跟属性没关系 * 自定属性类,定义泛型 *&#x2F;object SparkSQL03_UDAF &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    &#x2F;&#x2F;创建SparkSQL的运行环境    val sparkConf: SparkConf &#x3D; new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;BASIC&quot;)    val sparkSession: SparkSession &#x3D; SparkSession.builder().config(sparkConf).getOrCreate()    &#x2F;&#x2F;聚合函数也是比较重要的,比如,平均值,最大值,最小值    &#x2F;&#x2F;DataFrame    val dataFrame: DataFrame &#x3D; sparkSession.read.json(&quot;.&#x2F;input&#x2F;user.json&quot;)&#x2F;&#x2F;    dataFrame.show()    &#x2F;&#x2F;将数据创建临时表    dataFrame.createOrReplaceTempView(&quot;user&quot;)    &#x2F;&#x2F;view只能查不能改    sparkSession.udf.register(&quot;prefixName&quot;,(name:String)&#x3D;&gt;&#123;&quot;name+&quot;+name&#125;)    &#x2F;&#x2F;将某一字段的名字加上前缀    sparkSession.sql(      &quot;&quot;&quot;        |select prefixName(name),age from user        |&quot;&quot;&quot;.stripMargin).show()    &#x2F;&#x2F;使用udaf-aggregator函数    val myAvg &#x3D; new MyAvg()    sparkSession.udf.register(&quot;myAvg&quot;,functions.udaf(new MyAvgAgg))    sparkSession.sql(      &quot;&quot;&quot;        |select myAvg(age) as avgAge from user        |&quot;&quot;&quot;.stripMargin).show()    &#x2F;&#x2F;关闭环境    sparkSession.close()  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="SparkSQL-Data"><a href="#SparkSQL-Data" class="headerlink" title="SparkSQL_Data"></a>SparkSQL_Data</h1><h2 id="加载数据"><a href="#加载数据" class="headerlink" title="加载数据"></a>加载数据</h2><p>如果不指定加载类型,默认的保存与加载类型是parquet</p><p>spark.read.load 是加载数据的通用方法</p><p>如果读取不同格式的数据，可以对不同的数据格式进行设定</p><p>我们前面都是使用 read API 先把文件加载到 DataFrame 然后再查询，其实，我们也可以直接在文件上查询</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">scala&gt; spark.read. scala&gt; spark.read.format(&quot;…&quot;)[.option(&quot;…&quot;)].load(&quot;…&quot;) #format(&quot;…&quot;)：指定加载的数据类型，包括&quot;csv&quot;、&quot;jdbc&quot;、&quot;json&quot;、&quot;orc&quot;、&quot;parquet&quot;和&quot;textFile&quot;。#load(&quot;…&quot;)：在&quot;csv&quot;、&quot;jdbc&quot;、&quot;json&quot;、&quot;orc&quot;、&quot;parquet&quot;和&quot;textFile&quot;格式下需要传入加载数据的路径。 #option(&quot;…&quot;)：在&quot;jdbc&quot;格式下需要传入 JDBC 相应参数，url、user、password 和 dbtable #直接在文件上进行查询:  文件格式.&#96;文件路径&#96; scala&gt; spark.sql(&quot;select * from json.&#96;&#x2F;opt&#x2F;module&#x2F;data&#x2F;user.json&#96;&quot;).show <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>实战</strong>:JSON与Parquet文件的读取与保存</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">spark.read.load()#默认情况下读取的格式是parquet文件val df &#x3D; spark.read.load(&quot;&#x2F;opt&#x2F;module&#x2F;spark&#x2F;examples&#x2F;src&#x2F;main&#x2F;resources&#x2F;users.parquet&quot;)#格式,例子的地方#&#x2F;opt&#x2F;module&#x2F;spark&#x2F;examples&#x2F;src&#x2F;main&#x2F;resources#保存数据 SparkSQL默认读取或者保存的文件格式parquetdf.write.save(&quot;output&quot;)#就想读json文件val df &#x3D; spark.read.format(&quot;json&quot;).load(&quot;data&#x2F;user.json&quot;)#比较简单的json文件spark.read.json()#保存json文件df.write.format(&quot;json&quot;).save(&quot;output1&quot;)#选择表 转换过程由spark自己完成 注意使用飘号spark.sql(&quot;select * from json.&#96;data&#x2F;user.json&#96;&quot;).show<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="保存数据"><a href="#保存数据" class="headerlink" title="保存数据"></a>保存数据</h2><p>df.write.save 是保存数据的通用方法 </p><p>scala&gt;df.write. csv  jdbc   json  orc   parquet textFile… … 如果保存不同格式的数据，可以对不同的数据格式进行设定 </p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">scala&gt;df.write. csv  jdbc   json  orc   parquet textFile… … <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p><img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_21_01/SparkSQL%E6%95%B0%E6%8D%AE%E4%BF%9D%E5%AD%98%E6%A0%BC%E5%BC%8F.png" alt="SparkSQL数据的保存"></p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">scala&gt;df.write.format(&quot;…&quot;)[.option(&quot;…&quot;)].save(&quot;…&quot;) # format(&quot;…&quot;)：指定保存的数据类型，包括&quot;csv&quot;、&quot;jdbc&quot;、&quot;json&quot;、&quot;orc&quot;、&quot;parquet&quot;和&quot;textFile&quot;。 # save (&quot;…&quot;)：在&quot;csv&quot;、&quot;orc&quot;、&quot;parquet&quot;和&quot;textFile&quot;格式下需要传入保存数据的路径。 # option(&quot;…&quot;)：在&quot;jdbc&quot;格式下需要传入 JDBC 相应参数，url、user、password 和 dbtable # 保存操作可以使用 SaveMode, 用来指明如何处理数据，使用 mode()方法来设置。 有一点很重要: 这些 SaveMode 都是没有加锁的, 也不是原子操作。 #SaveMode 是一个枚举类，其中的常量包括： #SaveMode.ErrorIfExists(default) &quot;error&quot;(default) 如果文件已经存在则抛出异常 #SaveMode.Append &quot;append&quot; 如果文件已经存在则追加 #SaveMode.Overwrite &quot;overwrite&quot; 如果文件已经存在则覆盖 #SaveMode.Ignore &quot;ignore&quot; 如果文件已经存在则忽略 df.write.mode(&quot;append&quot;).json(&quot;&#x2F;opt&#x2F;module&#x2F;data&#x2F;output&quot;) .save(&quot;output&quot;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="JSON-Parquet-CSV"><a href="#JSON-Parquet-CSV" class="headerlink" title="JSON/Parquet/CSV"></a>JSON/Parquet/CSV</h2><h3 id="Parquet"><a href="#Parquet" class="headerlink" title="Parquet"></a>Parquet</h3><p>Spark SQL 的默认数据源为 Parquet 格式。Parquet 是一种能够有效存储嵌套数据的列式存储格式。<br>数据源为 Parquet 文件时，Spark SQL 可以方便的执行所有的操作，不需要使用 format。<br>修改配置项 spark.sql.sources.default，可修改默认数据源格式。 </p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">scala&gt; val df &#x3D; spark.read.load(&quot;examples&#x2F;src&#x2F;main&#x2F;resources&#x2F;users.parquet&quot;) scala&gt; df.showscala&gt; var df &#x3D; spark.read.json(&quot;&#x2F;opt&#x2F;module&#x2F;data&#x2F;input&#x2F;people.json&quot;) #保存为 parquet 格式 scala&gt; df.write.mode(&quot;append&quot;).save(&quot;&#x2F;opt&#x2F;module&#x2F;data&#x2F;output&quot;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="JSON"><a href="#JSON" class="headerlink" title="JSON"></a>JSON</h3><p>Spark SQL 能够自动推测 JSON 数据集的结构，并将它加载为一个 Dataset[Row]. 可以通过 SparkSession.read.json()去加载 JSON 文件。 </p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">#将读取的文件保存val df &#x3D; spark.read.json(&quot;data&#x2F;user.json&quot;)#保存 因为保存模式的原因,再次保存会报错df.write.format(&quot;json&quot;).save(&quot;output&quot;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>注意：Spark 读取的 JSON 文件不是传统的 JSON 文件，每一行都应该是一个 JSON 串。格式如下： </p><pre class="line-numbers language-json" data-language="json"><code class="language-json">&#123;&quot;name&quot;:&quot;Michael&quot;&#125; &#123;&quot;name&quot;:&quot;Andy&quot;， &quot;age&quot;:30&#125; [&#123;&quot;name&quot;:&quot;Justin&quot;， &quot;age&quot;:19&#125;,&#123;&quot;name&quot;:&quot;Justin&quot;， &quot;age&quot;:19&#125;] #因为Spark读取是一行一行读的,所以一行应该是一个标准的json文件<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h3 id="CSV"><a href="#CSV" class="headerlink" title="CSV"></a>CSV</h3><p>Spark SQL 可以配置 CSV 文件的列表信息，读取 CSV 文件,CSV 文件的第一行设置为数据列 spark</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">#option例子是分号 optipn-分隔符 header-表头 inferSchema-??spark.read.format(&quot;csv&quot;).option(&quot;sep&quot;, &quot;;&quot;).option(&quot;inferSchema&quot;, &quot;true&quot;).option(&quot;header&quot;, &quot;true&quot;).load(&quot;data&#x2F;user.csv&quot;) <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h2 id="HIVE"><a href="#HIVE" class="headerlink" title="HIVE"></a>HIVE</h2><p>Apache Hive 是 Hadoop 上的 SQL 引擎，Spark SQL 编译时可以包含 Hive 支持，也可以不包含。</p><p>包含 Hive 支持的 Spark SQL 可以支持 Hive 表访问、UDF (用户自定义函数)以及 Hive 查询语言(HiveQL/HQL)等。</p><p>SparkSQL想连接HIVE有两种连接方式</p><h3 id="内置Hive"><a href="#内置Hive" class="headerlink" title="内置Hive"></a>内置Hive</h3><p>SparkSQL本身也具有元数据,数据仓库,全部都有,通过内置Hive实现</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">#显示全部表spark.sql(&quot;show tables&quot;).show<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>发现内部文件系统自动生成了metadb</p><p><img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_21_01/Spark%E5%86%85%E9%83%A8%E6%96%87%E4%BB%B6.png" alt="Spark内置文件系统"></p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"># 导入数据val df &#x3D; spark.read.json(&quot;data&#x2F;user.json&quot;)# 创建表spark.sql(&quot;create table test(age int)&quot;)#查看表spark.sql(&quot;show tables&quot;).show#加载数据spark.sql(&quot;load data local inpath &#39;data&#x2F;user.text&#39; into table test&quot;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_21_01/spark%E5%86%85%E7%BD%AEhive%E7%9A%84warehouse.png" alt="spark-sql数据仓库"></p><h3 id="外置Hive"><a href="#外置Hive" class="headerlink" title="外置Hive"></a>外置Hive</h3><p>一般使用外置Hive如果想连接外部已经部署好的 Hive，需要通过以下几个步骤： </p><ol><li> Spark 要接管 Hive 需要把 hive-site.xml 拷贝到 conf/目录下 </li><li> 把 Mysql 的驱动 copy 到 jars/目录下 </li><li> 如果访问不到 hdfs，则需要把 core-site.xml 和 hdfs-site.xml 拷贝到 conf/目录下 </li><li> 重启 spark-shell </li></ol><p>然后查看一下数据库</p><p>已经连接到Hive了</p><h3 id="Beeline"><a href="#Beeline" class="headerlink" title="Beeline"></a>Beeline</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">bin&#x2F;spark-sql#为什么带有(default)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p><img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_21_01/spark-sql.png" alt="spark-sql显示bug"></p><p>Spark Thrift Server 是 Spark 社区基于 HiveServer2 实现的一个 Thrift 服务。旨在无缝兼容HiveServer2。</p><p>因为 Spark Thrift Server 的接口和协议都和 HiveServer2 完全一致，因此我们部署好 Spark Thrift Server 后，可以直接使用 hive 的 beeline 访问 Spark Thrift Server 执行相关语句。</p><p>Spark Thrift Server 的目的也只是取代 HiveServer2，因此它依旧可以和 Hive Metastore进行交互，获取到 hive 的元数据。 </p><p>如果想连接 Thrift Server，需要通过以下几个步骤： </p><ol><li><p>Spark 要接管 Hive 需要把 hive-site.xml 拷贝到 conf/目录下  </p></li><li><p> 把 Mysql 的驱动 copy 到 jars/目录下 </p></li><li><p>如果访问不到 hdfs，则需要把 core-site.xml 和 hdfs-site.xml 拷贝到 conf/目录下 </p></li><li><p>启动 Thrift Server</p></li></ol><p><strong>beeline客户端连接</strong></p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">sbin&#x2F;start-thriftserver.sh # 使用 beeline 连接 Thrift Server bin&#x2F;beeline -u jdbc:hive2:&#x2F;&#x2F;linux1:10000 -n root <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>代码操作</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">#注意：在开发工具中创建数据库默认是在本地仓库，通过参数修改数据库仓库的地址: config(&quot;spark.sql.warehouse.dir&quot;, &quot;hdfs:&#x2F;&#x2F;linux1:8020&#x2F;user&#x2F;hive&#x2F;warehouse&quot;) <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>权限问题</p><pre class="line-numbers language-java" data-language="java"><code class="language-java">System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;root&quot;)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>]]></content>
      
      
      
        <tags>
            
            <tag> spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Git的使用</title>
      <link href="2020/10/27/Git/"/>
      <url>2020/10/27/Git/</url>
      
        <content type="html"><![CDATA[<h1 id="Git"><a href="#Git" class="headerlink" title="Git"></a>Git</h1><h1 id="0x0-GitHub用户信息"><a href="#0x0-GitHub用户信息" class="headerlink" title="0x0. GitHub用户信息"></a>0x0. GitHub用户信息</h1><pre class="language-shell" data-language="shell"><code class="language-shell">Username: fourgoldEmail address: lijinxinok@163.com密码：IELTS-rise-to-6.5-!验证邮箱：lijinxinok@163.com邮箱密码：qinni123！</code></pre><h1 id="0x1-Github常用命令"><a href="#0x1-Github常用命令" class="headerlink" title="0x1. Github常用命令"></a>0x1. Github常用命令</h1><h3 id="2-1初始化操作"><a href="#2-1初始化操作" class="headerlink" title="2.1初始化操作"></a>2.1初始化操作</h3><pre class="language-shell" data-language="shell"><code class="language-shell">#初始化本地仓库git init#查看隐藏文件.&#x2F;gitls -lAll .git&#x2F;#.git存放的是本地库相关的子目录以及文件#初始化本地配置 设置签名 这个签名和登录远程库和账号密码没有任何关系#global代表的是系统用户级别git config --global user.name JInxinLi#初始化本地邮箱$ git config --global user.email Jinxin@atguigu.com#签名的作用是区分不同操作者身份。用户的签名信息在每一个版本的提交信息中能够看到，以此确认本次提交是谁做的。</code></pre><p>就近原则:项目级别的优先于系统用户级别,采用项目级别的签名</p><p><strong>签名信息保存在哪里?</strong></p><pre class="language-bash" data-language="bash"><code class="language-bash">.&#x2F;git&#x2F;config</code></pre><p><strong>系统用户保存在哪里?</strong></p><pre class="language-bash" data-language="bash"><code class="language-bash">~&#x2F;.gitconfig</code></pre><h3 id="2-2实际操作"><a href="#2-2实际操作" class="headerlink" title="2.2实际操作"></a>2.2实际操作</h3><p>git add添加到暂存区还可以撤销</p><p>HEAD是一个指针</p><pre class="language-bash" data-language="bash"><code class="language-bash">#查看主文档 暂存区与工作区的状态 默认matser分支git status#添加暂存区git add#提交到本地库 hello.txt是文件名1 git commit -m &quot;version1.0&quot; hello.txt#推送matser开发线到远程仓库git push sparkStreaming master#将远程仓库的内容克隆到本地git clone http#拉取远程仓库内容git pull sparkStreaming master#查看历史版本 (HEAD -&gt; matser) HEAD是一个指针git loggit reflog#切换版本号git reset --hard 087a1a7#创建分支git branch hot-fix#查看分支git branch -v#切换分支git checkout hot-fix#合并分支#注意当前分支为mastergit merge hot-fix#冲突解决后提交git commit -m &quot;merge hot-fix&quot;####新建别名urlgit remote -vgit remote add origin https:&#x2F;&#x2F;url....git remote -v##推送git push origin master##克隆操作git clone url#回车#完整的把远程库下载到本地#创建origin远程地址别名#初始化本地库#邀请别人加入到协作组中#点击邀请,就变成了团队成员#pull &#x3D; fetch+mergegit fetch origin master#将远程库的master拉取下来#切换到origin master#查看远程的master分支[远程地址][远程分支]git checkout origin&#x2F;master#合并远程的master分支[远程地址][远程分支]git merge origin&#x2F;mastergit pull origin&#x2F;master</code></pre><p><strong>冲突解决</strong></p><p>当主分支已经提升了版本,而我们的clone的是旧版本,这个时候就已经无法推送</p><pre class="language-shell" data-language="shell"><code class="language-shell">#解决,先拿到远程的修改 如果不是最新版所做的修改,不能修改,必须先拉取git pull origin&#x2F;master#文件里东西修改git add [filename]git commit -m &quot;version2.0&quot;git push origin&#x2F;master</code></pre><p><strong>跨团队协作</strong></p><pre class="language-shell" data-language="shell"><code class="language-shell">#以第三者的身份先进行fork fork到自己的远程库#克隆到本地git clone url[自己的地址]#然后进行增加内容git commit -m &quot;...&quot; [filename]git push origin master#现在已经推送到自己的远程库#本地修改,然后推送到远程#new pull request#create new pull result#发送消息进行提交#经理 打开pull request#点击内容#两人可以聊天#点commits files changed#审核代码#merge pull request#点这里进行合并#合并的时候也要添加相关信息</code></pre><p>展示的时候显示了来源</p><h3 id="2-3查看历史记录log的方式"><a href="#2-3查看历史记录log的方式" class="headerlink" title="2.3查看历史记录log的方式"></a>2.3查看历史记录log的方式</h3><pre class="language-shell" data-language="shell"><code class="language-shell">#最完整的形式git log#日志以一个漂亮的格式进行显示git log --pretty&#x3D;oneline#hash值显示一部分git log --oneline#多屏幕显示控制方法#空格向下翻页#b向上翻页#q退出#在oneline的基础上显示了移动到某一个版本要移动几步git reflog</code></pre><p><img src="https://i.loli.net/2020/12/24/mvKrNGdHQwWt9Ty.png" alt="注意HEAD"></p><h3 id="2-4版本回退穿梭"><a href="#2-4版本回退穿梭" class="headerlink" title="2.4版本回退穿梭"></a>2.4版本回退穿梭</h3><p>管理历史记录的时候存在一个<strong>指针</strong>(HEAD)</p><p>我们可以把HEAD指针进行移动</p><pre class="language-shell" data-language="shell"><code class="language-shell">#切换版本号 git reset --hard 087a1a7</code></pre><h3 id="2-5三种操作的参数"><a href="#2-5三种操作的参数" class="headerlink" title="2.5三种操作的参数"></a>2.5<strong>三种操作</strong>的参数</h3><ol><li>基于索引值操作[推荐]</li><li>使用^符号:只能往后退</li></ol><pre class="language-shell" data-language="shell"><code class="language-shell">#回退一步git reset --hard HEAD^#回退三步git reset --hard HEAD^^^</code></pre><ol><li>使用~符号</li></ol><pre class="language-shell" data-language="shell"><code class="language-shell">#回退3步git reset --hard HEAD~3</code></pre><h3 id="2-6参数说明"><a href="#2-6参数说明" class="headerlink" title="2.6参数说明"></a>2.6参数说明</h3><p>reset命令的三个参数对比</p><pre class="language-shell" data-language="shell"><code class="language-shell">#查看本地帮助文档git help reset#命令--soft不会动index file(暂存区) and work tree(工作区)仅仅在本地库移动HEAD指针#将本地库后退--mixed在本地移动指针重置暂存区#将暂存区与本地库后退--hard移动指针重置缓存区重置工作区#全部后退</code></pre><h3 id="2-7永久删除文件的保存"><a href="#2-7永久删除文件的保存" class="headerlink" title="2.7永久删除文件的保存"></a>2.7永久删除文件的保存</h3><p>删除仅仅是一条记录,可以回退版本进行恢复</p><p>前提:删除前,文件存在时的状态提交到了本地库</p><pre class="language-shell" data-language="shell"><code class="language-shell">git reset -hard[指针位置]</code></pre><h3 id="2-8添加到暂存区的删除文件找回"><a href="#2-8添加到暂存区的删除文件找回" class="headerlink" title="2.8添加到暂存区的删除文件找回"></a>2.8添加到暂存区的删除文件找回</h3><pre class="language-shell" data-language="shell"><code class="language-shell">#暂存区与工作都是git reset --hard HEAD</code></pre><h3 id="2-9比较文件差异"><a href="#2-9比较文件差异" class="headerlink" title="2.9比较文件差异"></a>2.9比较文件差异</h3><pre class="language-shell" data-language="shell"><code class="language-shell">git diff test.txtgit diff [本地区中的历史版本][文件名]</code></pre><p><img src="https://i.loli.net/2020/12/24/GsheZUHS9PQ3zWN.png" alt="image-20201224171304996"></p><p>将工作区的文件个暂存区的文件进行比较</p><h3 id="2-10分支管理"><a href="#2-10分支管理" class="headerlink" title="2.10分支管理"></a>2.10分支管理</h3><p>在版本控制过程中,使用多条线控制任务的分支</p><p>分支的命名以feature开头 feature_bule</p><p>热修复的命名 hot_fix </p><p>分支能够同时并行推进多个功能的开发,提高开发效率</p><p>如果分支在开发过程中,如果某一个分支开发失败,不会对其他分支有任何影响,失败的分支删除重新开始</p><h3 id="2-11分支的具体操作"><a href="#2-11分支的具体操作" class="headerlink" title="2.11分支的具体操作"></a>2.11分支的具体操作</h3><p>master是默认分支</p><pre class="language-shell" data-language="shell"><code class="language-shell">#查看分支git branch -v#创建分支git branch hot_fix#切换分支git checkout hot_fix#合并分支的步骤#第一步:切换到接受修改的分支git checkout master#第二步:执行merge命令git merge hot_fix#解决冲突#第一步编辑文件,删除特殊符号#第二步把文件修改到满意#第三步 git add[文件名]#第四部 git commit</code></pre><h1 id="0x3-Git基本原理"><a href="#0x3-Git基本原理" class="headerlink" title="0x3. Git基本原理"></a>0x3. Git基本原理</h1><h2 id="3-1哈希算法"><a href="#3-1哈希算法" class="headerlink" title="3.1哈希算法"></a>3.1哈希算法</h2><pre class="mermaid">graph LR明文-->加密算法-->密文</pre><p>同一个数保证加密后得到同一个结果</p><p>输入数据细微变化会引起Hash巨大的变化</p><p>哈希算法不可逆</p><p>不管输入的数据的数据量有多大,输入同一个哈希算法,得到的加密结果长度固定</p><p>很多内容也会加密成得到的长度相同32位16进制数</p><p>Git底层采用的是SHA-1算法</p><p><strong>用途</strong>:哈希算法用于校验文件</p><h2 id="3-2-Git保存版本的机制"><a href="#3-2-Git保存版本的机制" class="headerlink" title="3.2 Git保存版本的机制"></a>3.2 Git保存版本的机制</h2><p>每个版本都会保存当前版本的文件状态</p><p>Git把数据看做是小型文件系统的一组快照,每次提交更新时Git都会对当前的全部文件制作一个快照并保存这个快照的索引</p><p>为了高效,如果文件没有修改,Git不再重新存储该文件,而是只保留一个链接指向之前的存储的文件,所以Git的工作方式可以称为快照流</p><h2 id="3-3Git如何管理分支"><a href="#3-3Git如何管理分支" class="headerlink" title="3.3Git如何管理分支"></a>3.3Git如何管理分支</h2><p>第一次提交是rootcommit</p><p>master与testing都算是指针,指向原来的对象</p><h1 id="0x4-idea使用GitHub"><a href="#0x4-idea使用GitHub" class="headerlink" title="0x4. idea使用GitHub"></a>0x4. idea使用GitHub</h1><h3 id="4-1创建同步忽略文件"><a href="#4-1创建同步忽略文件" class="headerlink" title="4.1创建同步忽略文件"></a>4.1创建同步忽略文件</h3><p>创建忽略规则文件xxxx.ignore（前缀名随便起）</p><p>这个文件的存放位置原则上在哪里都可以，</p><p>为了便于让~/.gitconfig文件引用，</p><p>建议也放在用户家目录下</p><p>xxxx.ignore文件内容如下：</p><h4 id="idea-ignore"><a href="#idea-ignore" class="headerlink" title="idea.ignore"></a>idea.ignore</h4><pre class="language-shell" data-language="shell"><code class="language-shell"># Compiled class file*.class# Log file*.log# BlueJ files*.ctxt# Mobile Tools for Java (J2ME).mtj.tmp&#x2F;# Package Files #*.jar*.war*.nar*.ear*.zip*.tar.gz*.rar# virtual machine crash logs, see http:&#x2F;&#x2F;www.java.com&#x2F;en&#x2F;download&#x2F;help&#x2F;error_hotspot.xmlhs_err_pid*.classpath.project.settingstarget.idea*.iml</code></pre><p>2）在.gitconfig文件中引用忽略配置文件（此文件在Windows的家目录中）</p><pre class="language-shell" data-language="shell"><code class="language-shell">[user]name &#x3D; ZhangJYemail &#x3D; ZhangJY@atguigu.com[core]excludesfile &#x3D; C:&#x2F;Users&#x2F;ZhangJY&#x2F;SH0720.ignore注意：这里要使用“正斜线（&#x2F;）”，不要使用“反斜线（\）”</code></pre><h2 id="4-2使用免密登录连接远程仓库"><a href="#4-2使用免密登录连接远程仓库" class="headerlink" title="4.2使用免密登录连接远程仓库"></a>4.2使用免密登录连接远程仓库</h2><p>配置免密登录时非常有必要的</p><p>首先要明白配置免密登录使用的SSH登录方式,使用RSA</p><p>免密登陆地址</p><pre class="language-shell" data-language="shell"><code class="language-shell">#1.进入家目录cd</code></pre><blockquote><p>89388@DESKTOP-CEH28KV MINGW64 ~</p></blockquote><pre class="language-shell" data-language="shell"><code class="language-shell">#2.删除.ssh目录rm -rvf .ssh</code></pre><blockquote><p>removed ‘.ssh/known_hosts’</p><p>removed directory ‘.ssh’</p></blockquote><pre class="language-shell" data-language="shell"><code class="language-shell">#3.生成免密密钥 -C +github账号ssh-keygen -t rsa -C lijinxinok@163.com</code></pre><pre class="language-shell" data-language="shell"><code class="language-shell">#4.查看公钥并复制公钥cat id_rsa.pub#复制公钥,注意有坑,在命令行复制容易出问题#可以去源文件的地方使用nodpad++打开</code></pre><pre class="language-shell" data-language="shell"><code class="language-shell">#5.添加公钥到github#如图</code></pre><p><img src="https://i.loli.net/2020/12/25/am5XnUMoeWY7G2x.png" alt=" 1606477780968"></p><p>然后将公钥复制进去就可以了</p><p>注意push的时候要使用SSH地址哦</p><p><img src="https://i.loli.net/2020/12/25/4PqjRpcbdrewtGv.png" alt="免密登陆地址"></p><pre class="language-shell" data-language="shell"><code class="language-shell">#6.查看当前所有远程地址别名git remote -v </code></pre><p>然后将远程登录的SSH复制,添加别名</p><pre class="language-shell" data-language="shell"><code class="language-shell">#7.别名 远程地址git remote add SparkStreaming http:ssh登录地址</code></pre><p>注意有坑,这里时ssh登录地址</p><p>接下来就可以测试了</p><pre class="language-shell" data-language="shell"><code class="language-shell">#8.测试一下git add hello.txtgit push SparkStreaming master</code></pre><h2 id="4-3-Git结构"><a href="#4-3-Git结构" class="headerlink" title="4.3.Git结构"></a>4.3.Git结构</h2><pre class="mermaid">graph TD历史版本---本地库临时存储---暂存区写代码---工作区工作区--git add-->暂存区暂存区--git commit-->本地库</pre><h2 id="4-4Git和代码托管中心"><a href="#4-4Git和代码托管中心" class="headerlink" title="4.4Git和代码托管中心"></a>4.4Git和代码托管中心</h2><h4 id="局域网环境"><a href="#局域网环境" class="headerlink" title="局域网环境"></a>局域网环境</h4><p>GitLab</p><h4 id="外网络环境"><a href="#外网络环境" class="headerlink" title="外网络环境"></a>外网络环境</h4><p>GitHub</p><p>码云</p><h2 id="4-5-本地与内部协作"><a href="#4-5-本地与内部协作" class="headerlink" title="4.5 本地与内部协作"></a>4.5 本地与内部协作</h2><h4 id="团队内部协作"><a href="#团队内部协作" class="headerlink" title="团队内部协作"></a>团队内部协作</h4><p>加入团队可以增加权限</p><h4 id="跨团队协作"><a href="#跨团队协作" class="headerlink" title="跨团队协作"></a>跨团队协作</h4><h2 id="4-6从零到一使用IDEA"><a href="#4-6从零到一使用IDEA" class="headerlink" title="4.6从零到一使用IDEA"></a>4.6从零到一使用IDEA</h2><h3 id="创建本地库"><a href="#创建本地库" class="headerlink" title="创建本地库"></a>创建本地库</h3><p>将一个文件添加到ignore忽视</p><pre class="language-shell" data-language="shell"><code class="language-shell">#将数据添加到ignoregit add to .gitignore</code></pre><p>将文件添加exclude</p><pre class="language-shell" data-language="shell"><code class="language-shell">git add to exclude</code></pre><h3 id="创建版本"><a href="#创建版本" class="headerlink" title="创建版本"></a>创建版本</h3><pre class="language-shell" data-language="shell"><code class="language-shell">commitversion-0.1 Copy Revison Number</code></pre><h3 id="回退版本"><a href="#回退版本" class="headerlink" title="回退版本"></a>回退版本</h3><p><img src="https://i.loli.net/2020/12/25/oyQbqeJjfGzdlF5.png" alt="回退版本图"></p><p>然后将版本哈希值粘贴到HEAD地方</p><h3 id="创建分支以及合并分支"><a href="#创建分支以及合并分支" class="headerlink" title="创建分支以及合并分支"></a>创建分支以及合并分支</h3><p>创建分支</p><p><img src="https://i.loli.net/2020/12/25/48awkZFMH3X7q2B.png" alt="image-20201225163340577"></p><p>合并分支</p><p><img src="https://i.loli.net/2020/12/25/DyPaoRHQ5Ud2Y63.png" alt="image-20201225163935837"></p><h3 id="如何在idea里解决分支冲突"><a href="#如何在idea里解决分支冲突" class="headerlink" title="如何在idea里解决分支冲突"></a>如何在idea里解决分支冲突</h3><p>merge</p><h3 id="添加合作伙伴"><a href="#添加合作伙伴" class="headerlink" title="添加合作伙伴"></a>添加合作伙伴</h3><p>setting</p><p>manage access</p><h1 id="0x5-HEXO个人博客"><a href="#0x5-HEXO个人博客" class="headerlink" title="0x5. HEXO个人博客"></a>0x5. HEXO个人博客</h1><pre class="language-bash" data-language="bash"><code class="language-bash">git config --global user.name &quot;godweiyang&quot;git config --global user.email &quot;792321264@qq.com&quot;ssh-keygen -t rsa -C &quot;lijinxinok@163.com&quot;</code></pre><h1 id="0x6-配置图床"><a href="#0x6-配置图床" class="headerlink" title="0x6. 配置图床"></a>0x6. 配置图床</h1><p><img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_20_12/%E6%B5%8B%E8%AF%95.png" alt="测试图"></p><p>加油</p>]]></content>
      
      
      
        <tags>
            
            <tag> git idea </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark_SparkCore</title>
      <link href="2020/10/21/Spark_SparkCore/"/>
      <url>2020/10/21/Spark_SparkCore/</url>
      
        <content type="html"><![CDATA[<h1 id="spark概述"><a href="#spark概述" class="headerlink" title="spark概述"></a>spark概述</h1><h2 id="1-历史"><a href="#1-历史" class="headerlink" title="1.历史"></a>1.历史</h2><p>在之前的学习中，Hadoop的MapReduce是大家广为熟知的计算框架，那为什么咱们还要学习新的计算框架Spark呢，这里就不得不提到Spark和Hadoop的关系。</p><p>首先从时间节点上来看:</p><p>Hadoop</p><ol><li>2006年1月，Doug Cutting加入Yahoo，领导Hadoop的开发</li><li>2008年1月，Hadoop成为Apache顶级项目</li><li>2011年1.0正式发布</li><li>2012年3月稳定版发布</li><li>2013年10月发布2.X (Yarn)版本</li></ol><p>Spark</p><ol><li>2009年，Spark诞生于伯克利大学的AMPLab实验室</li><li>2010年，伯克利大学正式开源了Spark项目</li><li>2013年6月，Spark成为了Apache基金会下的项目</li><li>2014年2月，Spark以飞快的速度成为了Apache的顶级项目</li><li>2015年至今，Spark变得愈发火爆，大量的国内公司开始重点部署或者使用Spark</li></ol><h2 id="2-Spark核心模块"><a href="#2-Spark核心模块" class="headerlink" title="2.Spark核心模块"></a>2.Spark核心模块</h2><p><img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_21_01/Spark%E6%A0%B8%E5%BF%83%E6%A8%A1%E5%9D%97.png" alt="spark核心模块"></p><p><strong>Spark Core</strong></p><p>Spark Core中提供了Spark最基础与最核心的功能，Spark其他的功能如：Spark SQL，Spark Streaming，GraphX, MLlib都是在Spark Core的基础上进行扩展的</p><p><strong>Spark SQL</strong></p><p>Spark SQL是Spark用来操作结构化数据的组件。通过Spark SQL，用户可以使用SQL或者Apache Hive版本的SQL方言（HQL）来查询数据。</p><p><strong>Spark Streaming</strong></p><p>Spark Streaming是Spark平台上针对实时数据进行流式计算的组件，提供了丰富的处理数据流的API。</p><p><strong>Spark MLlib</strong></p><p>MLlib是Spark提供的一个机器学习算法库。MLlib不仅提供了模型评估、数据导入等额外的功能，还提供了一些更底层的机器学习原语。</p><p><strong>Spark GraphX</strong></p><p>GraphX是Spark面向图计算提供的框架与算法库。</p><h2 id="3-入门wordCount"><a href="#3-入门wordCount" class="headerlink" title="3.入门wordCount"></a>3.入门wordCount</h2><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql">object wordCountTest &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    &#x2F;&#x2F;spark标准获取流程    val sparkConf: SparkConf &#x3D; new SparkConf().setAppName(&quot;wordCount&quot;).setMaster(&quot;local[1]&quot;)    val sc &#x3D; new SparkContext(sparkConf)    &#x2F;&#x2F;创建RDD,使用textFile的方式创建RDD    val line: RDD[String] &#x3D; sc.textFile(&quot;.&#x2F;word.txt&quot;, 1)    val result: RDD[(String, Int)] &#x3D; line.flatMap(_.split(&quot; &quot;)).map((_, 1)).reduceByKey(_ + _)    &#x2F;&#x2F;打印    result.collect().foreach(println)    sc.close()  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="4-spark运行模式"><a href="#4-spark运行模式" class="headerlink" title="4.spark运行模式"></a>4.spark运行模式</h2><table><thead><tr><th>Spark运行环境</th><th>用法</th></tr></thead><tbody><tr><td>Local模式</td><td>测试</td></tr><tr><td>Standalone模式</td><td>独立部署Matser-Worker<br/>自身提供计算资源,降低了耦合性</td></tr><tr><td>Yarn模式</td><td>提供集群模式</td></tr><tr><td>K8S模式</td><td></td></tr><tr><td>Mesos模式</td><td>Twitter</td></tr><tr><td>Windows模式</td><td></td></tr></tbody></table><h2 id="5-spark端口号"><a href="#5-spark端口号" class="headerlink" title="5.spark端口号"></a>5.spark端口号</h2><table><thead><tr><th>Spark端口号</th><th>详解</th><th>Hadoop</th></tr></thead><tbody><tr><td>内部通信:==7077==</td><td>Spark Master内部通信服务端口号</td><td>8020/9000</td></tr><tr><td>Master资源监控:8080/改==8989==</td><td>Standalone模式下，Spark Master Web端口号</td><td>9870</td></tr><tr><td>Spark-Shell监控:==4040==</td><td>Spark查看当前Spark-shell运行任务情况端口号</td><td></td></tr><tr><td>Spark使用HDFS端口:==8020==</td><td></td><td></td></tr><tr><td>历史服务器UI端口:==18080==</td><td>Spark历史服务器端口号</td><td>19888</td></tr><tr><td>Yarn:==8088==</td><td>Hadoop YARN任务运行情况查看端口号</td><td>8088</td></tr></tbody></table><h2 id="6-Spark运行组件"><a href="#6-Spark运行组件" class="headerlink" title="6.Spark运行组件"></a>6.Spark运行组件</h2><p>Spark框架的核心是一个计算引擎，整体来说，它采用了标准 master-slave 的结构。</p><p><img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_21_01/Spark%E8%BF%90%E8%A1%8C%E7%BB%84%E4%BB%B6.png" alt="Spark运行组件"></p><h3 id="6-1Spark-Executor"><a href="#6-1Spark-Executor" class="headerlink" title="6.1Spark Executor"></a>6.1Spark Executor</h3><p>集群中运行在==工作节点（Worker）中的一个JVM进程==，是整个集群中的专门用于计算的节点。在提交应用中，可以提供参数指定计算节点的个数，以及对应的资源。这里的资源一般指的是工作节点Executor的内存大小和使用的虚拟CPU核（Core）数量。</p><table><thead><tr><th>名称</th><th>说明</th></tr></thead><tbody><tr><td>–num-executors</td><td>配置Executor的数量</td></tr><tr><td>–executor-memory</td><td>配置每个Executor的内存大小</td></tr><tr><td>–executor-cores</td><td>配置每个Executor的虚拟CPU   core数量</td></tr><tr><td></td><td>Executor如果是3核,设备是单核,模拟的多线程操作,其实是并发操作</td></tr></tbody></table><h3 id="6-2并行与并发"><a href="#6-2并行与并发" class="headerlink" title="6.2并行与并发"></a>6.2并行与并发</h3><p>我们会给Executor分配虚拟的核心数量,如果核心不够会触发多线程操作,并发</p><p>如果核心够用,则进行并行操作,可以进行配置</p><h3 id="6-3有向无环图"><a href="#6-3有向无环图" class="headerlink" title="6.3有向无环图"></a>6.3有向无环图</h3><p>表示一种依赖关系,依赖关系形成的拓扑图形称为DAG,有向无环图</p><pre class="mermaid">graph LRA-->BB-->CB-->DD--禁止-->A</pre><p>如图,D向A会形成无环图,有环会形成死循环(与maven类似)</p><h2 id="7-Yarn-Cluster任务提交流程"><a href="#7-Yarn-Cluster任务提交流程" class="headerlink" title="7.Yarn Cluster任务提交流程"></a>7.Yarn Cluster任务提交流程</h2><p><strong>核心</strong>:分两大块 1.资源的申请 2.计算的准备 任务发给资源</p><p>Client与Cluster区别在于Driver程序运行的节点位置</p><p><img src="https://i.loli.net/2020/12/23/JjlbBD6sYXgC3Hx.png" alt="Spark-Yarn提交流程"></p><ol><li>任务提交</li><li>向ResourceManager通讯申请启动ApplicationMaster</li><li>ApplicationMaster选择合适的节点借用NodeManager启动一个container</li><li>在container中运行AppMatser=Driver</li><li>Driver启动后,向RM申请container运行Executor进程</li><li>Executor进程启动后反向向Driver进行注册</li><li>全部注册完成后开始执行main函数</li><li>执行到action算子,触发一个job,根据是否发生shuffle开始划分stage</li><li>每个stage生成对应的TaskSet[task1,task2,task3…]</li><li>然后将task分发到各个Executor上执行</li></ol><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">[shenneng@hadoop102 spark-yarn]$ bin&#x2F;spark-submit \--class org.apache.spark.examples.SparkPi \--master yarn \--deploy-mode cluster \.&#x2F;examples&#x2F;jars&#x2F;spark-examples_2.12-3.0.0.jar \10<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="Spark框架"><a href="#Spark框架" class="headerlink" title="Spark框架"></a>Spark框架</h1><p>Spark和Hadoop的根本差异是多个作业之间的数据通信问题</p><p><img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_20_12/Spark%E8%BF%90%E8%A1%8C%E6%A8%A1%E5%BC%8F%E7%A4%BA%E6%84%8F%E5%9B%BE.png" alt="Spark运行模式示意图"></p><table><thead><tr><th>Spark运行环境</th><th>用法</th></tr></thead><tbody><tr><td>Local模式</td><td>测试</td></tr><tr><td>Standalone模式</td><td>独立部署Matser-Worker<br/>自身提供计算资源,降低了耦合性</td></tr><tr><td>Yarn模式</td><td>提供集群模式</td></tr><tr><td>K8S模式</td><td></td></tr><tr><td>Mesos模式</td><td>Twitter</td></tr><tr><td>Windows模式</td><td></td></tr></tbody></table><h2 id="Yarn模式"><a href="#Yarn模式" class="headerlink" title="Yarn模式"></a>Yarn模式</h2><p><img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_20_12/Yarn-Cluster%E6%8F%90%E4%BA%A4%E6%B5%81%E7%A8%8B.png" alt="Yarn-Cluster提交流程"></p><p>Client与Cluster的主要区别是Driver是否在本地运行</p><h2 id="分布式计算模拟"><a href="#分布式计算模拟" class="headerlink" title="分布式计算模拟"></a>分布式计算模拟</h2><p><img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_20_12/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97%E6%A0%B8%E5%BF%83.png" alt="分布式计算核心-拆分Task"></p><p>通过简单的分布式计算模拟,理解任务的拆分,运行的模块,并行的原理,RDD的封装,底层数据结构</p><p><strong>DRIVER</strong></p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">&#x2F;&#x2F;DRIVERpackage com.ecust.saprkcoreimport java.io.&#123;ObjectOutputStream, OutputStream&#125;import java.net.&#123;ServerSocket, Socket&#125;&#x2F;** * @author Jinxin Li * @create 2020-12-31 13:43 *&#x2F;object Driver &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    &#x2F;&#x2F;进行逻辑的封装,计算的准备,数据的提交    val client1 &#x3D; new Socket(&quot;localhost&quot;, 9999)    val client2 &#x3D; new Socket(&quot;localhost&quot;, 8888)    val out1: OutputStream &#x3D; client1.getOutputStream    val out2: OutputStream &#x3D; client2.getOutputStream    val objOut1 &#x3D; new ObjectOutputStream(out1)    val objOut2 &#x3D; new ObjectOutputStream(out2)    val  task &#x3D; new Task()    val subTask1 &#x3D; new SubTask()    subTask1.logic&#x3D;task.logic    subTask1.data&#x3D;task.data.take(2)    val subTask2 &#x3D; new SubTask()    subTask2.logic&#x3D;task.logic    subTask2.data&#x3D;task.data.takeRight(2)    objOut1.writeObject(subTask1)    objOut1.flush()    objOut1.close()    objOut2.writeObject(subTask2)    objOut2.flush()    objOut2.close()    &#x2F;&#x2F;发送,注意在网络中传递的数据要进行序列化,不可能传递对象,必须序列化    println(&quot;任务发送完毕&quot;)    &#x2F;&#x2F;关闭客户端    client1.close()    client2.close()  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>EXECUTOR1</strong></p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">object Executor1 &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    &#x2F;&#x2F;启动服务器,接受数据    val server &#x3D; new ServerSocket(9999)    println(&quot;服务器9999启动,等待接受数据...&quot;)    val client: Socket &#x3D; server.accept()    val in: InputStream &#x3D; client.getInputStream    val objIn &#x3D; new ObjectInputStream(in)    val task &#x3D; objIn.readObject().asInstanceOf[SubTask]    val ints: List[Int] &#x3D; task.compute()    println(&quot;接收到客户端9999接受的数据:&quot;+ints)    objIn.close()    client.close()    server.close()  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>EXECUTOR2</strong></p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">object Executor2 &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    &#x2F;&#x2F;启动服务器,接受数据    val server &#x3D; new ServerSocket(8888)    println(&quot;服务器9999启动,等待接受数据...&quot;)    val client: Socket &#x3D; server.accept()    val in: InputStream &#x3D; client.getInputStream    val objIn &#x3D; new ObjectInputStream(in)    val task&#x3D; objIn.readObject().asInstanceOf[SubTask]    val ints: List[Int] &#x3D; task.compute()    println(&quot;接收到客户端8888接受的数据:&quot;+ints)    objIn.close()    client.close()    server.close()  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>SUBTASK</strong></p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">class SubTask extends Serializable &#123;  &#x2F;&#x2F;这是一种特殊的数据结构,其中包含了数据的格式,数据的计算逻辑与算子转换  &#x2F;&#x2F;接收到数据之后,可以进行计算  &#x2F;&#x2F;RDD 广播变量 累加器 就是类似的数据结构  var data :List[Int] &#x3D; _  var logic:Int&#x3D;&gt;Int &#x3D; _  &#x2F;&#x2F;计算任务  def  compute() &#x3D;&#123;    data.map(logic)  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>TASK</strong></p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">class Task extends Serializable &#123;&#x2F;&#x2F;实现序列化 特质  &#x2F;&#x2F;包含原数据的数据结构  val data &#x3D; List(1, 2, 3, 4)  val function: Int &#x3D;&gt; Int &#x3D; (num: Int) &#x3D;&gt; &#123;    num * 2  &#125;  &#x2F;&#x2F;注意函数的类型是Int&#x3D;&gt;Int  val logic:Int&#x3D;&gt;Int &#x3D; _*2  &#x2F;&#x2F;计算任务  def  compute() &#x3D;&#123;    data.map(logic)  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="RDD与IO"><a href="#RDD与IO" class="headerlink" title="RDD与IO"></a>RDD与IO</h2><p><strong>字节流&amp;字符流</strong></p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">InputStream in &#x3D; new FileInputStream(&quot;path&quot;)int i &#x3D; -1while(i &#x3D; in.read()!&#x3D;-1)&#123;    println(i)&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="mermaid">graph LRFile-->FileInputStream--read-->console</pre><p><strong>缓冲流</strong></p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">InputStream in &#x3D; new BufferedInputStream(new FileInputStream(&quot;path&quot;))int i &#x3D; -1while(i &#x3D; in.read()!&#x3D;-1)&#123;    println(i)&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="mermaid">graph LRFile-->FileInputStream-->BufferedInputStream--read-->console</pre><p><img src="C:%5CUsers%5C89388%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20201231224057782.png" alt="缓冲区的缓冲流"></p><p><strong>转换流</strong>InputStreamReader</p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">Reader in &#x3D; new BufferedReader(    new InputStreamReader(        new FileInputStream(&quot;path&quot;),        &quot;UTF-8&quot;        )    )String s &#x3D; nullwhile((s&#x3D;in.readLine())!&#x3D;null)&#123;    println(i);    )<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_20_12/%E6%B5%81%E7%9A%84%E8%A3%85%E9%A5%B0%E8%80%85%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F.png" alt="装饰者设计模式的IO"></p><p>可以看出核心是FileInputFormat,转换流与缓冲流都是包装,这种设计模式成为装饰者设计模式</p><p>哪些inputformat,都是对读取逻辑的封装,没有真正的读取数据</p><p>readLine才会真正的执行,new的过程仅仅是建立连接,但是没有真正的读取,有种延迟加载的感觉</p><p>RDD的组装方式,与IO的封装也是非常的类似</p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">new HadoopRDD&#x2F;&#x2F;textFilenew MapPartitionsRDD()&#x2F;&#x2F;flatMapnew MapPartitionsRDD()&#x2F;&#x2F;mapnew ShuffleRDD()&#x2F;&#x2F;reduceByKey&#x2F;&#x2F;执行rdd.collect()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>一层一层的包装</p><p><img src="C:%5CUsers%5C89388%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20201231232951858.png" alt="RDD的装饰者模式理解"></p><p>RDD的数据处理方式类似于IO流,也有装饰者设计模式</p><p>RDD的数据只有在调用collect方法时,才会真正的执行</p><p>RDD是不保存数据的,但是IO可以临时保存一部分数据</p><h2 id="分区"><a href="#分区" class="headerlink" title="分区"></a>分区</h2><p>RDD是一个最基本的数据处理模型</p><p>类似于Kafka中的分区,我们将数据进行分区,分区之后分成不成的Task,可以分发至Executor进行计算</p><p>RDD是最小的数据处理单元,里面包含了分区信息,提高并行计算的能力</p><p><img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_20_12/RDD%E6%95%B0%E6%8D%AE%E7%9A%84%E5%88%87%E5%88%86.png" alt="数据的分区"></p><h1 id="spark核心"><a href="#spark核心" class="headerlink" title="spark核心"></a>spark核心</h1><h2 id="1-spark核心三大数据结构"><a href="#1-spark核心三大数据结构" class="headerlink" title="1.spark核心三大数据结构"></a>1.spark核心三大数据结构</h2><p>RDD : 弹性分布式数据集</p><p>累加器：分布式共享只写变量</p><p>广播变量：分布式共享只读变量</p><h2 id="2-RDD基本概念"><a href="#2-RDD基本概念" class="headerlink" title="2.RDD基本概念"></a>2.RDD基本概念</h2><p>RDD（Resilient Distributed Dataset）叫做弹性分布式数据集，是Spark中最基本的数据处理模型。代码中是一个抽象类，它代表一个弹性的、不可变、可分区、里面的元素可并行计算的集合。</p><p><strong>1.弹性</strong></p><p>存储的弹性：内存与磁盘的自动切换；</p><p>容错的弹性：数据丢失可以自动恢复；</p><p>计算的弹性：计算出错重试机制；</p><p>分片的弹性：可根据需要重新分片。</p><p><strong>2.分布式</strong>：数据存储在大数据集群不同节点上</p><p><strong>3.数据集</strong>：RDD封装了计算逻辑，并不保存数据</p><p><strong>4.数据抽象</strong>：RDD是一个抽象类，需要子类具体实现</p><p><strong>5.不可变</strong>：RDD封装了计算逻辑，是不可以改变的，想要改变，只能产生新的RDD，在新的RDD里面封装计算逻辑</p><p><strong>6.可分区、并行计算</strong></p><h2 id="3-RDD核心属性"><a href="#3-RDD核心属性" class="headerlink" title="3.RDD核心属性"></a>3.RDD核心属性</h2><p><a href="https://data-flair.training/blogs/spark-rdd-tutorial/">RDD详细描述</a></p><p><img src="https://i.loli.net/2020/12/23/PTqaCxnHeBdl91G.png" alt="SparkRDD的核心属性"></p><p>图:Spark RDD核心属性</p><ol><li><p>粗粒度操作(无法对单个元素进行操作)</p></li><li><p>内存中计算</p></li><li><p>懒执行</p></li><li><p>不变性</p></li><li><p>容错性</p></li><li><p>持久性(cache可以选择等级与checkpoint)</p><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql">--数据缓存wordToOneRdd.cache()--可以更改存储级别mapRdd.persist(StorageLevel.MEMORY_AND_DISK_2)--设置检查点路径sc.setCheckpointDir(&quot;.&#x2F;checkpoint1&quot;)--数据检查点：针对wordToOneRdd做检查点计算wordToOneRdd.checkpoint()--一般两者联合使用<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>可分区(分区列表)</p></li><li><p>粘度分区(自定分区)</p></li></ol><h2 id="4-RDD缺点"><a href="#4-RDD缺点" class="headerlink" title="4.RDD缺点"></a>4.RDD缺点</h2><ol><li>没有内置的优化引擎,RDD无法利用Spark的高级优化器（包括catalyst optimizer与Tungsten执行引擎）的优势。开发人员需要根据其属性优化每个RDD</li><li>只能处理结构化数据与DataFrame和数据集不同，RDD不会推断所摄取数据的模式，而是需要用户指定它。</li><li>性能限制,作为内存中的JVM对象，RDD涉及垃圾收集和Java序列化的开销，这在数据增长时非常昂贵。</li><li>没有足够的内存来存储RDD时，它们会拖慢运行速度。也可以将RDD的该分区存储在不适合RAM的磁盘上。结果，它将提供与当前数据并行系统类似的性能。</li></ol><h2 id="5-RDD的来源"><a href="#5-RDD的来源" class="headerlink" title="5.RDD的来源"></a>5.RDD的来源</h2><ol><li>使用集合创建parallelize MakeRDD</li><li>外部存储文件创建RDD textfile</li><li>从其他RDD创建(血缘关系,cache,checkpoint)</li><li>直接创建RDD 内部使用</li></ol><h2 id="6-RDD的-分区分片问题"><a href="#6-RDD的-分区分片问题" class="headerlink" title="6.RDD的==分区分片问题=="></a>6.RDD的==分区分片问题==</h2><p>RDD分区意味着一个分区一个job么,</p><p>RDD分区3意味着要在三个executor里执行么</p><p>重新分区,加入三个executor在不同的container里是如何发生shuffle里的,还是三个分区是一个job,这一个job在一个container里执行</p><h2 id="7-RDD的序列化"><a href="#7-RDD的序列化" class="headerlink" title="7.RDD的序列化"></a>7.RDD的序列化</h2><p>从计算的角度, 算子以外的代码都是在Driver端执行, 算子里面的代码都是在Executor端执行。</p><p>那么在scala的函数式编程中，就会导致算子内经常会用到算子外的数据，这样就形成了闭包的效果，如果使用的算子外的数据无法序列化，就意味着无法传值给Executor端执行，就会发生错误</p><p>所以需要在执行任务计算前，检测闭包内的对象是否可以进行序列化，这个操作我们称之为闭包检测。Scala2.12版本后闭包编译方式发生了改变  </p><p><strong>Kryo序列化框架</strong></p><p>Spark2.0开始支持另外一种Kryo序列化机制。Kryo速度是Serializable的10倍。当RDD在Shuffle数据的时候，简单数据类型、数组和字符串类型已经在Spark内部使用Kryo来序列化。</p><p>即使使用Kryo序列化，也要继承Serializable接口</p><h2 id="8-RDD依赖与血缘"><a href="#8-RDD依赖与血缘" class="headerlink" title="8.RDD依赖与血缘"></a>8.RDD依赖与血缘</h2><h3 id="8-1概述"><a href="#8-1概述" class="headerlink" title="8.1概述"></a>8.1概述</h3><p>RDD的Lineage会记录RDD的元数据信息和转换行为，当该RDD的部分分区数据丢失时，它可以根据这些信息来重新运算和恢复丢失的数据分区。</p><p>相邻的两个RDD之间的关系称为<strong>依赖关系</strong></p><p>多个连续的RDD的依赖关系,称之为<strong>血缘关系</strong></p><p>我们的每一个RDD都会保存我们的血缘关系,会保存之前的血缘关系</p><p>RDD为了提供容错性,需要将RDD间的关系保存下来,一旦出现错误,可以根据血缘关系将数据源重新计算</p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">val rdd1 &#x3D; rdd.map(_.2)&#x2F;&#x2F;新的RDD依赖于旧的RDD<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre class="mermaid">graph LRRDD1--依赖-->RDD2--依赖-->RDD3--依赖-->RDD4RDD4--flatmap-->RDD3RDD3--map-->RDD2RDD2--reduceByKey-->RDD1</pre><h3 id="8-2血缘关系的查看"><a href="#8-2血缘关系的查看" class="headerlink" title="8.2血缘关系的查看"></a>8.2血缘关系的查看</h3><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">&#x2F;&#x2F;血缘关系的演示&#x2F;&#x2F;每个RDD记录了以前所有的血缘关系package com.testimport org.apache.spark.api.java.JavaSparkContext.fromSparkContextimport org.apache.spark.rdd.RDDimport org.apache.spark.&#123;SparkConf, SparkContext&#125;&#x2F;** * @author Jinxin Li * @create 2020-10-26 10:04 *&#x2F;object wordCount &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    val config: SparkConf &#x3D; new SparkConf().setAppName(&quot;MyWordCount&quot;).setMaster(&quot;local[*]&quot;)    val sc &#x3D; new SparkContext(config)    val lines: RDD[String] &#x3D; sc.textFile(&quot;.&#x2F;word.txt&quot;)    println(&quot;++++++++++++++++++++++++++++++++++&#x3D;&quot;)    println(lines.toDebugString)&#x2F;&#x2F;打印血缘关系    val words: RDD[String] &#x3D; lines.flatMap(_.split(&quot; &quot;))    println(&quot;++++++++++++++++++++++++++++++++++&#x3D;&quot;)    println(words.toDebugString)    val pairs: RDD[(String, Int)] &#x3D; words.map((_, 1))    println(&quot;++++++++++++++++++++++++++++++++++&#x3D;&quot;)    println(pairs.toDebugString)    val word: RDD[(String, Int)] &#x3D; pairs.reduceByKey(_ + _)    println(&quot;++++++++++++++++++++++++++++++++++&#x3D;&quot;)    println(word.toDebugString)    println(&quot;++++++++++++++++++++++++++++++++++&#x3D;&quot;)    word.collect().foreach(println(_))    sc.close;  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-java" data-language="java"><code class="language-java">++++++++++++++++++++++++++++++++++&#x3D;(2) .&#x2F;word.txt MapPartitionsRDD[1] at textFile at wordCount.scala:15 [] |  .&#x2F;word.txt HadoopRDD[0] at textFile at wordCount.scala:15 []++++++++++++++++++++++++++++++++++&#x3D;(2) MapPartitionsRDD[2] at flatMap at wordCount.scala:18 [] |  .&#x2F;word.txt MapPartitionsRDD[1] at textFile at wordCount.scala:15 [] |  .&#x2F;word.txt HadoopRDD[0] at textFile at wordCount.scala:15 []++++++++++++++++++++++++++++++++++&#x3D;(2) MapPartitionsRDD[3] at map at wordCount.scala:21 [] |  MapPartitionsRDD[2] at flatMap at wordCount.scala:18 [] |  .&#x2F;word.txt MapPartitionsRDD[1] at textFile at wordCount.scala:15 [] |  .&#x2F;word.txt HadoopRDD[0] at textFile at wordCount.scala:15 []++++++++++++++++++++++++++++++++++&#x3D;(2) ShuffledRDD[4] at reduceByKey at wordCount.scala:24 []    &#x2F;&#x2F;这个地方断开,表示shuffle +- +-(2) MapPartitionsRDD[3] at map at wordCount.scala:21 []    |  MapPartitionsRDD[2] at flatMap at wordCount.scala:18 []    |  .&#x2F;word.txt MapPartitionsRDD[1] at textFile at wordCount.scala:15 []    |  .&#x2F;word.txt HadoopRDD[0] at textFile at wordCount.scala:15 []++++++++++++++++++++++++++++++++++&#x3D;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>可以看出每个RDD会存储所有的血缘关系</p><p>同时使用dependices可以查看依赖关系</p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">object wordCount &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    val config: SparkConf &#x3D; new SparkConf().setAppName(&quot;MyWordCount&quot;).setMaster(&quot;local[*]&quot;)    val sc &#x3D; new SparkContext(config)    val lines: RDD[String] &#x3D; sc.textFile(&quot;.&#x2F;word.txt&quot;)    println(&quot;++++++++++++++++++++++++++++++++++&#x3D;&quot;)    println(lines.dependencies)    val words: RDD[String] &#x3D; lines.flatMap(_.split(&quot; &quot;))    println(&quot;++++++++++++++++++++++++++++++++++&#x3D;&quot;)    println(words.dependencies)    val pairs: RDD[(String, Int)] &#x3D; words.map((_, 1))    println(&quot;++++++++++++++++++++++++++++++++++&#x3D;&quot;)    println(pairs.dependencies)    val word: RDD[(String, Int)] &#x3D; pairs.reduceByKey(_ + _)    println(&quot;++++++++++++++++++++++++++++++++++&#x3D;&quot;)    println(word.dependencies)    println(&quot;++++++++++++++++++++++++++++++++++&#x3D;&quot;)    word.collect().foreach(println(_))    sc.close;  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">++++++++++++++++++++++++++++++++++&#x3D;List(org.apache.spark.OneToOneDependency@1a2bcd56)++++++++++++++++++++++++++++++++++&#x3D;List(org.apache.spark.OneToOneDependency@3c3a0032)++++++++++++++++++++++++++++++++++&#x3D;List(org.apache.spark.OneToOneDependency@5e519ad3)++++++++++++++++++++++++++++++++++&#x3D;List(org.apache.spark.ShuffleDependency@765d55d5)++++++++++++++++++++++++++++++++++&#x3D;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>可以看出存在两种依赖关系,一种OneToOneDependency与ShuffleDependency</p><p>新的RDD的一个分区的数据依赖于旧的RDD的一个分区的数据,这种依赖称之为OneToOne依赖</p><p>新的RDD的一个分区的数据依赖于旧的RDD的多个分区的数据,这种依赖称为Shuffle依赖(数据被打乱重新组合)</p><p>源码中的依赖关系</p><p><img src="https://i.loli.net/2020/12/23/zWdaBIDnpNuxlr7.png" alt="RDD依赖关系的继承关系"></p><p><img src="https://i.loli.net/2020/12/23/w2M9xlshmoVAHL4.png" alt="宽依赖的图"></p><h3 id="8-3阶段划分与源码"><a href="#8-3阶段划分与源码" class="headerlink" title="8.3阶段划分与源码"></a>8.3阶段划分与源码</h3><p>Shuffle划分阶段</p><p>如果是oneToOne不需要划分阶段</p><p>不同的阶段要保证Task执行完毕才能执行下一个阶段</p><p>阶段的数量等于shuffle依赖的数量+1</p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">collectdagScheduler.runjobval waiter &#x3D; submitJob&#x2F;&#x2F;DAGScheduler-681&#x2F;&#x2F;让下翻override def run(): Unit &#x3D; eventProcessLoop.post(JobSubmitted)&#x2F;&#x2F;DAGScheduler-714private[scheduler] def handleJobSubmitted&#x2F;&#x2F;DAGScheduler-975&#123;    var finalStage: ResultStage &#x3D; null&#x2F;&#x2F;判定finalStage是否存在 985    finalStage &#x3D; createResultStage(finalRDD, func, partitions, jobId, callSite)&#x2F;&#x2F;如果不存在则创建一个空的ResultStage 986&#125;&#x2F;&#x2F;也就是说ResultStage只有一个private def createResultStage:ResultStage &#x3D; &#123;    &#x2F;&#x2F;445    val parents &#x3D; getOrCreateParentStages(rdd, jobId)&#x2F;&#x2F;有没有上一个阶段,这个rdd是当前的reduceBykey最后的rdd    val stage &#x3D; new ResultStage(id, rdd, func, partitions, parents, jobId, callSite)    stage  &#125;&#x2F;**获得父阶段列表*&#x2F;private def getOrCreateParentStages(rdd: RDD[_], firstJobId: Int): List[Stage] &#x3D; &#123;&#x2F;&#x2F;466    getShuffleDependencies(rdd).map &#123; shuffleDep &#x3D;&gt;      getOrCreateShuffleMapStage(shuffleDep, firstJobId)&#x2F;&#x2F;一个shuffle就会转换为一个阶段    &#125;.toList  &#125;private[scheduler] def getShuffleDependencies(&#x2F;&#x2F;508      rdd: RDD[_]): HashSet[ShuffleDependency[_, _, _]] &#x3D; &#123;    val parents &#x3D; new HashSet[ShuffleDependency[_, _, _]]    val visited &#x3D; new HashSet[RDD[_]]    val waitingForVisit &#x3D; new ListBuffer[RDD[_]]    waitingForVisit +&#x3D; rdd&#x2F;&#x2F;放入当前rdd reduceByKey的rdd    while (waitingForVisit.nonEmpty) &#123;      val toVisit &#x3D; waitingForVisit.remove(0)      if (!visited(toVisit)) &#123;&#x2F;&#x2F;判断之前是否访问过        visited +&#x3D; toVisit        toVisit.dependencies.foreach &#123;          case shuffleDep: ShuffleDependency[_, _, _] &#x3D;&gt;            parents +&#x3D; shuffleDep&#x2F;&#x2F;模式匹配判断是否是shuffle依赖          case dependency &#x3D;&gt;            waitingForVisit.prepend(dependency.rdd)        &#125;      &#125;    &#125;    parents  &#125;private def getOrCreateShuffleMapStage( &#x2F;&#x2F;338    ... ShuffleMapStage &#x3D; &#123;...        createShuffleMapStage(shuffleDep, firstJobId)                          &#125;     def createShuffleMapStage[K, V, C](&#x2F;&#x2F;384      ... ShuffleMapStage &#x3D; &#123;    val rdd &#x3D; shuffleDep.rdd          ...    val numTasks &#x3D; rdd.partitions.length    val parents &#x3D; getOrCreateParentStages(rdd, jobId)    val id &#x3D; nextStageId.getAndIncrement()    val stage &#x3D; new ShuffleMapStage()      ...    stage  &#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="8-4RDD的任务划分"><a href="#8-4RDD的任务划分" class="headerlink" title="8.4RDD的任务划分"></a>8.4RDD的任务划分</h3><p>行动算子底层是runJob</p><p><strong>Application</strong>:初始化一个SparkContext即生成一个Application</p><p><strong>Job</strong>:一个Action算子就会生成一个Job</p><p><strong>Stage</strong>:Stage等于宽依赖(ShuffleDependency)+1</p><p><strong>Task</strong>:一个Stage阶段中,最后一个RDD分区个数就是Task的个数</p><p>注意:Application-&gt;Job-&gt;Stage-&gt;Task每一层都是一对n的关系</p><p>提交过程是一个阶段一个阶段的提交</p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">private def submitStage(stage: Stage): Unit &#x3D; &#123;&#x2F;&#x2F;1084    val jobId &#x3D; activeJobForStage(stage)    if (jobId.isDefined) &#123;      logDebug(s&quot;submitStage($stage (name&#x3D;$&#123;stage.name&#125;;&quot; +        s&quot;jobs&#x3D;$&#123;stage.jobIds.toSeq.sorted.mkString(&quot;,&quot;)&#125;))&quot;)      if (!waitingStages(stage) &amp;&amp; !runningStages(stage) &amp;&amp; !failedStages(stage)) &#123;        val missing &#x3D; getMissingParentStages(stage).sortBy(_.id)&#x2F;&#x2F;有没有上一级阶段        logDebug(&quot;missing: &quot; + missing)        if (missing.isEmpty) &#123;&#x2F;&#x2F;如果没有上一级的stage,则为空          logInfo(&quot;Submitting &quot; + stage + &quot; (&quot; + stage.rdd + &quot;), which has no missing parents&quot;)          submitMissingTasks(stage, jobId.get)&#x2F;&#x2F;为空就提交stage&#x2F;tasks        &#125; else &#123;          for (parent &lt;- missing) &#123;            submitStage(parent)          &#125;          waitingStages +&#x3D; stage        &#125;      &#125;    &#125; else &#123;      abortStage(stage, &quot;No active job for stage &quot; + stage.id, None)    &#125;  &#125;val tasks: Seq[Task[_]] &#x3D; try &#123;&#x2F;&#x2F;1217      val serializedTaskMetrics &#x3D; closureSerializer.serialize(stage.latestInfo.taskMetrics).array()      stage match &#123;&#x2F;&#x2F;匹配的阶段类型        case stage: ShuffleMapStage &#x3D;&gt;&#x2F;&#x2F;shuffleMaptask          &#x2F;&#x2F;new 几个跟map相关,ShuffleMapStage          stage.pendingPartitions.clear()          partitionsToCompute.map &#123; id &#x3D;&gt;&#x2F;&#x2F;计算分区            val locs &#x3D; taskIdToLocations(id)            val part &#x3D; partitions(id)            stage.pendingPartitions +&#x3D; id            new ShuffleMapTask(stage.id, stage.latestInfo.attemptNumber,              taskBinary, part, locs, properties, serializedTaskMetrics, Option(jobId),              Option(sc.applicationId), sc.applicationAttemptId, stage.rdd.isBarrier())          &#125;        case stage: ResultStage &#x3D;&gt;          partitionsToCompute.map &#123; id &#x3D;&gt;&#x2F;&#x2F;这里面有多少元素            val p: Int &#x3D; stage.partitions(id)            val part &#x3D; partitions(p)            val locs &#x3D; taskIdToLocations(id)&#x2F;&#x2F;到底有多个new            new ResultTask(stage.id, stage.latestInfo.attemptNumber,              taskBinary, part, locs, id, properties, serializedTaskMetrics,              Option(jobId), Option(sc.applicationId), sc.applicationAttemptId,              stage.rdd.isBarrier())          &#125;      &#125;&#125;&#x2F;&#x2F;计算分区&#x2F;&#x2F; Figure out the indexes of partition ids to compute.val partitionsToCompute: Seq[Int] &#x3D; stage.findMissingPartitions()&#123;&#125;    &#x2F;&#x2F;ResultStage 61override def findMissingPartitions(): Seq[Int] &#x3D; &#123;    val job &#x3D; activeJob.get    (0 until job.numPartitions).filter(id &#x3D;&gt; !job.finished(id))&#125;&#x2F;&#x2F;此处的job.numPartitions就是最后一个RDD的分区&#x2F;&#x2F;三个分区就是0-3&#x2F;&#x2F;一个RDD的三个分区,从并行角度就会分配为3个Task&#x2F;&#x2F;SuffleMapStage 91override def findMissingPartitions(): Seq[Int] &#x3D; &#123;    mapOutputTrackerMaster      .findMissingPartitions(shuffleDep.shuffleId)      .getOrElse(0 until numPartitions)  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>一个应用程序会对应多个job(一个行动算子算是一个job)</p><p>ShuffleMapStage=&gt;ShuffleMapTask</p><p>ResultStage=&gt;ResultTask</p><h2 id="9-RDD分区器"><a href="#9-RDD分区器" class="headerlink" title="9.RDD分区器"></a>9.RDD分区器</h2><ol><li>Hash分区(默认)</li><li>Range分区</li><li>自定义分区</li></ol><p>只有Key-Value类型的RDD才有分区器，非Key-Value类型的RDD分区的值是None</p><p>每个RDD的分区ID范围：0 ~ (numPartitions - 1)，决定这个值是属于那个分区的。</p><p><strong>Hash</strong>分区：对于给定的key，计算其hashCode,并除以分区个数取余</p><p><strong>Range分区</strong>：将一定范围内的数据映射到一个分区中，尽量保证每个分区数据均匀，而且分区间有序</p><h2 id="10-RDD累加器"><a href="#10-RDD累加器" class="headerlink" title="10.RDD累加器"></a>10.RDD累加器</h2><p><strong>系统累加器</strong></p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">object MyAccumulator &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    val sc &#x3D; new SparkContext(new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;myAccumulator&quot;))    val rdd: RDD[Int] &#x3D; sc.makeRDD(List(1, 2, 3, 4, 5))    &#x2F;&#x2F;声明系统累加器    val sum: LongAccumulator &#x3D; sc.longAccumulator(&quot;sum&quot;)    rdd.foreach(      num&#x3D;&gt;&#123;        sum.add(num)      &#125;    )    &#x2F;&#x2F;获取累加器    println(sum.value)  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>自定义累加器</strong></p><ol><li>自定wordcount累加器</li></ol><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">&#x2F;&#x2F;自定义累加器实现wordcountclass DefineAccumulator extends AccumulatorV2[String,mutable.Map[String,Int]]&#123;  val map: mutable.Map[String, Int] &#x3D; mutable.Map()  &#x2F;&#x2F;判断累加器是否为初始状态  override def isZero: Boolean &#x3D; &#123;map.isEmpty&#125;  &#x2F;&#x2F;复制累加器  override def copy(): AccumulatorV2[String, mutable.Map[String, Int]] &#x3D; &#123;new DefineAccumulator()&#125;  &#x2F;&#x2F;重置累加器  override def reset(): Unit &#x3D; map.clear()  &#x2F;&#x2F;区内相加  override def add(v: String): Unit &#x3D; &#123;    &#x2F;&#x2F;区内相加的定义,如果存在元素,就对key值+1,如果不存在,就添加当前元素,key+1    map(v)&#x3D;map.getOrElse(v,0)+1  &#125;  &#x2F;&#x2F;区间相加  override def merge(other: AccumulatorV2[String, mutable.Map[String, Int]]): Unit &#x3D; &#123;    &#x2F;&#x2F;区间相加,固定    val map1: mutable.Map[String, Int] &#x3D; this.value    val map2: mutable.Map[String, Int] &#x3D; other.value    map2.foreach&#123;      case (k,v) &#x3D;&gt; map1(k)&#x3D;map1.getOrElse(k,0)+v    &#125;  &#125;  override def value: mutable.Map[String, Int] &#x3D; map&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ol start="2"><li>注册并使用定义累加器</li></ol><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">object MyAccumulator2 &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    val sc &#x3D; new SparkContext(new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;accumulator&quot;))    val word: RDD[String] &#x3D; sc.textFile(&quot;.&#x2F;word.txt&quot;)    val words &#x3D; word.flatMap(_.split(&quot; &quot;))    &#x2F;&#x2F;new出累加器    val uacc &#x3D; new DefineAccumulator    &#x2F;&#x2F;注册累加器    sc.register(uacc)    &#x2F;&#x2F;使用累加器    words.foreach(uacc.add(_))    println(uacc.value)&#x2F;&#x2F;注意输出为accumulator的值  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="11-广播变量"><a href="#11-广播变量" class="headerlink" title="11.广播变量"></a>11.广播变量</h2><p>广播变量用来高效分发较大的对象。向所有工作节点发送一个较大的只读值，以供一个或多个Spark操作使用。比如，如果你的应用需要向所有节点发送一个较大的只读查询表，广播变量用起来都很顺手。在多个并行操作中使用同一个变量，但是 Spark会为每个任务分别发送。</p><p>在整个队列中,仅仅存在一次</p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">object BoardCast &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    val sc &#x3D; new SparkContext(new SparkConf().setAppName(&quot;emm&quot;).setMaster(&quot;local[*]&quot;))    val rdd1 &#x3D; sc.makeRDD(List( (&quot;a&quot;,1), (&quot;b&quot;, 2), (&quot;c&quot;, 3), (&quot;d&quot;, 4) ),4)    val list &#x3D; List((&quot;a&quot;,4), (&quot;b&quot;, 5), (&quot;c&quot;, 6), (&quot;d&quot;, 7))    val broadcast: Broadcast[List[(String, Int)]] &#x3D; sc.broadcast(list)    val value &#x3D; rdd1.map &#123;      case (key, num) &#x3D;&gt; &#123;        var num1 &#x3D; 0        for ((k, v) &lt;- broadcast.value) &#123;          if (k &#x3D;&#x3D; key)&#123;            num1&#x3D;v          &#125;        &#125;        (key, num+num1)      &#125;    &#125;    value.collect().foreach(println)  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="12-RDD的持久化"><a href="#12-RDD的持久化" class="headerlink" title="12.RDD的持久化"></a>12.RDD的持久化</h2><h3 id="12-1为什么要使用RDD的持久化"><a href="#12-1为什么要使用RDD的持久化" class="headerlink" title="12.1为什么要使用RDD的持久化"></a>12.1为什么要使用RDD的持久化</h3><p>数据不存储在RDD中</p><p><img src="https://i.loli.net/2020/12/25/m4gxMj5cHAh8KaG.png" alt="RDD的重用"></p><p>如果一个RDD需要重复使用,需要从头再次执行来获取数据</p><p>RDD的对象可以重用,但是数据没法重用</p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">object SparkCore02_RDD_Persist &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    val sparkConf: SparkConf &#x3D; new SparkConf().setAppName(&quot;WordCountPersist&quot;).setMaster(&quot;local[1]&quot;)    val sc &#x3D; new SparkContext(sparkConf)    val list &#x3D; List(&quot;Hello Spark&quot;, &quot;Hello Scala&quot;)    &#x2F;&#x2F;生成RDD RDD中不存储数据    val listRDD: RDD[String] &#x3D; sc.makeRDD(list, 1)    val wordRDD: RDD[String] &#x3D; listRDD.flatMap(word &#x3D;&gt; word.split(&quot; &quot;))    val tupleRDD: RDD[(String, Int)] &#x3D; wordRDD.map(word &#x3D;&gt;    &#123; println(&quot;map阶段&quot;)      (word, 1)    &#125;)    &#x2F;&#x2F;分组的操作    val groupRDD: RDD[(String, Iterable[Int])] &#x3D; tupleRDD.groupByKey()    val resultRDD: RDD[(String, Int)] &#x3D; tupleRDD.reduceByKey(_ + _)    resultRDD.collect().foreach(println)    println(&quot;------------华丽分割线----------------&quot;)    groupRDD.collect().foreach(println)    sc.stop()  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">&#x2F;&#x2F;结果map阶段map阶段map阶段map阶段(Spark,1)(Hello,2)(Scala,1)------------华丽分割线----------------map阶段map阶段map阶段map阶段(Spark,CompactBuffer(1))(Hello,CompactBuffer(1, 1))(Scala,CompactBuffer(1))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>发现map阶段运行了两波,所有的执行都会从头开始计算</p><p>这样的执行影响了效率</p><p>要想解决这个问题,数据持久化提高效率</p><p><img src="https://i.loli.net/2020/12/25/MAwm1jeOrELZWqK.png" alt="RDD持久化的作用"></p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">&#x2F;&#x2F;进行缓存tupleRDD.cache() &#x2F;&#x2F;本质是persist&#x2F;&#x2F;tupleRDD.cache()tupleRDD.persist(StorageLevel.MEMORY_AND_DISK)&#x2F;&#x2F;memory_only当内存不够的情况下,数据不能溢写到磁盘,会丢失数据&#x2F;&#x2F;memory_and_disk当内存不够的情况下,会将数据落到磁盘&#x2F;&#x2F;持久化操作,必须在行动算子执行时,完成的sc.setCheckpointDir(&quot;.&#x2F;checkPoint&quot;)&#x2F;&#x2F;一般要保存到分布式存储中tupleRDD.checkpoint()&#x2F;&#x2F;检查点路径,在作业执行完毕之后也是不会删除的<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>RDD对象的持久化操作不一定为了重用,在数据执行较长,或者数据比较重要的场合也可以进行持久化操作</p><h3 id="12-2-三种持久化方法"><a href="#12-2-三种持久化方法" class="headerlink" title="12.2 三种持久化方法"></a>12.2 三种持久化方法</h3><ol><li><p><strong>cache</strong>:将数据临时存储在内存中进行数据重用,会添加新的依赖,出现问题从头开始计算</p></li><li><p><strong>persist</strong>:将数据临时存储在磁盘文件中进行数据重用,涉及到磁盘IO,性能较低,但是数据安全,如果作业执行完毕,临时保存在数据文件就会丢失</p></li><li><p><strong>checkpoint</strong>:将磁盘长久地保存在磁盘文件中进行数据重用,涉及到磁盘IO时,性能较低,但是会切断血缘关系,相当于改变数据源</p><blockquote><p>但是数据安全,为了保证数据安全,所以一般情况下,会独立的执行作业,为了能够提高效率,一般情况下,会跟cache联合使用,先cache在使用checkpoint这个时候会保存cache的文件,而不会独立的跑一个单独的任务</p></blockquote></li></ol><pre class="mermaid">graph LRsc-->map-reduceByKey-->cache--保存-->CheckPointcache-->collect</pre><p><strong>大区别</strong></p><p>cache会添加新的依赖</p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">package com.ecust.rdd.persistimport org.apache.spark.rdd.RDDimport org.apache.spark.storage.StorageLevelimport org.apache.spark.&#123;SparkConf, SparkContext&#125;&#x2F;** * @author Jinxin Li * @create 2020-12-24 14:03 *&#x2F;object SparkCore03_RDD_CheckPoint &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    val sparkConf: SparkConf &#x3D; new SparkConf().setAppName(&quot;WordCountPersist&quot;).setMaster(&quot;local[1]&quot;)    val sc &#x3D; new SparkContext(sparkConf)    val list &#x3D; List(&quot;Hello Spark&quot;, &quot;Hello Scala&quot;)    val listRDD: RDD[String] &#x3D; sc.makeRDD(list, 1)    val wordRDD: RDD[String] &#x3D; listRDD.flatMap(word &#x3D;&gt; word.split(&quot; &quot;))    val tupleRDD: RDD[(String, Int)] &#x3D; wordRDD.map(word &#x3D;&gt; (word, 1))    tupleRDD.cache()    println(tupleRDD.toDebugString)&#x2F;&#x2F;运行之前看血缘关系&#x2F;&#x2F;    tupleRDD.persist(StorageLevel.MEMORY_AND_DISK)&#x2F;&#x2F;    sc.setCheckpointDir(&quot;.&#x2F;checkPoint&quot;)&#x2F;&#x2F;    tupleRDD.checkpoint()    val groupRDD: RDD[(String, Iterable[Int])] &#x3D; tupleRDD.groupByKey()    groupRDD.collect().foreach(println)    println(&quot;----------------------------&quot;)    println(tupleRDD.toDebugString)&#x2F;&#x2F;运行之后看血缘    sc.stop()  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">(1) MapPartitionsRDD[2] at map at SparkCore03_RDD_CheckPoint.scala:21 [Memory Deserialized 1x Replicated] |  MapPartitionsRDD[1] at flatMap at SparkCore03_RDD_CheckPoint.scala:20 [Memory Deserialized 1x Replicated] |  ParallelCollectionRDD[0] at makeRDD at SparkCore03_RDD_CheckPoint.scala:19 [Memory Deserialized 1x Replicated](Spark,CompactBuffer(1))(Hello,CompactBuffer(1, 1))(Scala,CompactBuffer(1))----------------------------(1) MapPartitionsRDD[2] at map at SparkCore03_RDD_CheckPoint.scala:21 [Memory Deserialized 1x Replicated] |       CachedPartitions: 1; MemorySize: 368.0 B; ExternalBlockStoreSize: 0.0 B; DiskSize: 0.0 B&#x2F;&#x2F;这里添加了新的依赖 |  MapPartitionsRDD[1] at flatMap at SparkCore03_RDD_CheckPoint.scala:20 [Memory Deserialized 1x Replicated] |  ParallelCollectionRDD[0] at makeRDD at SparkCore03_RDD_CheckPoint.scala:19 [Memory Deserialized 1x Replicated]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>因此cache会在血缘关系中添加新的依赖,一旦出现问题,可以重头读取数据</p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">&#x2F;&#x2F;使用checkPoint之后(1) MapPartitionsRDD[2] at map at SparkCore03_RDD_CheckPoint.scala:21 [] |  MapPartitionsRDD[1] at flatMap at SparkCore03_RDD_CheckPoint.scala:20 [] |  ParallelCollectionRDD[0] at makeRDD at SparkCore03_RDD_CheckPoint.scala:19 [](Spark,CompactBuffer(1))(Hello,CompactBuffer(1, 1))(Scala,CompactBuffer(1))----------------------------(1) MapPartitionsRDD[2] at map at SparkCore03_RDD_CheckPoint.scala:21 [] |  ReliableCheckpointRDD[4] at collect at SparkCore03_RDD_CheckPoint.scala:31 []<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>使用checkPoint会切断血缘关系,重新建立新的血缘关系等同于改变数据源</p><h3 id="12-3源码解析CheckPoint单独执行任务"><a href="#12-3源码解析CheckPoint单独执行任务" class="headerlink" title="12.3源码解析CheckPoint单独执行任务"></a>12.3源码解析CheckPoint单独执行任务</h3><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">&#x2F;&#x2F;SparkContext.scala 2093-2095dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get)    progressBar.foreach(_.finishAll())    rdd.doCheckpoint()&#x2F;&#x2F;--&gt;&#x2F;&#x2F;runJob之后调用了doCheckPoint()方法&#x2F;&#x2F;-----------------------------&#x2F;&#x2F;RDD.scala 1789-1805 doCheckPointif (checkpointData.isDefined) &#123;    if (checkpointAllMarkedAncestors) &#123;        dependencies.foreach(_.rdd.doCheckpoint())          &#125;    checkpointData.get.checkpoint()&#x2F;&#x2F;如果需要checkPoint然后进行checkPoint&#125; else &#123;    dependencies.foreach(_.rdd.doCheckpoint())&#125;&#x2F;&#x2F;----------------------------------\&#x2F;&#x2F;org.apache.spark.rdd.LocalRDDCheckpointData 53-54if (missingPartitionIndices.nonEmpty) &#123;      rdd.sparkContext.runJob(rdd, action, missingPartitionIndices)&#125;&#x2F;&#x2F;单独执行任务<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="12-4使用CheckPoint恢复计算"><a href="#12-4使用CheckPoint恢复计算" class="headerlink" title="12.4使用CheckPoint恢复计算"></a>12.4使用CheckPoint恢复计算</h3><p>checkpoint会将结果写到hdfs上，当driver 关闭后数据不会被清除。所以可以在其他driver上重复利用该checkpoint的数据。</p><p>checkpoint write data:</p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">sc.setCheckpointDir(&quot;data&#x2F;checkpoint&quot;)val rddt &#x3D; sc.parallelize(Array((1,2),(3,4),(5,6)),2)rddt.checkpoint()rddt.count() &#x2F;&#x2F;要action才能触发checkpoint<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>read from checkpoint data:</p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">package org.apache.sparkimport org.apache.spark.rdd.RDDobject RDDUtilsInSpark &#123;  def getCheckpointRDD[T](sc:SparkContext, path:String) &#x3D; &#123;  &#x2F;&#x2F;path要到part-000000的父目录    val result : RDD[Any] &#x3D; sc.checkpointFile(path)    result.asInstanceOf[T]  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><em>note:因为sc.checkpointFile(path)是private[spark]的，所以该类要写在自己工程里新建的package org.apache.spark中</em></p><p>example:</p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">val rdd : RDD[(Int, Int)]&#x3D; RDDUtilsInSpark.getCheckpointRDD(sc, &quot;data&#x2F;checkpoint&#x2F;963afe46-eb23-430f-8eae-8a6c5a1e41ba&#x2F;rdd-0&quot;)   println(rdd.count())   rdd.collect().foreach(println)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>这样就可以原样复原了。</p><p><strong>Demo</strong></p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">object SparkCore05_RDD_CheckPointUse &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    val sparkConf: SparkConf &#x3D; new SparkConf().setAppName(&quot;WordCountPersist&quot;).setMaster(&quot;local[1]&quot;)    val sc &#x3D; new SparkContext(sparkConf)&#x2F;&#x2F;使用工具类,注意工具类的包,要自己建立,注意泛型    val rdd: RDD[(String, Int)] &#x3D; RDDUtilsInSpark.getCheckpointRDD[RDD[(String, Int)]](sc, &quot;.&#x2F;checkPoint&#x2F;1186c961-ddb4-4af5-b7dc-6cc99776490b&#x2F;rdd-2&quot;)      &#x2F;&#x2F;之前的map之后reduceBykey之前的checkPoint文件    val result: RDD[(String, Int)] &#x3D; rdd.reduceByKey(_ + _)    result.collect().foreach(println)    sc.stop()  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">import org.apache.spark.rdd.RDD&#x2F;&#x2F;可以恢复checkPoint的工具类,注意放置的包object RDDUtilsInSpark &#123;  def getCheckpointRDD[T](sc: SparkContext, path: String) &#x3D; &#123;    &#x2F;&#x2F;path要到part-000000的父目录    val result: RDD[Any] &#x3D; sc.checkpointFile(path)    result.asInstanceOf[T]  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="13-RDD自定义分区器"><a href="#13-RDD自定义分区器" class="headerlink" title="13.RDD自定义分区器"></a>13.RDD自定义分区器</h2><p>Spark目前支持Hash分区、Range分区和用户自定义分区。Hash分区为当前的默认分区。分区器直接决定了RDD中分区的个数、RDD中每条数据经过Shuffle后进入哪个分区和Reduce的个数。</p><p>1）注意：</p><p>（1）只有Key-Value类型的RDD才有分区器，非Key-Value类型的RDD分区的值是None</p><p>（2）每个RDD的分区ID范围：0~numPartitions-1，决定这个值是属于那个分区的。</p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">package com.ecust.rdd.partitionimport org.apache.spark.rdd.RDDimport org.apache.spark.&#123;HashPartitioner, Partitioner, SparkConf, SparkContext&#125;&#x2F;** * @author Jinxin Li * @create 2020-12-26 10:52 * 自定义分区规则 *&#x2F;object SparkCore01_RDD_Partitioner &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    val sparkConf: SparkConf &#x3D; new SparkConf().setMaster(&quot;local&quot;).setAppName(&quot;Partitioner&quot;)    val sc &#x3D; new SparkContext(sparkConf)    val rdd: RDD[(String, String)] &#x3D; sc.makeRDD(List(      (&quot;nba&quot;, &quot;xxxxxxxxxxxxxxx&quot;),      (&quot;wba&quot;, &quot;aaaaaaaaaaaaaa&quot;),      (&quot;cba&quot;, &quot;dddddddddddd&quot;),      (&quot;wcba&quot;, &quot;ppppppppppppppppppppppp&quot;)    ), 3)    &#x2F;*    自动义分区器,决定数据去哪个分区     *&#x2F;    val rddPar: RDD[(String, String)] &#x3D; rdd.partitionBy(new MyPartitioner())    rddPar.saveAsTextFile(&quot;.&#x2F;par&quot;)    sc.stop()  &#125;  class MyPartitioner extends Partitioner&#123;    &#x2F;&#x2F;分区数量    override def numPartitions: Int &#x3D; 3    &#x2F;&#x2F;返回Int类型,返回数据的分区索引 从零开始    &#x2F;&#x2F;Key表示数据的KV到底是什么    &#x2F;&#x2F;根据数据的key值返回数据所在分区索引    override def getPartition(key: Any): Int &#x3D; &#123;      key match &#123;        case &quot;nba&quot; &#x3D;&gt; 0        case &quot;cba&quot; &#x3D;&gt; 1        case _ &#x3D;&gt; 2      &#125;      &#x2F;*if (key &#x3D;&#x3D; &quot;nba&quot;)&#123;        0      &#125; else if(key &#x3D;&#x3D; &quot;cba&quot;)&#123;        1      &#125;else&#123;        2      &#125;*&#x2F;    &#125;  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="14-RDD的存储与保存"><a href="#14-RDD的存储与保存" class="headerlink" title="14.RDD的存储与保存"></a>14.RDD的存储与保存</h2><p>Spark的数据读取及数据保存可以从两个维度来作区分：文件格式以及文件系统。</p><p>文件格式分为：Text文件、Json文件、Csv文件、Sequence文件以及Object文件；</p><p>文件系统分为：本地文件系统、HDFS以及数据库。</p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">&#x2F;&#x2F;集群文件系统存储示例hdfs:&#x2F;&#x2F;hadoop102:8020&#x2F;input&#x2F;1.txt<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h2 id="15-RDD的累加器"><a href="#15-RDD的累加器" class="headerlink" title="15.RDD的累加器"></a>15.RDD的累加器</h2><p>如果没有累加器,我们计算时只能使用reduce,要想把executor的变量拉回到Driver困难</p><p><img src="https://i.loli.net/2020/12/26/b24Xm8BYltu1PF3.png" alt="引入问题"></p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">object SparkCore02_RDD_accumulator &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    val sparkConf: SparkConf &#x3D; new SparkConf().setMaster(&quot;local&quot;).setAppName(&quot;Partitioner&quot;)    val sc &#x3D; new SparkContext(sparkConf)    val rdd: RDD[Int] &#x3D; sc.makeRDD(List(1, 2, 3, 4),1)    var sum:Int &#x3D; 0    &#x2F;&#x2F;行动算子返回非RDD    rdd.foreach(num&#x3D;&gt;&#123;      sum+&#x3D;num      println(&quot;executor:&quot;+sum)    &#125;)    println(&quot;driver:&quot;+sum)&#x2F;&#x2F;打印结果为零,Driver-&gt;executor,结果返回不了    sc.stop()  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="https://i.loli.net/2020/12/26/9PtGzVBQ8dD3sya.png" alt="累加器的主要目的"></p><p>累加器：分布式共享只写变量。（Executor和Executor之间不能读数据）</p><p>累加器用来把Executor端变量信息聚合到Driver端。在Driver程序中定义的变量，在Executor端的每个task都会得到这个变量的一份新的副本，每个task更新这些副本的值后，传回Driver端进行merge。</p><p>long累加器Demo</p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">object SparkCore03_RDD_accumulator &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    val sparkConf: SparkConf &#x3D; new SparkConf().setMaster(&quot;local&quot;).setAppName(&quot;Partitioner&quot;)    val sc &#x3D; new SparkContext(sparkConf)    val rdd: RDD[Int] &#x3D; sc.makeRDD(List(1, 2, 3, 4),1)    &#x2F;&#x2F;todo 自定义累加器    val sum: LongAccumulator &#x3D; sc.longAccumulator(&quot;sum&quot;)    &#x2F;&#x2F;系统自带了一些累加器&#x2F;&#x2F;    sc.doubleAccumulator&#x2F;&#x2F;    sc.collectionAccumulator()    rdd.foreach(num&#x3D;&gt;sum.add(num))    println(&quot;driver:&quot;+sum.value)    sc.stop()  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>特殊情况</strong></p><p>少加:转换算子中调用累加器,如果没有行动算子的话,name不会执行</p><p>多加:转换算子中调用累加器,多次行动算子会调用多次,一般会放在行动算子中进行操作</p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">object SparkCore04_RDD_accumulator &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    val sparkConf: SparkConf &#x3D; new SparkConf().setMaster(&quot;local&quot;).setAppName(&quot;Partitioner&quot;)    val sc &#x3D; new SparkContext(sparkConf)    val rdd: RDD[Int] &#x3D; sc.makeRDD(List(1, 2, 3, 4),1)    &#x2F;&#x2F;todo 自定义累加器    val sum: LongAccumulator &#x3D; sc.longAccumulator(&quot;sum&quot;)    &#x2F;&#x2F;系统自带了一些累加器&#x2F;&#x2F;    sc.doubleAccumulator&#x2F;&#x2F;    sc.collectionAccumulator()    val result: RDD[Unit] &#x3D; rdd.map(num &#x3D;&gt; sum.add(num))        result.collect()    result.collect()&#x2F;&#x2F;两个行动算子会多加    println(&quot;driver:&quot;+sum.value)    sc.stop()  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="16-RDD的自定义累加器"><a href="#16-RDD的自定义累加器" class="headerlink" title="16.RDD的自定义累加器"></a>16.RDD的自定义累加器</h2><p>分布式共享只写变量</p><p>表示累加器的值互相之间是没法访问的,自己能读自己,只有Driver进行读到,然后在Driver端进行合并</p><p>我们可以将一些Shuffle的东西使用累加器来实现(==优化==)</p><p>比方:需要shuffle的方法就不要shuffle了</p><p>闭包数据,都是以Task为单位发送的,每个人物中包含的闭包数据这样可能会导致,一个Executor中含有大量的重复的数据,并且占用大量的内存</p><p>Executor本质其实就是JVM,所以在启动时,会自动分配内存</p><p> 完全可以将任务中的闭包数据放置到Executor的内存中,达到共享的目的</p><p>Spark中的广播变量可以将闭包的数据保存在Executor的内存中</p><p>分布式共享只读变量</p><pre class="mermaid">graph TDmap-->Executor/task1map-->Executor/task2map-->Executor/task3</pre><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">object SparkCore04_RDD_BroadCast &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    val sparkConf: SparkConf &#x3D; new SparkConf().setMaster(&quot;local&quot;).setAppName(&quot;acc&quot;)    val sc &#x3D; new SparkContext(sparkConf)    val rdd1: RDD[(String, Int)] &#x3D; sc.makeRDD(List((&quot;a&quot;, 1), (&quot;b&quot;, 2), (&quot;c&quot;, 3)))    val map: mutable.Map[String, Int] &#x3D; mutable.Map((&quot;a&quot;, 4), (&quot;b&quot;, 5), (&quot;c&quot;, 6))    &#x2F;&#x2F;定义广播变量    val value: Broadcast[mutable.Map[String, Int]] &#x3D; sc.broadcast(map)    &#x2F;&#x2F;每个task都有一份数据    val result: RDD[(String, (Int, Int))] &#x3D; rdd1.map &#123; case (w, c) &#x3D;&gt; &#123;      val i: Int &#x3D; value.value.getOrElse(w, 0)      (w, (c, i))    &#125;&#125;    result.collect().foreach(println)    sc.stop()  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      
        <tags>
            
            <tag> spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="2020/09/01/Hadoop%E7%AC%94%E8%AE%B0%E6%80%BB%E7%BB%93-01/"/>
      <url>2020/09/01/Hadoop%E7%AC%94%E8%AE%B0%E6%80%BB%E7%BB%93-01/</url>
      
        <content type="html"><![CDATA[<h1 id="Hadoop入门-笔记总结"><a href="#Hadoop入门-笔记总结" class="headerlink" title="Hadoop入门-笔记总结"></a>Hadoop入门-笔记总结</h1><h2 id="学习目标："><a href="#学习目标：" class="headerlink" title="学习目标："></a>学习目标：</h2><ul><li> 了解大数据的概念以及应用场景和发展前景（这部分还是会讲故事即可）</li><li> 初步掌握大数据部门业务分析流程以及完整的大数据部门的组织架构（还是了解讲故事…）</li><li> 通俗易懂的说明白Hadoop的概念以及发展历史</li><li> 掌握Hadoop的前后的版本迭代更新以及Hadoop的优势</li><li> <strong>重点理解Hadoop框架的三大组成部分，并准确的表述各自的作用</strong></li><li> 掌握大数据生态的概念</li><li> <strong>熟练操作Hadoop运行环境的搭建（重点掌握）</strong></li><li> <strong>熟练掌握Hadoop的运行模式（重点掌握）</strong></li><li> 掌握Hadoop2.x和Hadoop3.x版本的差异</li><li> 能够对Hadoop的源码进行编译</li><li> <strong>掌握常见的错误和问题（重点）</strong></li></ul><h2 id="一、大数据概论"><a href="#一、大数据概论" class="headerlink" title="一、大数据概论"></a>一、大数据概论</h2><p><strong>前言：</strong>这部分主要讲解的就是大数据的概念，以及大数据的应用领域和发展前景，要求大家能够用自己的话去描述，讲给别人听即可！</p><h3 id="1-大数据的发展史"><a href="#1-大数据的发展史" class="headerlink" title="1.大数据的发展史"></a>1.大数据的发展史</h3><figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">In pioneer days they used oxen for heavy pulling, and when one ox couldn’t budge a log,they didn’t try to grow a larger ox. We shouldn’t be trying for bigger computers, but formore systems of computers.</span><br><span class="line">—Grace Hopper</span><br></pre></td></tr></table></figure><h3 id="2-大数据的概念"><a href="#2-大数据的概念" class="headerlink" title="2.大数据的概念"></a>2.大数据的概念</h3><p>​    大数据（big data），IT行业术语，是指无法在一定时间范围内用常规软件工具进行捕捉、管理和处理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的海量、高增长率和多样化的信息资产。</p><p>​    简单的讲 <strong>大数据</strong> 就是海量数据，我们想要利用这海量数据，必然要对它进行<strong>存储</strong> ，然后又想让其实现价值，必须得通过 <strong>分析计算</strong> 得到结果，而分析计算也不能没有时间限制，那就得在合理的时间内分析计算。最后一句话就是 <strong>大数据技术就是来完成海量数据的存储以及对海量数据在合理时间内进行分析运算的</strong></p><p>​    最小的基本单位是bit，按顺序给出所有单位：bit、Byte、KB、MB、GB、TB、PB、EB、ZB、YB、BB、NB、DB，它们按照进率1024（2的十次方）来计算：</p><p>8bit= 1Byte</p><p>1KB= 1,024 Bytes</p><p>1MB= 1,024 KB = 1,048,576 Bytes</p><p>1GB= 1,024 MB = 1,048,576 KB</p><p><strong>1TB= 1,024 GB = 1,048,576 MB</strong></p><p><strong>1PB= 1,024 TB = 1,048,576 GB</strong></p><p><strong>1EB= 1,024 PB = 1,048,576 TB</strong></p><p>1ZB= 1,024 EB = 1,048,576 PB</p><p>1YB= 1,024 ZB = 1,048,576 EB</p><p>1BB= 1,024 YB = 1,048,576 ZB</p><p>1NB= 1,024 BB = 1,048,576 YB</p><p>1 DB = 1,024 NB = 1,048,576 BB</p><h3 id="3-大数据的特点"><a href="#3-大数据的特点" class="headerlink" title="3.大数据的特点"></a>3.大数据的特点</h3><p>​    <strong>3.1大量（Volume）</strong></p><p>​    想要贴近大数据的概念，必然要求海量数据，用量化的单位来描述的话至少也得PB级别的起步。</p><p>​    <strong>3.2高速（Velocity）</strong></p><p>​    所谓的高速是指海量数据产生的速度是非常快的，例如 <strong>天猫双十一</strong> 大约1分钟左右成交100亿的，100亿背后所涉及的数据可想而知。同时数据产生速度的也要求我们对数据的处理的效率要跟上节奏才可以。</p><p>​    <strong>3.3多样（Variety）</strong></p><p>​    多样是指数据的体现形式是多样化的，大体分为三种形式  <strong>结构化数据</strong>  <strong>半结构化数据</strong>  <strong>非结构化化数据</strong>，这些所说的基本上都是原始数据，我们将来要想地数据更高效的运算都会对原始数据进行清洗。</p><p>​    <strong>3.4低价值密度（Value）</strong></p><p>​    在通常情况下，面对海量数据，往往我们需要的可能只是其中的一小部分，这就是说 <strong>价值密度的高低和数据总量是成反比的</strong> 这也是大数据比较显著的一个特点，所以 高效快速的对有价值的数据进行<strong>“提纯”</strong> 成为目前大数据领域一个攻坚破阻的难题。</p><h3 id="4-大数据的应用场景"><a href="#4-大数据的应用场景" class="headerlink" title="4.大数据的应用场景"></a>4.大数据的应用场景</h3><p>​    本章节主要了解大数据的真实应用场景和领域。这部分大家作为了解即可，推荐下面一片文章作为参考！</p><p>​    <a href="https://www.jianshu.com/p/bb989c2fbc76">https://www.jianshu.com/p/bb989c2fbc76</a></p><h3 id="5-大数据的发展前景"><a href="#5-大数据的发展前景" class="headerlink" title="5.大数据的发展前景"></a>5.大数据的发展前景</h3><p>​    大数据行业的前景毋庸置疑是非常好的，从国家政策的推动再到行业的人才缺口以及未来的发展趋势都让大数据成为一个很有前途的专业。但是还是要求大家稳扎稳打 技术到家 才能翻江倒海！</p><h3 id="6-大数据部门业务流程分析"><a href="#6-大数据部门业务流程分析" class="headerlink" title="6.大数据部门业务流程分析"></a>6.大数据部门业务流程分析</h3><p>​    本小节主要介绍在工作当中我们将来完成一个项目的业务流程，我们大数据的工作在哪一环节崭露头角！我们大数据主要任务就是根据具体的需求对数据进行存储和分析运算，最后获取想要的数据结果。</p><h3 id="7-大数据部门组织结构（重点）"><a href="#7-大数据部门组织结构（重点）" class="headerlink" title="7.大数据部门组织结构（重点）"></a>7.大数据部门组织结构（重点）</h3><p>​    这一小节主要阐述一个公司通常大数据部门的智能分布，可以参考下图：</p><p><img src="Hadoop%E7%AC%94%E8%AE%B0%E6%80%BB%E7%BB%93-01.assets/image-20200815225821434.png" alt="image-20200815225821434"></p><h2 id="二、从Hadoop框架讨论大数据生态"><a href="#二、从Hadoop框架讨论大数据生态" class="headerlink" title="二、从Hadoop框架讨论大数据生态"></a>二、从Hadoop框架讨论大数据生态</h2><h3 id="1-Hadoop的概念"><a href="#1-Hadoop的概念" class="headerlink" title="1. Hadoop的概念"></a>1. Hadoop的概念</h3><p>​    <strong>理解Hadoop是什么要从两个层面去入手：</strong></p><p>​    <strong>1.1 狭义：</strong>Hadoop是Apache旗下的一个用java语言实现开源软件框架，是一个开发和运行处理大规模数据的软件平台。允许使用简单的编程模型在大量计算机集群上对大型数据集进行分布式处理。它的核心组件有：</p><p>HDFS（分布式文件系统）：解决海量数据存储</p><p>YARN（作业调度和集群资源管理的框架）：解决资源任务调度</p><p>MAPREDUCE（分布式运算编程框架）：解决海量数据计算</p><p>​    <img src="Hadoop笔记总结-01.assets/image-20200815231415210.png" alt="image-20200815231415210" style="zoom:50%;" /></p><p>​    <strong>1.2 广义：</strong>广义上来说，Hadoop通常是指一个更广泛的概念——Hadoop生态圈。 </p><p>​    <img src="Hadoop%E7%AC%94%E8%AE%B0%E6%80%BB%E7%BB%93-01.assets/image-20200815231602113.png" alt="image-20200815231602113"></p><p>当下的Hadoop已经成长为一个庞大的体系，随着生态系统的成长，新出现的项目越来越多，其中不乏一些非Apache主管的项目，这些项目对HADOOP是很好的补充或者更高层的抽象。比如：</p><p>HDFS：分布式文件系统</p><p>MAPREDUCE：分布式运算程序开发框架</p><p>HIVE：基于HADOOP的分布式数据仓库，提供基于SQL的查询数据操作</p><p>HBASE：基于HADOOP的分布式海量数据库</p><p>ZOOKEEPER：分布式协调服务基础组件</p><p>Mahout：基于mapreduce/spark/flink等分布式运算框架的机器学习算法库</p><p>OOZIE：工作流调度框架</p><p>Sqoop：数据导入导出工具（比如用于mysql和HDFS之间）</p><p>FLUME：日志数据采集框架</p><p>IMPALA：基于hive的实时sql查询分析</p><h3 id="2-Hadoop的发展史"><a href="#2-Hadoop的发展史" class="headerlink" title="2. Hadoop的发展史"></a>2. Hadoop的发展史</h3><p><img src="Hadoop%E7%AC%94%E8%AE%B0%E6%80%BB%E7%BB%93-01.assets/image-20200814220122350.png" alt="image-20200814220122350"></p><ol><li><p>2002年10月，Doug Cutting和Mike Cafarella创建了开源网页爬虫项目Nutch。</p></li><li><p>2003年10月，Google发表Google File System论文。</p></li><li><p>2004年7月，Doug Cutting和Mike Cafarella在Nutch中实现了类似GFS的功能，即后来HDFS的前身。</p></li><li><p>2004年10月，Google发表了MapReduce论文。</p></li><li><p>2005年2月，Mike Cafarella在Nutch中实现了MapReduce的最初版本。</p></li><li><p>2005年12月，开源搜索项目Nutch移植到新框架，使用MapReduce和NDFS在20个节点稳定运行。</p></li><li><p>2006年1月，Doug Cutting加入雅虎，Yahoo!提供一个专门的团队和资源将Hadoop发展成一个可在网络上运行的系统。</p></li><li><p>2006年2月，Apache Hadoop项目正式启动以支持MapReduce和HDFS的独立发展。</p></li><li><p>2006年3月，Yahoo!建设了第一个Hadoop集群用于开发。</p></li></ol><p>10.2006年4月，第一个Apache Hadoop发布。</p><p>11.2006年11月，Google发表了Bigtable论文，激起了Hbase的创建。</p><p>12.2007年10月，第一个Hadoop用户组会议召开，社区贡献开始急剧上升。</p><p>13.2007年，百度开始使用Hadoop做离线处理。</p><p>14.2007年，中国移动开始在“大云”研究中使用Hadoop技术。</p><p>15.2008年，淘宝开始投入研究基于Hadoop的系统——云梯，并将其用于处理电子商务相关数据。</p><p>16.2008年1月，Hadoop成为Apache顶级项目。</p><p>17.2008年2月，Yahoo!运行了世界上最大的Hadoop应用，宣布其搜索引擎产品部署在一个拥有1万个内核的Hadoop集群上。</p><p>18.2008年4月，在900个节点上运行1TB排序测试集仅需209秒，成为世界最快。</p><p>19.2008年8月，第一个Hadoop商业化公司Cloudera成立。</p><p>20.2008年10月，研究集群每天装载10TB的数据。</p><p>21.2009 年3月，Cloudera推出世界上首个Hadoop发行版——CDH（Cloudera’s Distribution including Apache Hadoop）平台，完全由开放源码软件组成。</p><p>22.2009年6月，Cloudera的工程师Tom White编写的《Hadoop权威指南》初版出版，后被誉为Hadoop圣经。</p><p>23.2009年7月 ，Hadoop Core项目更名为Hadoop Common;</p><p>24.2009年7月 ，MapReduce 和 Hadoop Distributed File System (HDFS) 成为Hadoop项目的独立子项目。</p><p>25.2009年8月，Hadoop创始人Doug Cutting加入Cloudera担任首席架构师。</p><p>26.2009年10月，首届Hadoop World大会在纽约召开。</p><p>27.2010年5月，IBM提供了基于Hadoop 的大数据分析软件——InfoSphere BigInsights，包括基础版和企业版。</p><p>28.2011年3月，Apache Hadoop获得Media Guardian Innovation Awards媒体卫报创新奖</p><p>29.2012年3月，企业必须的重要功能HDFS NameNode HA被加入Hadoop主版本。</p><p>30.2012年8月，另外一个重要的企业适用功能YARN成为Hadoop子项目。</p><p>31.2014年2月，Spark逐渐代替MapReduce成为Hadoop的缺省执行引擎，并成为Apache基金会顶级项目。</p><p>2017年12月，Release 3.0.0 generally available</p><h3 id="3-Hadoop三大发行版本"><a href="#3-Hadoop三大发行版本" class="headerlink" title="3. Hadoop三大发行版本"></a>3. Hadoop三大发行版本</h3><p><strong>3.1 Apache</strong></p><p>企业实际使用并不多。最原始（基础）版本。这是学习hadoop的基础。</p><p><strong>3.2 cloudera</strong></p><p>对hadoop的升级，打包，开发了很多框架。flume、hue、impala都是这个公司开发</p><p>2008 年成立的 Cloudera 是最早将 Hadoop 商用的公司，为合作伙伴提 供 Hadoop 的商用解决方案，主要是包括支持，咨询服务，培训。</p><p>2009年Hadoop的创始人 Doug Cutting也加盟 Cloudera公司。Cloudera 产品主要 为 CDH，Cloudera Manager，Cloudera Support</p><p>CDH是Cloudera的Hadoop发行版，完全开源，比Apache Hadoop在兼容性，安全 性，稳定性上有所增强。</p><p>Cloudera Manager是集群的软件分发及管理监控平台，可以在几个小时内部署 好一个Hadoop集群，并对集群的节点及服务进行实时监控。Cloudera Support即 是对Hadoop的技术支持。</p><p>Cloudera 的标价为每年每个节点4000美元。Cloudera开发并贡献了可实时处理大 数据的Impala项目。</p><p><strong>3.3 Hortonworks</strong></p><p>2011年成立的Hortonworks是雅虎与硅谷风投公司Benchmark Capital合资组建</p><p>公司成立之初就吸纳了大约25名至30名专门研究Hadoop的雅虎工程师，上述工 程师均在2005年开始协助雅虎开发Hadoop，贡献了Hadoop 80%的代码。</p><p>雅虎工程副总裁、雅虎Hadoop开发团队负责人Eric Baldeschwieler出任<br> Hortonworks的首席执行官。</p><p>Hortonworks 的主打产品是Hortonworks Data Platform (HDP)，也同样是100%开 源的产品，HDP除常见的项目外还包含了Ambari，一款开源的安装和管理系统</p><p>HCatalog，一个元数据管理系统，HCatalog现已集成到Facebook 开源的Hive中<br> 。Hortonworks的Stinger开创性地极大地优化了Hive项目。Hortonworks为入门提 供了一个非常好的，易于使用的沙盒。</p><p>Hortonworks开发了很多增强特性并提交至核心主干，这使得Apache Hadoop能 够在包括Windows Server和Windows Azure在内的Microsoft Windows平台上本地 运行。定价以集群为基础，每10个节点每年为12500美元。</p><h3 id="4-Hadoop的优势"><a href="#4-Hadoop的优势" class="headerlink" title="4. Hadoop的优势"></a>4. Hadoop的优势</h3><h5 id="4-1-高可靠性"><a href="#4-1-高可靠性" class="headerlink" title="4.1 高可靠性"></a>4.1 高可靠性</h5><p> Hadoop底层维护多个数据副本，所以即使Hadoop某个计算元素或存储出现故障，也不会导致数据的丢失。</p><h5 id="4-2-高可扩展性"><a href="#4-2-高可扩展性" class="headerlink" title="4.2 高可扩展性"></a>4.2 高可扩展性</h5><p> 在集群间分配任务数据，可方便的扩展数以千计的节点。</p><h5 id="4-3-高效性"><a href="#4-3-高效性" class="headerlink" title="4.3 高效性"></a>4.3 高效性</h5><p> 在MapReduce的思想下，Hadoop是并行工作的，以加快任务处理速度。</p><h5 id="4-4-高容错性"><a href="#4-4-高容错性" class="headerlink" title="4.4 高容错性"></a>4.4 高容错性</h5><p> 能够自动将失败的任务重新分配。</p><h3 id="5-Hadoop框架组成"><a href="#5-Hadoop框架组成" class="headerlink" title="5. Hadoop框架组成"></a>5. Hadoop框架组成</h3><p>Hadoop是一个能够对大量数据进行分布式处理的软件框架，以一种可靠、高效、可伸缩的方式进行数据处理，其有许多元素构成，以下是其组成元素：</p><p><img src="Hadoop%E7%AC%94%E8%AE%B0%E6%80%BB%E7%BB%93-01.assets/image-20200816214808801.png" alt="image-20200816214808801"></p><p><strong>注意：</strong> 通过上图我们要掌握的重点是Hadoop是由核心的三大组件构成的，在hadoop1.x的版本中 只有两大组件分别是 <strong>HDFS(负责文件的存储)**和</strong>MapReduce(负责文件的计算和资源调度)** 。后来在hadoop2.x的时候出于架构的解耦考虑以及让 资源调度 工作能更加灵活多样化就把 原来MapReduce中的负责资源调度的功能剥离出来 单独形成 Yarn 这个核心组件。</p><h4 id="5-1HDFS理论概述"><a href="#5-1HDFS理论概述" class="headerlink" title="5.1HDFS理论概述"></a>5.1HDFS理论概述</h4><p><strong>HDFS:</strong> Hadoop Distributed File System(hadoop分布式文件系统)</p><p><strong>注意：</strong> 本小节主要是从理论的角度先去理解HDFS的概念，HDFS中还包含很多概念我们逐个来分析理解。</p><p><strong>1.HDFS的特点：</strong> </p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1. 保存多个副本，且提供容错机制，副本丢失或宕机自动恢复。默认存3份。</span><br><span class="line"></span><br><span class="line">2. 运行在廉价的机器上。</span><br><span class="line"></span><br><span class="line">3. 适合大数据的处理。HDFS默认会将文件分割成block，64M为1个block。</span><br><span class="line">   然后将block按键值对存储在HDFS上，并将键值对的映射存到内存中。如果小文件太多，那内存的负担会很  重。</span><br></pre></td></tr></table></figure><p><strong>2.在HDFS中有三个重要的角色相互协调工作，分别是NameNode  SecondaryNameNode   DataNode</strong> </p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1.NameNode   Master节点，大领导。</span><br><span class="line">  -- 管理数据块映射；</span><br><span class="line">  -- 处理客户端的读写请求；</span><br><span class="line">  -- 配置副本策略；</span><br><span class="line">  -- 管理HDFS的名称空间。 </span><br><span class="line">  -- namenode 内存中存储的是 &#x3D; fsimage + edits。</span><br><span class="line">     其中fsimage元数据镜像文件（文件系统的目录树），edits元数据的操作日志（针对文件系统做的修改操  作记录）</span><br><span class="line">  总之：NameNode很重要，在海量数据的存储和管理，NameNode就相当于是所有数据的描述或者指针，有了它才能进一步操作真实数据。</span><br><span class="line">  </span><br><span class="line">2.SecondaryNameNode  它是个小弟，分担大哥NameNode的工作量。</span><br><span class="line">  -- SecondaryNameNode负责定时默认1小时，从namenode上，获取fsimage和edits来进行合并，然后再      发送给namenode。减少namenode的工作量。</span><br><span class="line">  -- NameNode的冷备份。</span><br><span class="line">  </span><br><span class="line">3.DataNode  真实数据的存储位置</span><br><span class="line">  -- 存储client发来的数据块block；</span><br><span class="line">  -- 执行数据块的读写操作。</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="5-2-YARN架构概述"><a href="#5-2-YARN架构概述" class="headerlink" title="5.2 YARN架构概述"></a>5.2 YARN架构概述</h4><p>​        本小节主要了解YARN架构中重要的几个 组件。本次接触YARN不要求掌握其本质原理，只要求混个脸熟，大概了解YARN的作用以及组成部分，为后面的学习建立基础。</p><p>​    <strong>1.为什么要用YARN？</strong></p><p>​        首先我们要知道的是在Hadoop1.x时代 是没有YARN的，那时候所有的数据计算以及计算过程的任务分配和资源调度都是在MapReduce中进行的，这样存在很多问题和隐患，典型的就是JobTracker容易存在单点故障和JobTracker负担重，既要负责资源管理，又要进行作业调度；当需处理太多任务时，会造成过多的资源消耗。所以在Hadoop2.x的时候，推出了YARN这套系统，其主要目的就是将Hadoop中的资源调度功能独立的分离出来，这样更方便扩展，也能高效合理的调度资源。</p><p>​    <strong>2.YARN中的几大角色</strong></p><p>​        <img src="Hadoop%E7%AC%94%E8%AE%B0%E6%80%BB%E7%BB%93-01.assets/image-20200818184449616.png" alt="image-20200818184449616"></p><p>​        <strong>– ResourceManager</strong></p><p>​            YARN 分层结构的本质是 ResourceManager。这个实体控制整个集群并管理应用程序向基础计算资源的分配。ResourceManager 将各个资源部分（计算、内存、带宽等）精心安排给基础 NodeManager（YARN 的每节点代理）。ResourceManager 还与 ApplicationMaster 一起分配资源，与 NodeManager 一起启动和监视它们的基础应用程序。在此上下文中，ApplicationMaster 承担了以前的 TaskTracker 的一些角色，ResourceManager 承担了 JobTracker 的角色。</p><p>​            <strong>总的来说，RM有以下作用：</strong></p><pre><code>        1）处理客户端请求        2）启动或监控ApplicationMaster</code></pre><p>​            3）监控NodeManager</p><pre><code>        4）资源的分配与调度</code></pre><p>​        <strong>– NodeManager</strong></p><p>​                ApplicationMaster 管理在YARN内运行的每个应用程序实例。ApplicationMaster 负责协调来自 ResourceManager 的资源，并通过 NodeManager 监视容器的执行和资源使用（CPU、内存等的资源分配）。请注意，尽管目前的资源更加传统（CPU 核心、内存），但未来会带来基于手头任务的新资源类型（比如图形处理单元或专用处理设备）。从 YARN 角度讲，ApplicationMaster 是用户代码，因此存在潜在的安全问题。YARN 假设 ApplicationMaster 存在错误或者甚至是恶意的，因此将它们当作无特权的代码对待。</p><p>​                <strong>总的来说,AM有以下作用：</strong></p><pre><code>            1）负责数据的切分</code></pre><p>​                2）为应用程序申请资源并分配给内部的任务</p><p>​                3）任务的监控与容错</p><p>​        <strong>– ApplicationMaster</strong></p><p>​                NodeManager管理YARN集群中的每个节点。NodeManager 提供针对集群中每个节点的服务，从监督对一个容器的终生管理到监视资源和跟踪节点健康。MRv1 通过插槽管理 Map 和 Reduce 任务的执行，而 NodeManager 管理抽象容器，这些容器代表着可供一个特定应用程序使用的针对每个节点的资源。</p><p>​                <strong>总的来说，NM有以下作用：</strong></p><pre><code>            1）管理单个节点上的资源            2）处理来自ResourceManager的命令            3）处理来自ApplicationMaster的命令</code></pre><p>​        <strong>– Container</strong></p><p>​            Container 是 YARN 中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等，当AM向RM申请资源时，RM为AM返回的资源便是用Container表示的。YARN会为每个任务分配一个Container，且该任务只能使用该Container中描述的资源。</p><p>​            <strong>总的来说，Container有以下作用：</strong></p><pre><code>       1）对任务运行环境进行抽象，封装CPU、内存等多维度的资源以及环境变量、启动命令等任务运行相关的信息</code></pre><p><strong>总结：要使用一个 YARN 集群，首先需要一个包含应用程序的客户的请求。ResourceManager 协商一个容器的必要资源，启动一个 ApplicationMaster 来表示已提交的应用程序。通过使用一个资源请求协议，ApplicationMaster 协商每个节点上供应用程序使用的资源容器。执行应用程序时，ApplicationMaster 监视容器直到完成。当应用程序完成时，ApplicationMaster 从 ResourceManager 注销其容器，执行周期就完成了。</strong></p><h4 id="5-3-MapReduce架构概述"><a href="#5-3-MapReduce架构概述" class="headerlink" title="5.3 MapReduce架构概述"></a>5.3 MapReduce架构概述</h4><h3 id="6-大数据技术生态体系"><a href="#6-大数据技术生态体系" class="headerlink" title="6. 大数据技术生态体系"></a>6. 大数据技术生态体系</h3><p><img src="Hadoop%E7%AC%94%E8%AE%B0%E6%80%BB%E7%BB%93-01.assets/image-20200726100836955.png" alt="image-20200726100836955"></p><p><strong>小结：</strong>大概描述就是 首先 大数据的根本就是分析计算数据，那一定要定位数据来源，数据来源大体包含三个方面，分别是 正规的数据库（结构化数据），其次还有文件日志（半结构化数据）以及通过一些爬虫手段获取的互联网数据（非结构化数据）这就组成了我们的**<em>数据来源层**</em>。 </p><p>​        有了数据来源接下来就需要将这些数据传输到我们的分布式文件存储系统（HDFS）或者直接通过消息队列（kafka）将数据传输到数据计算层来做数据分析和运算，这里我们把专门做数据传输的技术层称之为*<strong>数据传输层***，同时保存到HDFS中后，我们成这块内容为 *</strong>数据存储层***。</p><p>​        有了具体的数据那后续就可以做数据分析运算了，这时候就要有 <strong><em>数据计算层</em></strong> 来完成，这部分大概根据数据结果的实效性可以分为两类数据分析运算的场景，一种是离线运算，一种实时运算，离线的话我们通常采用MapReduce和Hive来完成。实时的话就会用到Spark体系架构完成或者用Fink框架。</p><p>​        结合上面提到的概念，我们还要加入 <strong><em>资源管理层</em></strong>   主要有 YARN 来完成，它的主要工作就是来分配调度计算资源的，用来协作 MapReduce 作业。同时在实行数据运算的时候 我们考虑到服务器的资源分配以及人物先后执行的顺序，有加入了一个 <strong><em>任务调度层</em></strong>  专门来控制运算作业的执行时间和先后顺序</p><p>​        以上就是大数据架构体系的协作规则和架构说明，但是我们最后又考虑到 分布式集群的操作，各个版块和服务一定会交叉协同工作，所以最后利用Zookeeper来统一管理 分布式集群架构。OK，以上就是关于大数据技术生态体系的话术表现。</p><p>​        </p><h3 id="7-推荐系统框架图"><a href="#7-推荐系统框架图" class="headerlink" title="7. 推荐系统框架图"></a>7. 推荐系统框架图</h3><img src="Hadoop笔记总结-01.assets/image-20200729111555275.png" alt="image-20200729111555275" style="zoom: 50%;" /><p><strong>小结：</strong>以上的一个推荐系统的大概描述，首先一定从用户的行为开始入手，当用户购买一件商品加入购物车后，我们往往会给用户推荐相关的类似产品或者连带产品，这是目前电商系统很常见的一种营销手段。这个推荐的数据是如何产生的呢？</p><p>1.用户将商品加入购物车，这是会产生购物车数据，这就是我们的数据来源</p><p>2.利用数据传输层的相关技术将数据进行搜集处理然后通过Kafak消息队列直接将数据传输到 实时运算的框架中进行分析运算。</p><p>3.当 数据计算层 把数据分析运算后会得到最终的结果，根据结果为依据找到相关的类似商品的数据进行整合。</p><p>4.最后回到电商系统中 的推荐模块 通过调用接口的方式获取最终的分析处理后整合的商品数据的结果，将其展示到客户端页面中。</p><p>上面大概就是一个推荐的流程，你学到了吗！！！</p><h2 id="三、Hadoop运行环境搭建（重点）"><a href="#三、Hadoop运行环境搭建（重点）" class="headerlink" title="三、Hadoop运行环境搭建（重点）"></a>三、Hadoop运行环境搭建（重点）</h2><h3 id="1-虚拟机环境准备"><a href="#1-虚拟机环境准备" class="headerlink" title="1. 虚拟机环境准备"></a>1. 虚拟机环境准备</h3><ul><li><p><strong>1). 准备模板机</strong>（安装最小化的Linux系统）</p><ul><li><p>yum安装必要的插件</p> <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo yum install -y epel-release</span><br><span class="line"></span><br><span class="line">sudo yum install -y psmisc nc net-tools rsync vim lrzsz ntp libzstd openssl-static tree iotop git</span><br></pre></td></tr></table></figure></li><li><p>修改 /etc/hosts 文件</p> <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">192.168.2.100 hadoop100</span><br><span class="line">192.168.2.101 hadoop101</span><br><span class="line">192.168.2.102 hadoop102</span><br><span class="line">192.168.2.103 hadoop103</span><br><span class="line">192.168.2.104 hadoop104</span><br><span class="line">192.168.2.105 hadoop105</span><br><span class="line">192.168.2.106 hadoop106</span><br><span class="line">192.168.2.107 hadoop107</span><br><span class="line">192.168.2.108 hadoop108</span><br></pre></td></tr></table></figure></li><li><p>设置Linux的防火墙开机不自启</p> <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl disable firewalld</span><br></pre></td></tr></table></figure></li><li><p>创建 atguigu 用户</p> <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">useradd atguigu</span><br></pre></td></tr></table></figure></li><li><p>修改/etc/sudoers文件 配置atguigu用户具有root权限</p> <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">在第92行的位置加上以下内容</span><br><span class="line">atguigu ALL&#x3D;(ALL)  NOPASSWD:ALL</span><br><span class="line"></span><br><span class="line">:wq! 强制保存退出。</span><br></pre></td></tr></table></figure></li><li><p>在/opt目录下创建两个文件夹 </p> <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir &#x2F;opt&#x2F;software   --放置需要安装的软件的安装包</span><br><span class="line">madir &#x2F;opt&#x2F;module     --软件的安装目录</span><br></pre></td></tr></table></figure></li><li><p>配置 两个文件夹 属于 atguigu 用户和 atguigu 组</p> <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">chown atguigu:atguigu &#x2F;opt&#x2F;software</span><br><span class="line"></span><br><span class="line">chown atguigu:atguigu &#x2F;opt&#x2F;module</span><br></pre></td></tr></table></figure></li></ul></li><li><p><strong>2). 准备开发用的虚拟机</strong></p><ul><li><p>根据模板机克隆一台机器</p><ul><li> 根据克隆的步骤进行克隆就可以(参考Linux阶段的克隆操作)</li><li> 启动虚拟机</li></ul></li><li><p>修改克隆机的主机名</p> <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1.编辑hostname文件</span><br><span class="line">vim &#x2F;etc&#x2F;hostname</span><br><span class="line"></span><br><span class="line">2.修改主机名称</span><br><span class="line">hadoop101</span><br><span class="line"></span><br><span class="line">3.重启机器 </span><br><span class="line">reboot</span><br></pre></td></tr></table></figure></li><li><p>修改克隆机的ip</p> <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1.编辑ifcfg-ens33文件</span><br><span class="line">vim &#x2F;etc&#x2F;sysconfig&#x2F;network-spcripts&#x2F;ifcfg-ens33</span><br><span class="line"></span><br><span class="line">2.重点修改的一下标注的地方</span><br></pre></td></tr></table></figure><p> <img src="Hadoop%E7%AC%94%E8%AE%B0%E6%80%BB%E7%BB%93-01.assets/image-20200729142023446.png" alt="image-20200729142023446"></p></li><li><p>利用FinallShell工具连接Linux</p><p> <img src="Hadoop%E7%AC%94%E8%AE%B0%E6%80%BB%E7%BB%93-01.assets/image-20200729142145665.png" alt="image-20200729142145665"></p></li></ul></li></ul><h3 id="2-在准备好开发机hadoop101安装JDK"><a href="#2-在准备好开发机hadoop101安装JDK" class="headerlink" title="2. 在准备好开发机hadoop101安装JDK"></a>2. 在准备好开发机hadoop101安装JDK</h3><p>​     <strong>概述：</strong>本小节主要讲解在Linux中如何安装jdk，首先要明白Hadoop是用Java开发的，换言之Hadoop就是一款Java写的软件，那么想要运行Hadoop必然需要jdk环境。在Linux中安装Jdk和Windows中安装原理相同，只不过在Linux中Jdk的体现形式是一个 tar.gz的压缩包而Windows中是一个可视化安装程序。</p><ul><li><p><strong>1). 卸载现有JDK</strong></p><p> ​    <strong>注意：如果首次安装就没必要进行这一步，如果想更换jdk,非首次安装则需要先把已有的卸载掉</strong></p> <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rpm -qa | grep -i java | xargs -n1 sudo rpm -e --nodeps</span><br></pre></td></tr></table></figure></li><li><p><strong>2). 将jdk的tar包导入到Linux中opt目录下的software下</strong></p> <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">在我们的FinallShell工具中，直接找到opt目录下的software文件夹，将Windows目录下的jdk-8u212-linux-x64.tar.gz 包拖拽到software文件夹里即可</span><br></pre></td></tr></table></figure></li><li><p><strong>3).解压jdk压缩包到opt目录下的module文件夹中</strong></p> <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tar -zxvf jdk-8u212-linux-x64.tar.gz -C &#x2F;opt&#x2F;module&#x2F;</span><br></pre></td></tr></table></figure></li><li><p><strong>4). 配置jdk的环境变量</strong></p><p> <strong>概述：</strong>接下来我们就要配置jdk的环境变量，思路和在windows系统下配置环境变量类似。这里注意一下，在Linux中 我们可以通过修改 Linux的核心profile文件来添加jdk的环境变量，但是我们通常不会这么做，原因就是不希望改动Linux原有的核心文件，以免引起不必要的麻烦，那我们怎么做呢？推荐方式就是自己在指定的目录下创建一个xxx.sh文件用来充当我们自己的配置文件。当Linux系统启动后会加载profile 文件，而profile文件中的脚本会循环遍历加载 /etc/profile.d/ 目录下所有以sh为后缀名的文件，所以我们自己创建xxx.sh文件也就被加载到了。固然环境变量也就生效了！</p><ul><li><p>在/etc/profile.d/目录下新建文件 my_env.sh文件</p> <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo vim &#x2F;etc&#x2F;profile.d&#x2F;my_env.sh</span><br></pre></td></tr></table></figure></li><li><p>在my_env.sh文件中添加一下内容</p> <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#JAVA_HOME</span><br><span class="line">export JAVA_HOME&#x3D;&#x2F;opt&#x2F;module&#x2F;jdk1.8.0_212</span><br><span class="line">export PATH&#x3D;$PATH:$JAVA_HOME&#x2F;bin</span><br></pre></td></tr></table></figure></li><li><p>保存后退出</p> <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">:wq</span><br></pre></td></tr></table></figure></li><li><p>source 重新加载 /etc/profile文件，环境变量生效</p> <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">source &#x2F;etc&#x2F;profile</span><br></pre></td></tr></table></figure></li><li><p>验证jdk是否安装以及配置成功</p> <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">java -version</span><br></pre></td></tr></table></figure><p> 如下图就成功了！</p> <img src="Hadoop笔记总结-01.assets/image-20200729231158594.png" alt="image-20200729231158594" style="zoom:80%;" /><p> 如果没成功就reboot重启Linux，如果没问题就不用了重启！</p></li></ul></li></ul><h3 id="3-在开发机hadoop101安装Hadoop"><a href="#3-在开发机hadoop101安装Hadoop" class="headerlink" title="3. 在开发机hadoop101安装Hadoop"></a>3. 在开发机hadoop101安装Hadoop</h3><p><strong>概述：</strong>终于要安装hadoop了，hadoop我们把它看做适合jdk是同一类型的软件，jdk怎么操作hadoop也怎么操作就可以！</p><ul><li><p> <strong>1). 将hadoop的tar包拖拽到/opt/software目录下</strong></p></li><li><p><strong>2). 将hadoop解压缩到/opt/module目录下</strong></p> <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tar -zxvf hadoop-3.1.3.tar.gz -C &#x2F;opt&#x2F;module&#x2F;</span><br></pre></td></tr></table></figure></li><li><p><strong>3).配置hadoop的环境变量</strong></p><p> <strong>注意：</strong>hadoop中有一个特别之处，就是在hadoop的目录下的bin目录和sbin目录都是hadoop的执行脚本，所以我们在配置hadoop的环境变量的时候要注意把这两个都配上才可以！剩下其他的操作都和jdk一样了！</p><ul><li><p>打开/etc/profile.d/my_env.sh文件</p> <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo vim &#x2F;etc&#x2F;profile.d&#x2F;my_env.sh</span><br></pre></td></tr></table></figure></li><li><p>在my_env.sh文件末尾添加如下内容：（shift+g）</p> <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#HADOOP_HOME</span><br><span class="line">export HADOOP_HOME&#x3D;&#x2F;opt&#x2F;module&#x2F;hadoop-3.1.3</span><br><span class="line">export PATH&#x3D;$PATH:$HADOOP_HOME&#x2F;bin</span><br><span class="line">export PATH&#x3D;$PATH:$HADOOP_HOME&#x2F;sbin</span><br></pre></td></tr></table></figure></li><li><p>保存退出</p> <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">:wq</span><br></pre></td></tr></table></figure></li><li><p>source 重新加载 /etc/profile文件，环境变量生效</p> <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">source &#x2F;etc&#x2F;profile</span><br></pre></td></tr></table></figure></li><li><p>验证hadoop是否安装以及配置成功</p> <figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">hadoop version</span><br></pre></td></tr></table></figure><p> <img src="Hadoop%E7%AC%94%E8%AE%B0%E6%80%BB%E7%BB%93-01.assets/image-20200730000725426.png" alt="image-20200730000725426"></p><p> 如图所示表示安装成功！</p></li></ul></li></ul><h3 id="4-Hadoop目录结构"><a href="#4-Hadoop目录结构" class="headerlink" title="4. Hadoop目录结构"></a>4. Hadoop目录结构</h3><ul><li> <strong>bin：</strong> bin目录是Hadoop最基本的管理脚本和使用脚本所在的目录，这些脚本是sbin目录下管理脚本的基础实现，用户可以直接使用这些脚本管理和使用Hadoop</li><li> <strong>etc：</strong> Hadoop配置文件所在的目录，包括：core-site.xml、hdfs-site.xml、mapred-site.xml和yarn-site.xml等配置文件。</li><li> <strong>include：</strong>对外提供的编程库头文件（具体的动态库和静态库在lib目录中），这些文件都是用C++定义的，通常用于C++程序访问HDFS或者编写MapReduce程序。</li><li> <strong>lib：</strong>包含了Hadoop对外提供的编程动态库和静态库，与include目录中的头文件结合使用。</li><li> <strong>libexec：</strong>各个服务对应的shell配置文件所在的目录，可用于配置日志输出目录、启动参数（比如JVM参数）等基本信息。</li><li> <strong>sbin：</strong> Hadoop管理脚本所在目录，主要包含HDFS和YARN中各类服务启动/关闭的脚本。</li><li> <strong>share：</strong> Hadoop各个模块编译后的Jar包所在目录，这个目录中也包含了Hadoop文档。</li></ul><h2 id="四、Hadoop运行模式"><a href="#四、Hadoop运行模式" class="headerlink" title="四、Hadoop运行模式"></a>四、Hadoop运行模式</h2><p><strong>前言：</strong>本章节主要来学习Hadoop的运行模式，何谓运行模式呢？简单的讲就是Hadoop该如何运作起来，或者理解为玩Hadoop的游戏规则，是单台机器运行，还是多台协作运行，不同的运行模式有不一样的配置和处理。Hadoop中一共存在三种运行模式， 本地模式、伪分布式模式、完全分布式模式。</p><p><strong>本地模式：</strong>在一台单机上运行，没有分布式文件系统，而是直接读写本地操作系统的文件系统。</p><p><strong>伪分布式：</strong>这种模式也是在一台单机上运行，但用不同的Java进程模仿分布式运行中的各类结点: (NameNode,DataNode,JobTracker,TaskTracker,SecondaryNameNode) ，同理 集群中的结点由一个JobTracker和若干个TaskTracker组成，JobTracker负责任务的调度，TaskTracker负责并行执行任务。TaskTracker必须运行在DataNode上，这样便于数据的本地计算。JobTracker和NameNode则无须在同一台机器上。一个机器上，既当namenode，又当datanode,或者说 既 是jobtracker,又是tasktracker。没有所谓的在多台机器上进行真正的分布式计算，故称为”伪分布式”。</p><p><strong>完全分布式：</strong>真正的分布式，由3个及以上的实体机或者虚拟机组件的机群。</p><p><strong>注意：</strong>我们在课程中 用本地模式来入门开胃，然后集中火力做 <strong>完全分布式</strong> 伪分布式只做了解即可，没有太大意义！</p><h3 id="1-本地运行模式"><a href="#1-本地运行模式" class="headerlink" title="1.本地运行模式"></a>1.本地运行模式</h3><p>​    本小节主要就是感受一把Hadoop的运行过程，根据Hadoop官方提供的示例来操作几个Hadoop的基本功能点。更重要的是掌握基本操作Hadoop的步骤和思路。</p><p><strong>案例1需求描述：</strong>利用hadoop的grep过滤功能，将一批文件中的一些内容过滤出来。</p><p><strong>实现步骤：</strong></p><p><strong>1.1 在hadoop的解压目录创建一个文件夹input，作为需要过滤的文件的输入目录</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir input</span><br></pre></td></tr></table></figure><p>*<em>1.2 将hadoop目录下的 etc/hadoop/</em>.xml文件都复制到 input目录下，作为被过滤文件**</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cp &#x2F;etc&#x2F;hadoop&#x2F;*.xml input</span><br></pre></td></tr></table></figure><p><strong>1.3 执行 bin/hadoop 命令，运行share/hadoop/mapreduce/目录下的hadoop-mapreduce-examples-3.1.3.jar包中的 grep 过滤功能，并限制一定的规则</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bin&#x2F;hadoop jar share&#x2F;hadoop&#x2F;mapreduce&#x2F;hadoop-mapreduce-examples-3.1.3.jar grep input output &#39;dfs[a-z.]+&#39;</span><br></pre></td></tr></table></figure><p><strong>1.4 最后在output目录下查看过滤的结果即可！</strong></p><p><strong>案例2需求描述：</strong>利用Hadoop完成经典wordcount(单词统计)，就是针对一些文件计算统计里面相同单词的个数。</p><p><strong>实现步骤：</strong></p><p><strong>1.1 创建在hadoop-3.1.3文件下面创建一个wcinput文件夹</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir wcinput</span><br></pre></td></tr></table></figure><p><strong>1.2 在wcinput文件下创建一个word.txt文件</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd wcinput</span><br></pre></td></tr></table></figure><p><strong>1.3 编辑word.txt文件</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim word.txt</span><br><span class="line"></span><br><span class="line">在文件中输入如下内容(内容随意)</span><br><span class="line">hadoop yarn</span><br><span class="line">hadoop mapreduce</span><br><span class="line">atguigu</span><br><span class="line">atguigu</span><br></pre></td></tr></table></figure><p><strong>1.4 回到Hadoop目录/opt/module/hadoop-3.1.3</strong>  <strong>执行程序</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hadoop jar share&#x2F;hadoop&#x2F;mapreduce&#x2F;hadoop-mapreduce-examples-3.1.3.jar wordcount wcinput wcoutput</span><br></pre></td></tr></table></figure><p><strong>1.5 查看结果</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"> cat wcoutput&#x2F;part-r-00000</span><br><span class="line"> </span><br><span class="line">看到如下结果：</span><br><span class="line">atguigu 2</span><br><span class="line">hadoop  2</span><br><span class="line">mapreduce 1</span><br><span class="line">yarn    1</span><br></pre></td></tr></table></figure><h3 id="2-完全分布式运行模式-重点掌握"><a href="#2-完全分布式运行模式-重点掌握" class="headerlink" title="2.完全分布式运行模式(重点掌握)"></a>2.完全分布式运行模式(重点掌握)</h3><p>​    本章节是重中之重，主要讲解完全分布式运行模式。</p><h4 id="2-1-准备3台服务器"><a href="#2-1-准备3台服务器" class="headerlink" title="2.1 准备3台服务器"></a>2.1 准备3台服务器</h4><p>为了满足集群的环境，我们需要准备三台服务器，准备方式就是根据我们之前做好的模板机进行克隆即可，但是需要注意，三台服务器的的 静态ip地址和主机名都要修改一下，以便区分！</p><h5 id="2-1-1-克隆第一台"><a href="#2-1-1-克隆第一台" class="headerlink" title="2.1.1 克隆第一台"></a>2.1.1 克隆第一台</h5><p>修改主机名为hadoop102</p><p>修改ip地址为：192.168.2.102</p><h5 id="2-1-2-克隆第二台"><a href="#2-1-2-克隆第二台" class="headerlink" title="2.1.2 克隆第二台"></a>2.1.2 克隆第二台</h5><p>修改主机名为hadoop103</p><p>修改ip地址为：192.168.2.103</p><h5 id="2-1-3-克隆第三台"><a href="#2-1-3-克隆第三台" class="headerlink" title="2.1.3 克隆第三台"></a>2.1.3 克隆第三台</h5><p>修改主机名为hadoop104</p><p>修改ip地址为：192.168.2.104</p><h4 id="2-2-集群分发脚本的应用场景"><a href="#2-2-集群分发脚本的应用场景" class="headerlink" title="2.2 集群分发脚本的应用场景"></a>2.2 集群分发脚本的应用场景</h4><p><strong>场景介绍：</strong></p><p>​        上面我们已经准备好了三台服务器，并且都各自修改了主机名和ip地址。但是我们知道 需要额必备软件以及环境变量还没有配置，如果机械的一台一台配置也可以但是这样会引发大量的重复性工作，没有必要。如何能避免重复配置呢，最好是值在一台机器进行修改 然后将修改的配置信息同步到集群的所有机器那就完美了！这时候就要用到 分发脚本 的方案！</p><h5 id="2-2-1-scp-安全拷贝"><a href="#2-2-1-scp-安全拷贝" class="headerlink" title="2.2.1 scp 安全拷贝"></a>2.2.1 scp 安全拷贝</h5><p><strong>scp含义：</strong></p><p>​    scp命令可以实现服务器与服务器之间的数据拷贝</p><p><strong>基本语法：</strong></p><p>​    scp      -r          $pdir/$fname                 $user@hadoop$host:$pdir/$fname</p><p>​    命令  递归    要拷贝的文件路径/名称   目的用户@主机:目的路径/名称</p><p><strong>案例实操：</strong></p><p>前提：在 hadoop102 hadoop103 hadoop104 都已经创建好的 /opt/module</p><p>​      /opt/software 两个目录， 并且已经把这两个目录修改为atguigu:atguigu</p><p>​      sudo chown atguigu:atguigu -R /opt/module</p><p>1).在hadoop101上，将hadoop101中/opt/module/目录下所有内容拷贝到hadoop102上的/opt/module/目录下。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scp -r &#x2F;opt&#x2F;module&#x2F;* atguigu@hadoop102:&#x2F;opt&#x2F;module&#x2F;</span><br></pre></td></tr></table></figure><p>2).在hadoop103上，将hadoop101中/opt/module/目录下的所有内容拷贝到hadoop103的/opt/module/目录下。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scp -r atguigu@hadoop101:&#x2F;opt&#x2F;module&#x2F;* &#x2F;opt&#x2F;module&#x2F;</span><br></pre></td></tr></table></figure><p>3).在hadoop103上，将hadoop101中/opt/module/目录下的所有内容拷贝到hadoop104的/opt/module/目录下。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scp -r atguigu@hadoop101:&#x2F;opt&#x2F;module&#x2F;* atguigu@hadoop104:&#x2F;opt&#x2F;module&#x2F;</span><br></pre></td></tr></table></figure><p>4).在任意一台机器上，将hadoop101中的/etc/profile.d目录下的my_env.sh配置文件分别复制到hadoop102、hadoop103、hadoop104上</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1. scp -r &#x2F;etc&#x2F;profile.d&#x2F;my_env.sh root@hadoop102:&#x2F;etc&#x2F;profile.d&#x2F;</span><br><span class="line">2. scp -r &#x2F;etc&#x2F;profile.d&#x2F;my_env.sh root@hadoop103:&#x2F;etc&#x2F;profile.d&#x2F;</span><br><span class="line">3. scp -r &#x2F;etc&#x2F;profile.d&#x2F;my_env.sh root@hadoop104:&#x2F;etc&#x2F;profile.d&#x2F;</span><br></pre></td></tr></table></figure><h5 id="2-2-2-rsync远程同步工具"><a href="#2-2-2-rsync远程同步工具" class="headerlink" title="2.2.2 rsync远程同步工具"></a>2.2.2 rsync远程同步工具</h5><p><strong>功能描述：</strong></p><p>​        rsync主要用于备份和镜像。具有速度快、避免复制相同内容和支持符号链接的优点。</p><p><strong>rsync和scp区别：</strong></p><p>​        用rsync做文件的复制要比scp的速度快，rsync只对差异文件做更新。scp是把所有文件都复制过去。</p><p><strong>基本语法：</strong></p><p>rsync   -av    $pdir/$fname        $user@hadoop$host:$pdir/$fname</p><p>命令  选项参数  要拷贝的文件路径/名称  目的用户@主机:目的路径/名称</p><p>​     选项参数说明</p><table><thead><tr><th>选项</th><th>功能</th></tr></thead><tbody><tr><td>-a</td><td>归档拷贝</td></tr><tr><td>-v</td><td>显示复制过程</td></tr></tbody></table><p><strong>案例实操:</strong></p><p>把hadoop102机器上的/opt/software目录同步到hadoop103服务器的/opt/software目录下（没有实际意义的操作只是为了练手）</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rsync -av &#x2F;opt&#x2F;software&#x2F;* atguigu@hadoop103:&#x2F;opt&#x2F;software&#x2F;</span><br></pre></td></tr></table></figure><h5 id="2-2-3-分发脚本的应用"><a href="#2-2-3-分发脚本的应用" class="headerlink" title="2.2.3 分发脚本的应用"></a>2.2.3 分发脚本的应用</h5><p><strong>概述：</strong>前面其实我们已经是实现了服务器之间的文件目录拷贝传递了，但是每次都得执行命令来实现，还是比较麻烦的，干脆一步到位，通过编写一个脚本 通过执行脚本来实现信息拷贝。</p><p><strong>前提：</strong> 在/home/atguigu/bin这个目录下存放的脚本，atguigu用户可以在系统任何地方直接执行。</p><p><strong>脚本实现：</strong></p><p>1). 在/home/atguigu/bin目录下创建xsync文件</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 opt]$ cd &#x2F;home&#x2F;atguigu</span><br><span class="line">[atguigu@hadoop102 ~]$ mkdir bin</span><br><span class="line">[atguigu@hadoop102 ~]$ cd bin</span><br><span class="line">[atguigu@hadoop102 bin]$ vim xsync</span><br></pre></td></tr></table></figure><p>2). 在该文件中编写如下代码</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash">1. 判断参数个数</span></span><br><span class="line">if [ $# -lt 1 ]</span><br><span class="line">then</span><br><span class="line">  echo Not Enough Arguement!</span><br><span class="line">  exit;</span><br><span class="line">fi</span><br><span class="line"><span class="meta">#</span><span class="bash">2. 遍历集群所有机器</span></span><br><span class="line">for host in hadoop102 hadoop103 hadoop104</span><br><span class="line">do</span><br><span class="line">  echo ====================  $host  ====================</span><br><span class="line"><span class="meta">  #</span><span class="bash">3. 遍历所有目录，挨个发送</span></span><br><span class="line">  for file in $@</span><br><span class="line">  do</span><br><span class="line">    #4. 判断文件是否存在</span><br><span class="line">    if [ -e $file ]</span><br><span class="line">    then</span><br><span class="line">      #5. 获取父目录</span><br><span class="line">      pdir=$(cd -P $(dirname $file); pwd)</span><br><span class="line">      #6. 获取当前文件的名称</span><br><span class="line">      fname=$(basename $file)</span><br><span class="line">      ssh $host &quot;mkdir -p $pdir&quot;</span><br><span class="line">      rsync -av $pdir/$fname $host:$pdir</span><br><span class="line">    else</span><br><span class="line">      echo $file does not exists!</span><br><span class="line">    fi</span><br><span class="line">  done</span><br><span class="line">done</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>3). 修改文件的执行权限</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">chmod 777 xsync</span><br></pre></td></tr></table></figure><p>4). 将脚本复制到/bin中，以便全局调用</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo cp xsync &#x2F;bin&#x2F;</span><br></pre></td></tr></table></figure><p>5). 测试脚本</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">xsync test.txt</span><br></pre></td></tr></table></figure><h4 id="2-3-分布式集群规划"><a href="#2-3-分布式集群规划" class="headerlink" title="2.3 分布式集群规划"></a>2.3 分布式集群规划</h4><p>​    <strong>概述：</strong>接下来我们就要搭建Hadoop集群了，在操作之前一定要有具体的集群规划，集群规划其实就是把Hadoop中的核心组件如何安排到每台机器上。</p><p>​    <strong>分析：</strong> 通过前面的介绍我们知道 在Hadoop集群当中先要考虑数据的存储以及资源调度的安排。那就会涉及到NameNode 、ResourceManager 、SecondaryNameNode 、DataNode 、 NodeManager。如何把这些组件分布到每一台机器上，就得合理分析一下。</p><p>NameNode 、ResourceManager 、SecondaryNameNode 这三个组件相对来说比较耗费资源，我们通常把他们分布到不同的机器上。所以三台机器每一台分布一个。</p><p>DataNode是具体存储数据的，因为三台机器都具备存储空间，那每一台都分布一个DataNode</p><p>NodeManager是负责每一台机器的资源的管理，因此三台机器每一台也分布一个NodeManager</p><p><strong>hadoop102            NameNode                         DataNode              NodeManager</strong></p><p><strong>hadoop103            ResourceManager              DataNode              NodeManager</strong></p><p><strong>hadoop104            SecondaryNameNode      DataNode                NodeManager</strong></p><h4 id="2-4-搭建完全集群"><a href="#2-4-搭建完全集群" class="headerlink" title="2.4 搭建完全集群"></a>2.4 搭建完全集群</h4><h5 id="1-先删除每个节点中hadoop安装目录下的-data-和-logs目录，如果是最新解压配置的hadoop集群，并没有这两个目录就不需要进行删除这步。"><a href="#1-先删除每个节点中hadoop安装目录下的-data-和-logs目录，如果是最新解压配置的hadoop集群，并没有这两个目录就不需要进行删除这步。" class="headerlink" title="1.先删除每个节点中hadoop安装目录下的 data 和 logs目录，如果是最新解压配置的hadoop集群，并没有这两个目录就不需要进行删除这步。"></a><strong>1.先删除每个节点中hadoop安装目录下的 data 和 logs目录，如果是最新解压配置的hadoop集群，并没有这两个目录就不需要进行删除这步。</strong></h5><h5 id="2-在hadoop-env-sh文件中，配置JAVA-HOME-的环境变量，这是因为Hadoop运行的时候需要java的环境变量。"><a href="#2-在hadoop-env-sh文件中，配置JAVA-HOME-的环境变量，这是因为Hadoop运行的时候需要java的环境变量。" class="headerlink" title="2.在hadoop-env.sh文件中，配置JAVA_HOME 的环境变量，这是因为Hadoop运行的时候需要java的环境变量。"></a><strong>2.在hadoop-env.sh文件中，配置JAVA_HOME 的环境变量，这是因为Hadoop运行的时候需要java的环境变量。</strong></h5><h5 id="3-配置Hadoop的4大核心配置文件"><a href="#3-配置Hadoop的4大核心配置文件" class="headerlink" title="3.配置Hadoop的4大核心配置文件"></a><strong>3.配置Hadoop的4大核心配置文件</strong></h5><ul><li><p><strong>core-site.xml</strong>  这个是hadoop总的核心配置文件，集群加载启动的时候首先会加载解析此配置文件，具体配置内容如下：</p> <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span></span><br><span class="line"><span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment">  Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span></span><br><span class="line"><span class="comment">  you may not use this file except in compliance with the License.</span></span><br><span class="line"><span class="comment">  You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">    http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">  Unless required by applicable law or agreed to in writing, software</span></span><br><span class="line"><span class="comment">  distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span></span><br><span class="line"><span class="comment">  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="comment">  See the License for the specific language governing permissions and</span></span><br><span class="line"><span class="comment">  limitations under the License. See accompanying LICENSE file.</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Put site-specific property overrides in this file. --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--cmeNode的地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop102:9820<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定hadoop数据的存储目录 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-3.1.3/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    <span class="comment">&lt;!-- 配置该atguigu(superUser)允许通过代理访问的主机节点 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.atguigu.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 配置该atguigu(superUser)允许通过代理用户所属组 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.atguigu.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 配置该atguigu(superUser)允许通过代理的用户--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.atguigu.users<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">   </span><br></pre></td></tr></table></figure></li><li><p><strong>hdfs-site.xml</strong> 这个是hdfs的核心配置文件，具体配置内容如下：</p> <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"> <span class="meta">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span></span><br><span class="line"> <span class="meta">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span></span><br><span class="line"> <span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment">   Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span></span><br><span class="line"><span class="comment">  you may not use this file except in compliance with the License.</span></span><br><span class="line"><span class="comment">  You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">    http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">  Unless required by applicable law or agreed to in writing, software</span></span><br><span class="line"><span class="comment">  distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span></span><br><span class="line"><span class="comment">  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="comment">  See the License for the specific language governing permissions and</span></span><br><span class="line"><span class="comment">  limitations under the License. See accompanying LICENSE file.</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Put site-specific property overrides in this file. --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--指定NameNode数据的存储目录--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>file://$&#123;hadoop.data.dir&#125;/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--指定DataNode数据的存储目录--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>file://$&#123;hadoop.data.dir&#125;/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--指定SecondaryNameNode数据的存储目录--&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>file://$&#123;hadoop.data.dir&#125;/namesecondary<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- nn web端访问地址--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:9870<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 2nn web端访问地址--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop104:9868<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>yarn-site.xml 这个是Yarn的核心配置文件,具体内容如下：</p> <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot;?&gt;</span></span><br><span class="line"><span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment">  Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span></span><br><span class="line"><span class="comment">  you may not use this file except in compliance with the License.</span></span><br><span class="line"><span class="comment">  You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">    http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">  Unless required by applicable law or agreed to in writing, software</span></span><br><span class="line"><span class="comment">  distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span></span><br><span class="line"><span class="comment">  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="comment">  See the License for the specific language governing permissions and</span></span><br><span class="line"><span class="comment">  limitations under the License. See accompanying LICENSE file.</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Site specific YARN configuration properties --&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 指定MR走shuffle --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定ResourceManager的地址--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop103<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 环境变量的继承 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.env-whitelist<span class="tag">&lt;/<span class="name">name</span>&gt;</span>                         <span class="tag">&lt;<span class="name">value</span>&gt;</span>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- yarn容器允许分配的最大最小内存 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.minimum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>512<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.maximum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>4096<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- yarn容器允许管理的物理内存大小 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.memory-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>4096<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 关闭yarn对物理内存和虚拟内存的限制检查 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.pmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></li><li><p>mapred-site.xml  这是MapReduce配置文件，配置内容如下：</p> <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot;?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span></span><br><span class="line"><span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment">  Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span></span><br><span class="line"><span class="comment">  you may not use this file except in compliance with the License.</span></span><br><span class="line"><span class="comment">  You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">    http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">  Unless required by applicable law or agreed to in writing, software</span></span><br><span class="line"><span class="comment">  distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span></span><br><span class="line"><span class="comment">  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="comment">  See the License for the specific language governing permissions and</span></span><br><span class="line"><span class="comment">  limitations under the License. See accompanying LICENSE file.</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Put site-specific property overrides in this file. --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定MapReduce程序运行在Yarn上 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h5 id="4-启动HDFS，单独启动每一台机器上的组件（重点）"><a href="#4-启动HDFS，单独启动每一台机器上的组件（重点）" class="headerlink" title="4. 启动HDFS，单独启动每一台机器上的组件（重点）"></a><strong>4. 启动HDFS，单独启动每一台机器上的组件（重点）</strong></h5></li><li><ol><li><p>因为hdfs分布式文件系统本质是一个文件系统，固然在使用之前要进行格式化，那么在哪台机器格式化呢，就是hdfs的大哥NameNode所在的节点进行格式化，格式化命令如下：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop]$ hdfs namenode -format</span><br></pre></td></tr></table></figure></li></ol></li><li><ol start="2"><li>启动HDFS文件系统，注意：我们现在是每台机器逐个启动所以一定要清晰之前定的集群规划的方案，现在要启动HDFS文件系统，而HDFS系统又包含 NameNode、SecondaryNameNode、DataNode，这三大组件有分别被规划在 NameNode在hadoop102、SecondaryNameNode在hadoop104、以及每一台机器上都有DataNode，所以启动流程如下：</li></ol><ul><li><p>在hadoop102上 启动NameNode 命令如下：</p> <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop]$ hdfs --daemon start namenode</span><br></pre></td></tr></table></figure></li><li><p>在hadoop104上 启动SecondaryNameNode 命令如下：</p> <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop104 ~]$ hdfs --daemon start secondarynamenode</span><br></pre></td></tr></table></figure></li><li><p>在hadoop102 hadoop103 hadoop104 都启动DataNode 命令如下：</p> <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop]$ hdfs --daemon start datanode</span><br><span class="line">[atguigu@hadoop103 hadoop]$ hdfs --daemon start namenode</span><br><span class="line">[atguigu@hadoop104 hadoop]$ hdfs --daemon start namenode</span><br></pre></td></tr></table></figure></li></ul></li><li><ol start="3"><li>检测hdfs是否启动成功 Web端查看HDFS的NameNode</li></ol></li></ul><p>（a）浏览器中输入：<a href="http://hadoop102:9870/">http://hadoop102:9870</a></p><p>（b）查看HDFS上存储的数据信息</p><p><img src="Hadoop%E7%AC%94%E8%AE%B0%E6%80%BB%E7%BB%93-01.assets/image-20200821105319878.png" alt="image-20200821105319878"></p><h5 id="5-启动Yarn"><a href="#5-启动Yarn" class="headerlink" title="5. 启动Yarn"></a><strong>5. 启动Yarn</strong></h5><p>​        根据集群规划，Yarn的ResourceManager我们分布在hadoop103上，NodeManager每一台机器上都存在所以启动流程如下：</p><ul><li><p>1). 在hadoop103 启动resourcemanager 命令如下：</p> <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop103 hadoop]$ yarn --daemon start resourcemanager</span><br></pre></td></tr></table></figure></li><li><p>2). 分别在hadoop102、hadoop103、hadoop104 启动nodemanager 命令如下：</p> <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop]$ hdfs --daemon start nodemanager</span><br><span class="line">[atguigu@hadoop103 hadoop]$ hdfs --daemon start nodemanager</span><br><span class="line">[atguigu@hadoop104 hadoop]$ hdfs --daemon start nodemanager</span><br></pre></td></tr></table></figure></li><li><p>3). 检测Yarn是否启动成功 Web端查看YARN的ResourceManager</p><p> （a）浏览器中输入：<a href="http://hadoop103:8088/">http://hadoop103:8088</a></p><p> （b）查看YARN上运行的Job信息</p><p> <img src="Hadoop%E7%AC%94%E8%AE%B0%E6%80%BB%E7%BB%93-01.assets/image-20200821110928665.png" alt="image-20200821110928665"></p></li></ul><h5 id="6-简单测试使用集群"><a href="#6-简单测试使用集群" class="headerlink" title="6.简单测试使用集群"></a>6.简单测试使用集群</h5><p>​    <strong>前言：</strong> 接下来简单测试试用一下我们搭建好的集群环境，操作的目标就是在HDFS 文件系统上上传文件以及运行一下简单的MapReduce程序即可！但是这里需要我们注意的一个 <strong>问题就是 HDFS系统所指向的物理路径究竟是哪 一会应该往哪个路径下上传文件！</strong></p><p>​    <strong>问题一：HDFS文件系统怎么定位？</strong></p><p>​    首先我们清楚，当前集群是运行在Linux上的，而Linux又是在Windows系统中的通过虚拟机的方式运行的，所以HDFS文件系统本质上也是占用了我们当前电脑硬盘的一部分，通过hadoop体系为HDFS分配出的一块存储空间。但是一定要注意它具有独立性，是由Hadoop独立来管理的。</p><p>​    <strong>问题二：在操作HDFS文件系统的时候如何理解它的输入路径和输出路径？</strong></p><p>​    Hadoop如何识别是Linux路径还是HDFS路径呢？本质上还得看 Hadoop的核心配置文件的fs.defaultFS的配置信息。</p><p>当前我们搭建的集群配置如下：</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop102:9820<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>参考官网默认配置如下：</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p><strong>对比分析：</strong></p><p>1). Hadoop的fs.defaultFS的默认配置是file:///  如果解析的是这个配置，file:/// 本质上所表示的就是Linux本地路径，那么在操作中写输入输出就按照Linux的规则正常写就行，例如编写执行wordcount程序的命令如下：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop]$ hadoop jar share/hadoop/mapredece/hadoop-mapreduce-ecanples.jar wordcount wcinput/wc.input wcoutput</span><br></pre></td></tr></table></figure><p>2). 如果我们自己修改了core-site.xml 核心配置文件配置 fs.defaultFS 的值为hdfs://hadoop102:9820 那么意味着在解析输入输出路径的时候指向的是HDFS系统维护的目录结构 在HDFS系统底层维护的路径是  <strong>/user/atguigu/wcinput</strong> 所以如果是在这个情况下我们要操作wordcount程序就应该这么写了 命令如下：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop]$ hadoop jar share/hadoop/mapredece/hadoop-mapreduce-ecanples.jar wordcount /user/atguigu/wcinput/wc.input /user/atguigu/wcoutput</span><br></pre></td></tr></table></figure><p><strong>OK! 有了上面的内容作为支撑，下面我们就正式对Hadoop集群进行简单测试操作！！！</strong></p><h6 id="6-1-在HDFS中创建一个目录-user-atguigu-input-目录"><a href="#6-1-在HDFS中创建一个目录-user-atguigu-input-目录" class="headerlink" title="6.1 在HDFS中创建一个目录 /user/atguigu/input 目录"></a>6.1 在HDFS中创建一个目录 /user/atguigu/input 目录</h6><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hdfs dfs -rm -R /user/atguigu/input</span><br></pre></td></tr></table></figure><h6 id="6-2-将hadoop安装目下的wcinput-wc-input-文件上传到HDFS文件系统上的-user-atguigu-input-目录下"><a href="#6-2-将hadoop安装目下的wcinput-wc-input-文件上传到HDFS文件系统上的-user-atguigu-input-目录下" class="headerlink" title="6.2 将hadoop安装目下的wcinput/wc.input 文件上传到HDFS文件系统上的 /user/atguigu/input 目录下"></a>6.2 将hadoop安装目下的wcinput/wc.input 文件上传到HDFS文件系统上的 /user/atguigu/input 目录下</h6><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hdfs dfs -put wciput/wc.input /user/atguigu/input</span><br></pre></td></tr></table></figure><h6 id="6-3-如何在HDFS上查看具体存储的文件"><a href="#6-3-如何在HDFS上查看具体存储的文件" class="headerlink" title="6.3 如何在HDFS上查看具体存储的文件"></a>6.3 如何在HDFS上查看具体存储的文件</h6><p>DataNode的存储目录：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop103 hadoop-3.1.3]$ cd data/data/current/BP-1528516923-192.168.2.102-1597943910514/current/finalized/subdir0/subdir0/</span><br></pre></td></tr></table></figure><h6 id="6-4-测试Yarn是否能正常使用-还是以Mapreduce的wordcount程序为例"><a href="#6-4-测试Yarn是否能正常使用-还是以Mapreduce的wordcount程序为例" class="headerlink" title="6.4 测试Yarn是否能正常使用 还是以Mapreduce的wordcount程序为例"></a>6.4 测试Yarn是否能正常使用 还是以Mapreduce的wordcount程序为例</h6><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop104 hadoop-3.1.3]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /user/atguigu/input /user/atguigu/output</span><br></pre></td></tr></table></figure><p>6.5 在hdfs上面查看执行后的结果</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hdfs dfs -cat /user/atguigu/output/part-r-00000</span><br></pre></td></tr></table></figure><h5 id="7-SSH免密登录"><a href="#7-SSH免密登录" class="headerlink" title="7. SSH免密登录"></a>7. SSH免密登录</h5><p><strong>存在的问题：</strong> 集群启动和关闭，目前我们都是通过单点操作完成的，这样很不方便，于是就考虑能不能在一台机器上就能搞定集群的启动和关闭？</p><p><strong>分析：</strong></p><p>参照之前的脚本分发的思路，我们可以编写一个集群启动和关闭的脚本，就是把哪些在每一台机器上输入的命令封装到一个脚本中，然后通过执行脚本来实现集群启动关闭的目的。</p><p><strong>脚本的大概思路：</strong></p><p>​    登录到hadoop102  启动/关闭 namenode</p><p>​    登录到hadoop104  启动/关闭 secondarynamenode</p><p>​    登录到hadoop102   hadoop103   hadoop104  启动/关闭 datanode</p><p>​    登录到hadoop103 启动/关闭 resourcemanager </p><p>​    登录到hadoop102 hadoop103 hadoop104  启动/关闭 nodemanager</p><p><strong>如何登录远程的机器：</strong></p><p>语法：ssh ip/主机名 </p><p><strong>无密钥配置：</strong> 单纯的 ssh 命令操作，虽然可以只在一台机器操作了但是操作步骤较多，而且登录的时候每次都需要输入密码，我们接下来要做到免密登录+脚本控制</p><p><strong>免密登录的原理：</strong></p><p><img src="Hadoop%E7%AC%94%E8%AE%B0%E6%80%BB%E7%BB%93-01.assets/image-20200823002113546.png" alt="image-20200823002113546"></p><p><strong>实现步骤：</strong></p><p>1). 生成公钥和私钥：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 .ssh]$ ssh-keygen -t rsa</span><br></pre></td></tr></table></figure><p>然后敲（四次回车），就会生成两个文件id_rsa（私钥）、id_rsa.pub（公钥）</p><p>2). 将公钥拷贝到要免密登录的目标机器上</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 .ssh]$ ssh-copy-id hadoop102</span><br><span class="line">[atguigu@hadoop102 .ssh]$ ssh-copy-id hadoop103</span><br><span class="line">[atguigu@hadoop102 .ssh]$ ssh-copy-id hadoop104</span><br></pre></td></tr></table></figure><p>3). 注意，集群机器的配置</p><ul><li><p> 还需要在hadoop103上采用atguigu账号配置一下无密登录到hadoop102、hadoop103、hadoop104服务器上。</p></li><li><p> 还需要在hadoop104上采用atguigu账号配置一下无密登录到hadoop102、hadoop103、hadoop104服务器上。</p></li><li><p> 还需要在hadoop102上采用atguigu账号，配置一下无密登录到hadoop102、hadoop103、hadoop104；</p></li></ul><p>4).  .ssh文件夹下（~/.ssh）的文件功能解释</p><table><thead><tr><th>known_hosts</th><th>记录ssh访问过计算机的公钥(public  key)</th></tr></thead><tbody><tr><td>id_rsa</td><td>生成的私钥</td></tr><tr><td>id_rsa.pub</td><td>生成的公钥</td></tr><tr><td>authorized_keys</td><td>存放授权过的无密登录服务器公钥</td></tr></tbody></table><h5 id="8-集群的群起操作"><a href="#8-集群的群起操作" class="headerlink" title="8.集群的群起操作"></a>8.集群的群起操作</h5><p>​    当配置过了ssh免密登录，就可以对hadoop进行群起了（多台机器通过脚本一起启动），群起的脚本hadoop已经帮我们内置好了直接使用即可！但是要最终完成群起操作我们必须让启动/关闭脚本知道 NameNode  SecondaryNameNode  DataNode ResourceManager  NodeManager都在哪一台机器上分配，这个怎么做到呢？这个是由  hadoop安装目录下的 etc/hadoop/workers 配置文件来控制。</p><ul><li> 配置 workers 文件，内容如下：</li></ul><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hadoop102</span><br><span class="line">hadoop103</span><br><span class="line">hadoop104</span><br></pre></td></tr></table></figure><p><strong>注意：</strong>该文件中添加的内容结尾不允许有空格，文件中不允许有空行。</p><ul><li><p>启动集群</p><p>1). <strong>如果集群是第一次启动</strong>，需要在hadoop102节点格式化NameNode（注意格式化NameNode，会产生新的集群id，导致NameNode和DataNode的集群id不一致，集群找不到已往数据。如果集群在运行过程中报错，需要重新格式化NameNode的话，一定要先停止namenode和datanode进程，并且要删除所有机器的data和logs目录，然后再进行格式化。</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ hdfs namenode -format</span><br></pre></td></tr></table></figure><p>2). 启动HDFS</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ sbin/start-dfs.sh</span><br></pre></td></tr></table></figure><p>3). 在配置了ResourceManager的节点（hadoop103）启动YARN</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop103 hadoop-3.1.3]$ sbin/start-yarn.sh</span><br></pre></td></tr></table></figure><h5 id="9-群起群停脚本的编写"><a href="#9-群起群停脚本的编写" class="headerlink" title="9.群起群停脚本的编写"></a>9.群起群停脚本的编写</h5></li></ul><p>​    上面我们已经完成对集群的群起，但是还不够完美，我们操作执行了两个脚本才启动了hdfs和yarn，虽然hadoop也给我们提供了start-all.sh 脚本，但是通常开发中不建议使用，因为start-all.sh脚本启动的话会默认启动一些不必要的组件。我们想更加完美的群起 只执行一个脚本就能把hdfs和yarn都启动或者停止。接下来我们自己封装一个脚本来实现，步骤如下：</p><p>1). 进入到/home/atguigu/bin目录下创建一个<strong>群起/群停</strong>脚本，这样操作为了在任何位置都能执行脚本</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ cd /home/atguigu/bin</span><br><span class="line">[atguigu@hadoop102 ~]$ vim mycluster.sh</span><br></pre></td></tr></table></figure><p>2). 编写脚本内容：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">if [ $# -lt 1 ]</span><br><span class="line">then</span><br><span class="line">         echo &quot;No Args Input...&quot;</span><br><span class="line">         exit</span><br><span class="line">fi</span><br><span class="line">case $1 in</span><br><span class="line">&quot;start&quot;)</span><br><span class="line">         echo &quot;==================START HDFS===================&quot; </span><br><span class="line">         ssh hadoop102 /opt/module/hadoop-3.1.3/sbin/start-dfs.sh</span><br><span class="line">         echo &quot;==================START YARN===================&quot;</span><br><span class="line">         ssh hadoop103 /opt/module/hadoop-3.1.3/sbin/start-yarn.sh</span><br><span class="line">;;</span><br><span class="line">&quot;stop&quot;)</span><br><span class="line">         echo &quot;==================STOP YARN===================&quot;</span><br><span class="line">         ssh hadoop103 /opt/module/hadoop-3.1.3/sbin/stop-yarn.sh</span><br><span class="line">         echo &quot;==================STOP HDFS===================&quot; </span><br><span class="line">         ssh hadoop102 /opt/module/hadoop-3.1.3/sbin/stop-dfs.sh</span><br><span class="line"></span><br><span class="line">;;</span><br><span class="line">*)</span><br><span class="line">  echo &quot;Input Args Error!!!!&quot;</span><br><span class="line">;;</span><br><span class="line">esac</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>3). 保存后退出，然后赋予脚本执行权限</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 bin]$ chmod 777 myhadoop.sh</span><br></pre></td></tr></table></figure><p>4). 分发/home/atguigu/bin目录，保证自定义脚本在三台机器上都可以使用</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ xsync /home/atguigu/bin/</span><br></pre></td></tr></table></figure><h5 id="10-编写统一查看jps的脚本"><a href="#10-编写统一查看jps的脚本" class="headerlink" title="10.编写统一查看jps的脚本"></a>10.编写统一查看jps的脚本</h5><p>​    上面我们做了一个频繁的操作，就是总是在每一机器上输入 jps 命令，来查看当前机器的java进程，而且每次输入都是切换到服务器上输入，很麻烦，接下来我们要实现在一台机器就能查看整个集群的java进程。</p><p>1). 进入到/home/atguigu/bin目录下创建一个查看jps的脚本</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ cd /home/atguigu/bin</span><br><span class="line">[atguigu@hadoop102 ~]$ vim jpsall.sh</span><br></pre></td></tr></table></figure><p>2). 编辑脚本内容如下：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">for i in hadoop102 hadoop103 hadoop104</span><br><span class="line">do</span><br><span class="line">echo &quot;***************$i JPS****************&quot;</span><br><span class="line">ssh $i /opt/module/jkd1.8.0_212/bin/jps</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p>3).  保存后退出，然后赋予脚本执行权限</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 bin]$ chmod 777 jpsall.sh</span><br></pre></td></tr></table></figure><p>4). 测试</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 bin]$ jpsall.sh </span><br></pre></td></tr></table></figure><p>结果如下：</p><p><img src="Hadoop%E7%AC%94%E8%AE%B0%E6%80%BB%E7%BB%93-01.assets/image-20200823233114341.png" alt="image-20200823233114341"></p><h5 id="11-历史服务器的使用"><a href="#11-历史服务器的使用" class="headerlink" title="11.历史服务器的使用"></a>11.历史服务器的使用</h5><p>​    这一小节主要介绍hadoop的历史服务器的使用！什么是历史服务器呢？举个例子就是我们在YARN上跑的一些job的历史记录，当重启YARN后之前执行过的job任务记录就会消失，hadoop为了更好的追溯和记录这些job执行记录专门提供了一个历史服务器，只要我们在Hadoop中配置了历史服务器那么以后就可以很方便查看执行过的所有job。</p><p>1).配置mapred-site.xml</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop]$ vim mapred-site.xml</span><br></pre></td></tr></table></figure><p>在该文件里面增加如下配置:</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 历史服务器端地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 历史服务器web端地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>2). 分发配置</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop]$ xsync $HADOOP_HOME/etc/hadoop/mapred-site.xml</span><br></pre></td></tr></table></figure><p>3). 在hadoop102启动历史服务器</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop]$ mapred --daemon start historyserver</span><br></pre></td></tr></table></figure><p>4). 查看历史服务器是否启动</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop]$ jps</span><br></pre></td></tr></table></figure><ol start="5"><li>web端查看历史服务器的图形化界面</li></ol><p><a href="http://hadoop102:19888/jobhistory">http://hadoop102:19888/jobhistory</a></p><h5 id="12-配置日志的聚集"><a href="#12-配置日志的聚集" class="headerlink" title="12.配置日志的聚集"></a>12.配置日志的聚集</h5><p>​    本小节主要对hadoop中的日志进行合理性的管理，方便我们更好的查阅。默认情况下 Hadoop作业执行的日志保存在hadoop的安装目录下logs下面。我们可以在linux上直接查看，但是这样操作不够人性化，查阅起来也比较麻烦。所以我们可以在执行job任务的时候产生日志后，让它自动的保存到hdfs系统中，这样就可以在网页中通过访问HDFS系统的web端地址来查看日志了！如果想完成上述操作需要我们进行以下几步配置和操作。</p><p>1）配置yarn-site.xml</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop]$ vim yarn-site.xml</span><br></pre></td></tr></table></figure><p>内容如下：</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 开启日志聚集功能 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 设置日志聚集服务器地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span>  </span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log.server.url<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>http://hadoop102:19888/jobhistory/logs<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 设置日志保留时间为7天 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation.retain-seconds<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>604800<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>2）分发配置</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop]$ xsync $HADOOP_HOME/etc/hadoop/yarn-site.xml</span><br></pre></td></tr></table></figure><p>3）关闭NodeManager、ResourceManager和HistoryServer</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop103 ~]$ stop-yarn.sh</span><br><span class="line">[atguigu@hadoop102 ~]$ mapred --daemon stop historyserver</span><br></pre></td></tr></table></figure><p>4）启动NodeManager 、ResourceManage和HistoryServer </p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop103 ~]$ start-yarn.sh</span><br><span class="line">[atguigu@hadoop102 ~]$ mapred --daemon start historyserver</span><br></pre></td></tr></table></figure><p>5）执行wordcount程序</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ hadoop jar  $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /input /output</span><br></pre></td></tr></table></figure><p>6）Web端查看日志</p><p>​    <a href="http://hadoop102:19888/jobhistory">http://hadoop102:19888/jobhistory</a></p><p><img src="Hadoop%E7%AC%94%E8%AE%B0%E6%80%BB%E7%BB%93-01.assets/image-20200824172706209.png" alt="image-20200824172706209"></p><p><img src="Hadoop%E7%AC%94%E8%AE%B0%E6%80%BB%E7%BB%93-01.assets/image-20200824172720289.png" alt="image-20200824172720289"></p><p><img src="Hadoop%E7%AC%94%E8%AE%B0%E6%80%BB%E7%BB%93-01.assets/image-20200824172736804.png" alt="image-20200824172736804"></p><h5 id="13-集群时间同步"><a href="#13-集群时间同步" class="headerlink" title="13. 集群时间同步"></a>13. 集群时间同步</h5><p>​    本小节主要操作在集群环境下，每一台服务器之间的时间同步。时间同步是很有必要的，因为在多台机器协同工作的时候，必然要求时间统一 要不然就会出问题。以下内容只要求大致了解 这项工作一般在运维的范畴。</p><p><strong>1）时间服务器配置(必须root用户</strong>)</p><p>（0）查看所有节点ntpd服务状态和开机自启动状态</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ sudo systemctl status ntpd</span><br><span class="line">[atguigu@hadoop102 ~]$ sudo systemctl is-enabled ntpd</span><br></pre></td></tr></table></figure><p>（1）在所有节点关闭ntpd服务和自启动</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ sudo systemctl stop ntpd</span><br><span class="line">[atguigu@hadoop102 ~]$ sudo systemctl disable ntpd</span><br></pre></td></tr></table></figure><p>（2）修改hadoop102的ntp.conf配置文件</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ sudo vim /etc/ntp.conf</span><br></pre></td></tr></table></figure><p>修改内容如下:</p><p>​    a）修改1（授权192.168.1.0-192.168.1.255网段上的所有机器可以从这台机器上查询和同步时间）</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap</span></span><br><span class="line">改为（就是把注释去掉）：</span><br><span class="line">restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap</span><br></pre></td></tr></table></figure><p>​    b）修改2（集群在局域网中，不使用其他互联网上的时间）</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">server 0.centos.pool.ntp.org iburst</span><br><span class="line">server 1.centos.pool.ntp.org iburst</span><br><span class="line">server 2.centos.pool.ntp.org iburst</span><br><span class="line">server 3.centos.pool.ntp.org iburst</span><br><span class="line">改为（都加上注释）：</span><br><span class="line"><span class="meta">#</span><span class="bash">server 0.centos.pool.ntp.org iburst</span></span><br><span class="line"><span class="meta">#</span><span class="bash">server 1.centos.pool.ntp.org iburst</span></span><br><span class="line"><span class="meta">#</span><span class="bash">server 2.centos.pool.ntp.org iburst</span></span><br><span class="line"><span class="meta">#</span><span class="bash">server 3.centos.pool.ntp.org iburst</span></span><br></pre></td></tr></table></figure><p>​    c）添加3（当该节点丢失网络连接，依然可以采用本地时间作为时间服务器为集群中的其他节点提供时间同步）</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">server 127.127.1.0</span><br><span class="line">fudge 127.127.1.0 stratum 10</span><br></pre></td></tr></table></figure><p>（3）修改hadoop102的/etc/sysconfig/ntpd 文件</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ sudo vim /etc/sysconfig/ntpd</span><br></pre></td></tr></table></figure><p>增加内容如下（让硬件时间与系统时间一起同步）</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">SYNC_HWCLOCK=yes</span><br></pre></td></tr></table></figure><p>（4）重新启动ntpd服务</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ sudo systemctl start ntpd</span><br></pre></td></tr></table></figure><p>（5）设置ntpd服务开机启动</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ sudo systemctl enable ntpd</span><br></pre></td></tr></table></figure><p><strong>2）在其他机器进行时间同步操作（必须root用户）</strong></p><p>（1）在其他机器配置1分钟与时间服务器同步一次</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop103 ~]$ sudo crontab -e</span><br></pre></td></tr></table></figure><p>编写定时任务如下：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">*/1 * * * * /usr/sbin/ntpdate hadoop102</span><br></pre></td></tr></table></figure><p>（2）修改任意机器时间</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop103 ~]$ sudo date -s &quot;2018-8-08 08:08:08&quot;</span><br></pre></td></tr></table></figure><p>（3）一分钟后查看机器是否与时间服务器同步</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop103 ~]$ sudo date</span><br></pre></td></tr></table></figure><h2 id="五、Hadoop编译源码"><a href="#五、Hadoop编译源码" class="headerlink" title="五、Hadoop编译源码"></a>五、Hadoop编译源码</h2><h2 id="六、常见错误及解决方案"><a href="#六、常见错误及解决方案" class="headerlink" title="六、常见错误及解决方案"></a>六、常见错误及解决方案</h2>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
