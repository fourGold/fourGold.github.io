<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Hadoop下的存储格式LZO</title>
      <link href="2021/01/04/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Hadoop%E4%B8%8B%E7%9A%84%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F/"/>
      <url>2021/01/04/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Hadoop%E4%B8%8B%E7%9A%84%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[<h1 id="Hadoop下的数据存储格式LZO"><a href="#Hadoop下的数据存储格式LZO" class="headerlink" title="Hadoop下的数据存储格式LZO"></a>Hadoop下的数据存储格式LZO</h1><h2 id="0x1-为什么要使用压缩技术"><a href="#0x1-为什么要使用压缩技术" class="headerlink" title="0x1 为什么要使用压缩技术?"></a>0x1 为什么要使用压缩技术?</h2><p>压缩技术能够有效减少底层存储系统(HDFS)读写字节数. 压缩提高了网络带宽和磁盘空间的效率</p><p>MR中的场景 (I/O,网络数据传输,Shuffle,Merge)</p><p>用CPU换I/O</p><h2 id="0x2-流行压缩技术1-Snappy"><a href="#0x2-流行压缩技术1-Snappy" class="headerlink" title="0x2 流行压缩技术1-Snappy"></a>0x2 流行压缩技术1-Snappy</h2><p>高速压缩速度,合理的压缩率</p><p>性能最高,但是不支持切片(split)</p><h2 id="0x3-流行压缩技术2-Lzo-在这里重点说明"><a href="#0x3-流行压缩技术2-Lzo-在这里重点说明" class="headerlink" title="0x3 流行压缩技术2-Lzo[在这里重点说明]"></a>0x3 流行压缩技术2-Lzo[在这里重点说明]</h2><h3 id="3-1-概要"><a href="#3-1-概要" class="headerlink" title="3.1 概要"></a>3.1 概要</h3><p>Hadoop体系下支持Lzo,需要安装</p><p>特点: 压缩/解压速度比较快,支持split</p><h3 id="3-2-详解"><a href="#3-2-详解" class="headerlink" title="3.2 详解"></a>3.2 详解</h3><p>优点:压缩/解压缩速度也比较快,合理的压缩率,支持split,是Hadoop中比较流行的压缩格式,可以再Linux中安装lzop命令,使用方便</p><p>缺点:压缩率比Gzip要低一些,Hadoop本身不支持,需要安装;在应用中对Lzo格式的文件需要做一些特殊处理(为了支持Split需要建立索引,还需要指定InputFormat为Lzo格式.</p><p>应用场景:一个很大的文本文件,压缩之后还大于200M以上的可以考虑,而且单个文件越大,lzo优点越明显.</p><p>看到这里肯定冒出几个问题</p><h4 id="3-2-1-LZO的压缩速度快-快多少呢-如何跟其他的压缩格式相比"><a href="#3-2-1-LZO的压缩速度快-快多少呢-如何跟其他的压缩格式相比" class="headerlink" title="3.2.1 LZO的压缩速度快,快多少呢,如何跟其他的压缩格式相比?"></a>3.2.1 LZO的压缩速度快,快多少呢,如何跟其他的压缩格式相比?</h4><p>直接上数据</p><table><thead><tr><th>压缩格式</th><th>算法</th><th>文件扩展名</th><th>是否可切分</th></tr></thead><tbody><tr><td>DEFLATE</td><td>DEFLATE</td><td>.deflate</td><td>否</td></tr><tr><td>Gzip</td><td>DEFLATE</td><td>.gz</td><td>否</td></tr><tr><td>bzip2</td><td>bzip2</td><td>.bz2</td><td>是</td></tr><tr><td>==LZO==</td><td>LZO</td><td>.lzo</td><td>是</td></tr><tr><td>Snappy</td><td>Snappy</td><td>.snappy</td><td>否</td></tr></tbody></table><p>为了支持多种压缩/解压缩算法，Hadoop引入了编码/解码器，如下表所示：</p><table><thead><tr><th>压缩格式</th><th>对应的编码/解码器</th></tr></thead><tbody><tr><td>DEFLATE</td><td>org.apache.hadoop.io.compress.DefaultCodec</td></tr><tr><td>gzip</td><td>org.apache.hadoop.io.compress.GzipCodec</td></tr><tr><td>bzip2</td><td>org.apache.hadoop.io.compress.BZip2Codec</td></tr><tr><td>LZO</td><td>com.hadoop.compression.lzo.LzopCodec</td></tr><tr><td>Snappy</td><td>org.apache.hadoop.io.compress.SnappyCodec</td></tr></tbody></table><p>压缩性能的比较：</p><table><thead><tr><th>压缩算法</th><th>原始文件大小</th><th>压缩文件大小</th><th>压缩速度</th><th>解压速度</th></tr></thead><tbody><tr><td>gzip</td><td>8.3GB</td><td>1.8GB</td><td>17.5MB/s</td><td>58MB/s</td></tr><tr><td>bzip2</td><td>8.3GB</td><td>1.1GB</td><td>2.4MB/s</td><td>9.5MB/s</td></tr><tr><td>LZO</td><td>8.3GB</td><td>2.9GB</td><td>49.3MB/s</td><td>74.6MB/s</td></tr></tbody></table><h4 id="3-2-2-LZO的比较好用-在大数据体系中哪些地方可以使用-如何使用"><a href="#3-2-2-LZO的比较好用-在大数据体系中哪些地方可以使用-如何使用" class="headerlink" title="3.2.2 LZO的比较好用,在大数据体系中哪些地方可以使用,如何使用?"></a>3.2.2 LZO的比较好用,在大数据体系中哪些地方可以使用,如何使用?</h4><p>1.Hive建表中可以直接导入HDFS中lzo格式的数据(要想走计算引擎需要增加索引)</p><p>2.Flume采集可以直接转换采集格式放入HDFS(hdfs sink)</p><p>参数在这里不多赘余,上链接</p><blockquote><p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+LZO#space-menu-link-content">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+LZO#space-menu-link-content</a></p></blockquote><h4 id="3-2-3-LZO为什么靠什么进行切片-为什么切片后能够被Driver进行Split-精"><a href="#3-2-3-LZO为什么靠什么进行切片-为什么切片后能够被Driver进行Split-精" class="headerlink" title="3.2.3 LZO为什么靠什么进行切片,为什么切片后能够被Driver进行Split[精]"></a>3.2.3 LZO为什么靠什么进行切片,为什么切片后能够被Driver进行Split[精]</h4><h2 id="0x4-文件存储格式-列存储的两种"><a href="#0x4-文件存储格式-列存储的两种" class="headerlink" title="0x4 文件存储格式(列存储的两种)"></a>0x4 文件存储格式(列存储的两种)</h2><p>(TEXTFILE 、SEQUENCEFILE、ORC、PARQUET)</p><p>行存储跟列存储 Row layout/Column layout</p><h3 id="4-1-ORC"><a href="#4-1-ORC" class="headerlink" title="4.1 ORC"></a>4.1 ORC</h3><p>Orc (Optimized Row Columnar)是Hive 0.11版里引入的新的存储格式。如下图所示可以看到每个Orc文件由1个或多个stripe组成，每个stripe一般为HDFS的块大小，每一个stripe包含多条记录，这些记录按照列进行独立存储，对应到Parquet中的row group的概念。每个Stripe里有三部分组成，分别是Index Data，Row Data，Stripe Footer</p><h3 id="4-2-Parquent"><a href="#4-2-Parquent" class="headerlink" title="4.2 Parquent"></a>4.2 Parquent</h3><p>Parquet文件是以二进制方式存储的，所以是不可以直接读取的，文件中包括该文件的数据和元数据，因此Parquet格式文件是自解析的。</p><p>（1）行组(Row Group)：每一个行组包含一定的行数，在一个HDFS文件中至少存储一个行组，类似于orc的stripe的概念。</p><p>（2）列块(Column Chunk)：在一个行组中每一列保存在一个列块中，行组中的所有列连续的存储在这个行组文件中。一个列块中的值都是相同类型的，不同的列块可能使用不同的算法进行压缩。</p><p>（3）页(Page)：每一个列块划分为多个页，一个页是最小的编码的单位，在同一个列块的不同页可能使用不同的编码方式。</p><p>通常情况下，在存储Parquet数据的时候会按照Block大小设置行组的大小，由于一般情况下每一个Mapper任务处理数据的最小单位是一个Block，这样可以把每一个行组由一个Mapper任务处理，增大任务执行并行度。Parquet文件的格式。</p>]]></content>
      
      
      <categories>
          
          <category> spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> lzo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark_累加器与分区器</title>
      <link href="2021/01/04/%E7%B4%AF%E5%8A%A0%E5%99%A8%E4%B8%8E%E5%88%86%E5%8C%BA%E5%99%A8%E6%80%BB%E7%BB%93/"/>
      <url>2021/01/04/%E7%B4%AF%E5%8A%A0%E5%99%A8%E4%B8%8E%E5%88%86%E5%8C%BA%E5%99%A8%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<h1 id="0x1-Long累加器的使用"><a href="#0x1-Long累加器的使用" class="headerlink" title="0x1 Long累加器的使用"></a>0x1 Long累加器的使用</h1><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">object selfAccumulator &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    val sc &#x3D; new SparkContext(new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;accumulator&quot;))    val word: RDD[String] &#x3D; sc.textFile(&quot;Day06&#x2F;input&#x2F;word.txt&quot;)    val words &#x3D; word.flatMap(_.split(&quot; &quot;))    val accumulator: LongAccumulator &#x3D; sc.longAccumulator&#x2F;&#x2F;定义数字累加器    words.foreach&#123;      case &quot;hello&quot; &#x3D;&gt; accumulator.add(1)      case _ &#x3D;&gt;    &#125;    println(&quot;hello:&quot;+accumulator.value)&#x2F;&#x2F;注意输出为accumulator的值  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="0x2-自定义分区器"><a href="#0x2-自定义分区器" class="headerlink" title="0x2 自定义分区器"></a>0x2 自定义分区器</h1><h2 id="1-HashPartitioner"><a href="#1-HashPartitioner" class="headerlink" title="1.HashPartitioner"></a>1.HashPartitioner</h2><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">object PartitionBy &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    val sparkConf: SparkConf &#x3D; new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;Main&quot;)    val sc: SparkContext &#x3D; new SparkContext(sparkConf)    val rdd1: RDD[(String, Int)] &#x3D; sc.textFile(&quot;Day06&#x2F;input&#x2F;number&quot;).map((_, 1))    &#x2F;&#x2F;只有k-v值才有分区器    rdd1.saveAsTextFile(&quot;.&#x2F;output&quot;)    &#x2F;&#x2F;使用hash分区器    val rdd2: RDD[(String, Int)] &#x3D; rdd1.partitionBy(new HashPartitioner(3))    rdd2.saveAsTextFile(&quot;.&#x2F;output2&quot;)  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="2-RangePartitioner"><a href="#2-RangePartitioner" class="headerlink" title="2.RangePartitioner"></a>2.RangePartitioner</h2><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">object PartitionBy &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    val sparkConf: SparkConf &#x3D; new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;Main&quot;)    val sc: SparkContext &#x3D; new SparkContext(sparkConf)    val rdd1&#x3D; sc.textFile(&quot;Day06&#x2F;input&#x2F;number&quot;).map((_, 1))    &#x2F;&#x2F;只有k-v值才有分区器    rdd1.saveAsTextFile(&quot;.&#x2F;output&quot;)    &#x2F;&#x2F;使用rangePartitioner    val value &#x3D; new RangePartitioner[String, Int](2, rdd1.sample(false, 0.5))    &#x2F;&#x2F;range分区器的使用,要定义泛型,传递分区,传递sample    &#x2F;&#x2F;首先要传递一个分区,传递一个    rdd1.partitionBy(value)    rdd1.saveAsTextFile(&quot;.&#x2F;output2&quot;)  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="3-自定义分区"><a href="#3-自定义分区" class="headerlink" title="3.自定义分区"></a>3.自定义分区</h2><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">case class MyPartitioner(numPartition:Int) extends Partitioner &#123;    override def numPartitions: Int &#x3D; numPartition    override def getPartition(key: Any): Int &#x3D; (math.random() * numPartition).toInt&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">object PartitionBy &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    val sparkConf: SparkConf &#x3D; new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;Main&quot;)    val sc: SparkContext &#x3D; new SparkContext(sparkConf)    val rdd1&#x3D; sc.textFile(&quot;Day06&#x2F;input&#x2F;number&quot;).map((_, 1))    &#x2F;&#x2F;只有k-v值才有分区器    rdd1.saveAsTextFile(&quot;.&#x2F;output&quot;)    &#x2F;&#x2F;使用rangePartitioner    val value &#x3D; new RangePartitioner[String, Int](2, rdd1.sample(false, 0.5))    &#x2F;&#x2F;range分区器的使用,要定义泛型,传递分区,传递sample    &#x2F;&#x2F;首先要传递一个分区,传递一个    val value1: RDD[(String, Int)] &#x3D; rdd1.partitionBy(MyPartitioner(2))    value1.saveAsTextFile(&quot;.&#x2F;output2&quot;)  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="0x3-自定义累加器"><a href="#0x3-自定义累加器" class="headerlink" title="0x3 自定义累加器"></a>0x3 自定义累加器</h1><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">package test.longAccumulatorimport org.apache.spark.rdd.RDDimport org.apache.spark.util.&#123;AccumulatorV2, LongAccumulator&#125;import org.apache.spark.&#123;SparkConf, SparkContext&#125;import scala.collection.mutable&#x2F;** * @create 2020-10-31 11:21 *&#x2F;object selfAccumulator &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    val sc &#x3D; new SparkContext(new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;accumulator&quot;))    val word: RDD[String] &#x3D; sc.textFile(&quot;Day06&#x2F;input&#x2F;word.txt&quot;)    val words &#x3D; word.flatMap(_.split(&quot; &quot;))    val accumulator: LongAccumulator &#x3D; sc.longAccumulator&#x2F;&#x2F;定义数字累加器    words.foreach&#123;      case &quot;hello&quot; &#x3D;&gt; accumulator.add(1)      case _ &#x3D;&gt;    &#125;    println(&quot;hello:&quot;+accumulator.value)&#x2F;&#x2F;注意输出为accumulator的值    println(&quot;--------------------------------------------------&quot;)  &#125;&#125;&#x2F;&#x2F;自定义累加器&#x2F;&#x2F;1.要继承的类为AccumulatorV2&#x2F;&#x2F;2.要注意描写泛型&#x2F;&#x2F;3.注意使用一种集合来累加class WCAccumulator extends AccumulatorV2[String,mutable.Map[String,Int]] &#123;  private val map &#x3D; mutable.Map.empty[String, Int]  &#x2F;&#x2F;如何理解Map.empty的创建map的方法&#x2F;&#x2F;创建空集合  &#x2F;&#x2F;传递一个你决定为空的方法  override def isZero: Boolean &#x3D; map.isEmpty  &#x2F;&#x2F;传递一个复制的方法  override def copy(): AccumulatorV2[String, mutable.Map[String, Int]] &#x3D; &#123;    val accumulator:WCAccumulator &#x3D; new WCAccumulator    accumulator.map ++&#x3D; this.map&#x2F;&#x2F;如何理解this?    accumulator&#x2F;&#x2F;传递回  &#125;  override def reset(): Unit &#x3D; map.clear()  override def add(v: String): Unit &#x3D; map.update(v,map.getOrElse(v,0)+1)  override def merge(other: AccumulatorV2[String, mutable.Map[String, Int]]): Unit &#x3D;    other.value.foreach&#123;&#x2F;&#x2F;传入的是每个key的值得键值对      case (word,count) &#x3D;&gt;        map.update(word,map.getOrElse(word,0)+count)&#x2F;&#x2F;累加器核心2    &#125;  override def value: mutable.Map[String, Int] &#x3D; map&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark_SubmitSource</title>
      <link href="2021/01/04/Spark_SubmitSource/"/>
      <url>2021/01/04/Spark_SubmitSource/</url>
      
        <content type="html"><![CDATA[<h1 id="1-Yarn-Client提交源码"><a href="#1-Yarn-Client提交源码" class="headerlink" title="1.Yarn Client提交源码"></a>1.Yarn Client提交源码</h1><p>爱护生命,少肝源码!</p><h2 id="1-任务的提交"><a href="#1-任务的提交" class="headerlink" title="1.任务的提交"></a>1.任务的提交</h2><p>==程序起点==</p><p>spark-submit.cmd</p><p>==脚本启动==</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">bin&#x2F;spark-submit--class WordCount--master yarn--deploy-mode cluster.&#x2F;wordCount.jar.&#x2F;input .&#x2F;output<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="2-SparkSubmit"><a href="#2-SparkSubmit" class="headerlink" title="2.SparkSubmit"></a>2.SparkSubmit</h2><h4 id="shell"><a href="#shell" class="headerlink" title="shell"></a>shell</h4><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">spark-submit --class org.apache.spark.examples.SparkPi --master local[2] ..&#x2F;examples&#x2F;jars&#x2F;spark-examples_2.12-3.0.0.jar 10<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="java-p"><a href="#java-p" class="headerlink" title="java -p"></a>java -p</h4><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">&quot;Java\jdk1.8.0_131\bin\java&quot; -cp &quot;C:\spark-3.0.0-bin-hadoop3.2\bin\..\conf\;\spark-3.0.0-bin-hadoop3.2\bin\..\jars\*&quot; -Xmx1g org.apache.spark.deploy.SparkSubmit --master local[2] --class org.apache.spark.examples.SparkPi ..&#x2F;examples&#x2F;jars&#x2F;spark-examples_2.12-3.0.0.jar 10<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>java -cp 等同于classpath</p><p>启动submit的main方法</p><ol><li>java -cp </li><li>开启JVM虚拟机 </li><li>开启Process（SparkSubmit)</li><li>程序入口SparkSubmit.main</li></ol><p>org.apache.spark.deploy.SparkSubmit，找到SparkSubmit的伴生对象，并找到main方法</p><h4 id="main"><a href="#main" class="headerlink" title="main"></a>main</h4><p><em>org/apache/spark/deploy/SparkSubmit.scala</em></p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">override def main(args: Array[String]): Unit &#x3D; &#123;    val submit &#x3D; new SparkSubmit() &#123;      self &#x3D;&gt;      override protected def parseArguments(args: Array[String]): SparkSubmitArguments &#x3D; &#123;new SparkSubmitArguments(args) &#123;...&#125;&#125;&#x2F;&#x2F;在主程序中new一个用于解析参数方法的的对象        ...&#x2F;&#x2F;日志方法    submit.doSubmit(args)&#x2F;&#x2F;调用doSubmit方法  &#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="doSubmit"><a href="#doSubmit" class="headerlink" title="doSubmit"></a>doSubmit</h4><p><em>org/apache/spark/deploy/SparkSubmit.scala</em></p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">def doSubmit(args: Array[String]): Unit &#x3D; &#123;    &#x2F;&#x2F; Initialize logging if it hasn&#39;t been done yet. Keep track of whether logging needs to    &#x2F;&#x2F; be reset before the application starts.    ...    val appArgs &#x3D; parseArguments(args)&#x2F;&#x2F;在doSubmit中调用解析参数方法-----------------&gt;parseArguments, appArgs为被解析出来的封装的参数   ...    appArgs.action match &#123;&#x2F;&#x2F;使用解析出来的参数appArgs判断参数信息的action字段      case SparkSubmitAction.SUBMIT &#x3D;&gt; submit(appArgs, uninitLog)&#x2F;&#x2F;----------------&gt;submit      case SparkSubmitAction.KILL &#x3D;&gt; kill(appArgs)      case SparkSubmitAction.REQUEST_STATUS &#x3D;&gt; requestStatus(appArgs)      case SparkSubmitAction.PRINT_VERSION &#x3D;&gt; printVersion()    &#125;  &#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="doSubmit-parseArguments"><a href="#doSubmit-parseArguments" class="headerlink" title="doSubmit/parseArguments"></a>doSubmit/parseArguments</h3><p><em>org/apache/spark/deploy/SparkSubmit.scala</em> </p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">protected def parseArguments(args: Array[String]): SparkSubmitArguments &#x3D; &#123;    new SparkSubmitArguments(args)  &#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="doSubmit-parseArguments-SparkSubmitArguments"><a href="#doSubmit-parseArguments-SparkSubmitArguments" class="headerlink" title="doSubmit/parseArguments/SparkSubmitArguments"></a>doSubmit/parseArguments/SparkSubmitArguments</h3><p><em>org/apache/spark/deploy/SparkSubmitArguments.scala</em></p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">private[deploy] class SparkSubmitArguments(args: Seq[String], env: Map[String, String] &#x3D; sys.env)  extends SparkSubmitArgumentsParser with Logging &#123;  var master: String &#x3D; null  var deployMode: String &#x3D; null  var executorMemory: String &#x3D; null  var executorCores: String &#x3D; null  ...&#x2F;&#x2F;各种参数  &#x2F;** Default properties present in the currently defined defaults file. *&#x2F;  lazy val defaultSparkProperties: HashMap[String, String] &#x3D; &#123;    val defaultProperties &#x3D; new HashMap[String, String]()    ...    &#125;    defaultProperties&#x2F;&#x2F;使用HashMap存储配置参数  &#125;&#x2F;&#x2F; Set parameters from command line arguments  parse(args.asJava)&#x2F;&#x2F;解析参数----------------------------&gt;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="doSubmit-parseArguments-SparkSubmitArguments-parse"><a href="#doSubmit-parseArguments-SparkSubmitArguments-parse" class="headerlink" title="doSubmit/parseArguments/SparkSubmitArguments/parse"></a>doSubmit/parseArguments/SparkSubmitArguments/parse</h3><p><em>org\apache\spark\launcher\SparkSubmitOptionParser.class</em></p><p>==疑问==</p><pre class="line-numbers language-java" data-language="java"><code class="language-java">protected final void parse(List&lt;String&gt; args) &#123;&#x2F;&#x2F;传入配置String        Pattern eqSeparatedOpt &#x3D; Pattern.compile(&quot;(--[^&#x3D;]+)&#x3D;(.+)&quot;);        int idx &#x3D; false;        int idx;        for(idx &#x3D; 0; idx &lt; args.size(); ++idx) &#123;            String arg &#x3D; (String)args.get(idx);            String value &#x3D; null;            Matcher m &#x3D; eqSeparatedOpt.matcher(arg);            ...                    value &#x3D; (String)args.get(idx);&#125;                if (!this.handle(name, value)) &#123;break;&#125;&#x2F;&#x2F;----------------------&gt;            &#125; else &#123;...&#125;        &#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="doSubmit-parseArguments-SparkSubmitArguments-parse-handle"><a href="#doSubmit-parseArguments-SparkSubmitArguments-parse-handle" class="headerlink" title="doSubmit/parseArguments/SparkSubmitArguments/parse/handle"></a>doSubmit/parseArguments/SparkSubmitArguments/parse/handle</h3><p><em>org/apache/spark/deploy/SparkSubmitArguments.scala</em></p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">override protected def handle(opt: String, value: String): Boolean &#x3D; &#123;    opt match &#123;      case NAME &#x3D;&gt;        name &#x3D; value      case MASTER &#x3D;&gt;        master &#x3D; value     ...      case DEPLOY_MODE &#x3D;&gt;        if (value !&#x3D; &quot;client&quot; &amp;&amp; value !&#x3D; &quot;cluster&quot;) &#123;          error(&quot;--deploy-mode must be either \&quot;client\&quot; or \&quot;cluster\&quot;&quot;)        &#125;        deployMode &#x3D; value      case NUM_EXECUTORS &#x3D;&gt;        numExecutors &#x3D; value      case TOTAL_EXECUTOR_CORES &#x3D;&gt;        totalExecutorCores &#x3D; value      case EXECUTOR_CORES &#x3D;&gt;        executorCores &#x3D; value      case EXECUTOR_MEMORY &#x3D;&gt;        executorMemory &#x3D; value      case DRIVER_MEMORY &#x3D;&gt;        driverMemory &#x3D; value      case DRIVER_CORES &#x3D;&gt;        driverCores &#x3D; value      ...    &#125;&#x2F;&#x2F;各种配置信息判断    action !&#x3D; SparkSubmitAction.PRINT_VERSION  &#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="doSubmit-submit"><a href="#doSubmit-submit" class="headerlink" title="doSubmit/submit"></a>doSubmit/submit</h3><p><em>org/apache/spark/deploy/SparkSubmit.scala</em></p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">private def submit(args: SparkSubmitArguments, uninitLog: Boolean): Unit &#x3D; &#123;    def doRunMain(): Unit &#x3D; &#123;      if (args.proxyUser !&#x3D; null) &#123;        val proxyUser &#x3D; UserGroupInformation.createProxyUser(args.proxyUser,          UserGroupInformation.getCurrentUser())        try &#123;          proxyUser.doAs(new PrivilegedExceptionAction[Unit]() &#123;            override def run(): Unit &#x3D; &#123;              runMain(args, uninitLog)&#x2F;&#x2F;-------------------------------&gt;            &#125;          &#125;)        &#125; catch &#123;...&#125;      &#125; else &#123;        runMain(args, uninitLog)&#x2F;&#x2F;---------------------------------&gt;      &#125;    &#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="doSubmit-submit-doRunMain-runMain"><a href="#doSubmit-submit-doRunMain-runMain" class="headerlink" title="doSubmit/submit/doRunMain/runMain"></a>doSubmit/submit/doRunMain/runMain</h3><p><em>org/apache/spark/deploy/SparkSubmit.scala</em></p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">&#x2F;**   * Run the main method of the child class using the submit arguments.   * 启动main程序的启动子类   * This runs in two steps. First, we prepare the launch environment by setting up   * the appropriate classpath, system properties, and application arguments for   * running the child main class based on the cluster manager and the deploy mode.   * Second, we use this launch environment to invoke the main method of the child   * main class.   * 激活启动子类   * Note that this main class will not be the one provided by the user if we&#39;re   * running cluster deploy mode or python applications.   *&#x2F;  private def runMain(args: SparkSubmitArguments, uninitLog: Boolean): Unit &#x3D; &#123;    val (childArgs, childClasspath, sparkConf, childMainClass) &#x3D; prepareSubmitEnvironment(args)&#x2F;&#x2F;------------------------------&gt;准备环境    ...&#x2F;&#x2F;日志相关    val loader &#x3D; getSubmitClassLoader(sparkConf)    for (jar &lt;- childClasspath) &#123;      addJarToClasspath(jar, loader)    &#125;    var mainClass: Class[_] &#x3D; null    try &#123;      mainClass &#x3D; Utils.classForName(childMainClass)&#x2F;&#x2F;使用反射将子启动类变为主启动类    &#125; catch &#123;...&#125;    val app: SparkApplication &#x3D; if (classOf[SparkApplication].isAssignableFrom(mainClass)) &#123;      mainClass.getConstructor().newInstance().asInstanceOf[SparkApplication]    &#125; else &#123;&#x2F;&#x2F;使用反射创建      new JavaMainApplication(mainClass)&#x2F;&#x2F;创建YarnClusterApplication    &#125;    @tailrec    def findCause(t: Throwable): Throwable &#x3D; t match &#123;      case e: UndeclaredThrowableException &#x3D;&gt;        if (e.getCause() !&#x3D; null) findCause(e.getCause()) else e      case e: InvocationTargetException &#x3D;&gt;        if (e.getCause() !&#x3D; null) findCause(e.getCause()) else e      case e: Throwable &#x3D;&gt;        e    &#125;    try &#123;      app.start(childArgs.toArray, sparkConf)&#x2F;&#x2F;启动YarnClusterApplication        &#x2F;&#x2F;----------------------------------&gt;    &#125; catch &#123;...&#125;  &#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="doSubmit-submit-doRunMain-runMain-prepareSubmitEnvironment"><a href="#doSubmit-submit-doRunMain-runMain-prepareSubmitEnvironment" class="headerlink" title="doSubmit/submit/doRunMain/runMain/prepareSubmitEnvironment"></a>doSubmit/submit/doRunMain/runMain/prepareSubmitEnvironment</h3><p><em>org/apache/spark/deploy/SparkSubmit.scala</em></p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">private[deploy] def prepareSubmitEnvironment(      args: SparkSubmitArguments,      conf: Option[HadoopConfiguration] &#x3D; None)      : (Seq[String], Seq[String], SparkConf, String) &#x3D; &#123;    &#x2F;&#x2F; Return values    val childArgs &#x3D; new ArrayBuffer[String]()    val childClasspath &#x3D; new ArrayBuffer[String]()    val sparkConf &#x3D; args.toSparkConf()    var childMainClass &#x3D; &quot;&quot;    &#x2F;&#x2F; Set the cluster manager    val clusterManager: Int &#x3D; args.master match &#123;      case &quot;yarn&quot; &#x3D;&gt; YARN&#x2F;&#x2F;判断环境为yarn环境------------------------&gt;      case m if m.startsWith(&quot;spark&quot;) &#x3D;&gt; STANDALONE      case m if m.startsWith(&quot;mesos&quot;) &#x3D;&gt; MESOS      case m if m.startsWith(&quot;k8s&quot;) &#x3D;&gt; KUBERNETES      case m if m.startsWith(&quot;local&quot;) &#x3D;&gt; LOCAL      case _ &#x3D;&gt;        error(&quot;Master must either be yarn or start with spark, mesos, k8s, or local&quot;)        -1    &#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="doSubmit-submit-doRunMain-runMain-start"><a href="#doSubmit-submit-doRunMain-runMain-start" class="headerlink" title="doSubmit/submit/doRunMain/runMain/start"></a>doSubmit/submit/doRunMain/runMain/start</h3><p>org/apache/spark/deploy/SparkApplication.scala</p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">private[spark] trait SparkApplication &#123;  def start(args: Array[String], conf: SparkConf): Unit&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>启动CtrL+H查看继承树</p><p><img src="Spark%E5%86%85%E6%A0%B8_%E6%80%BB%E7%BB%93.assets/1606639008096.png" alt="继承树"></p><p>Yarn源码文件结构</p><p><img src="Spark%E5%86%85%E6%A0%B8_%E6%80%BB%E7%BB%93.assets/1606638669349.png" alt="Yarn"></p><h3 id="Client-start"><a href="#Client-start" class="headerlink" title="Client/start"></a>Client/start</h3><p><em>org/apache/spark/deploy/yarn/Client.scala</em></p><p><em>org.apache.spark.deploy.yarn.YarnClusterApplication</em></p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">private[spark] class YarnClusterApplication extends SparkApplication &#123;  override def start(args: Array[String], conf: SparkConf): Unit &#x3D; &#123;    &#x2F;&#x2F; SparkSubmit would use yarn cache to distribute files &amp; jars in yarn mode,    &#x2F;&#x2F; so remove them from sparkConf here for yarn mode.    conf.remove(JARS)    conf.remove(FILES)    new Client(new ClientArguments(args), conf, null).run()&#x2F;&#x2F;---------------------------&gt;  &#125;&#x2F;&#x2F;---------------&gt;Client    &#x2F;&#x2F;--------------&gt;ClientArguments&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="Client-ClientArguments"><a href="#Client-ClientArguments" class="headerlink" title="Client/ClientArguments"></a>Client/ClientArguments</h3><p>org/apache/spark/deploy/yarn/ClientArguments.scala</p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">private[spark] class ClientArguments(args: Array[String]) &#123;  var userJar: String &#x3D; null  var userClass: String &#x3D; null  var primaryPyFile: String &#x3D; null  var primaryRFile: String &#x3D; null  var userArgs: ArrayBuffer[String] &#x3D; new ArrayBuffer[String]()  parseArgs(args.toList)&#x2F;&#x2F;-----------------------&gt;  private def parseArgs(inputArgs: List[String]): Unit &#x3D; &#123;&#x2F;&#x2F;&lt;---------------------    var args &#x3D; inputArgs    while (!args.isEmpty) &#123;      args match &#123;        case (&quot;--jar&quot;) :: value :: tail &#x3D;&gt;          userJar &#x3D; value          args &#x3D; tail        case (&quot;--class&quot;) :: value :: tail &#x3D;&gt;          userClass &#x3D; value          args &#x3D; tail        ....      &#125;    &#125;   ...  &#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="YarnClient-createYarnClient"><a href="#YarnClient-createYarnClient" class="headerlink" title="YarnClient.createYarnClient"></a>YarnClient.createYarnClient</h3><p><em>org/apache/spark/deploy/yarn/Client.scala/YarnClient.createYarnClient</em></p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">private[spark] class Client(    val args: ClientArguments,    val sparkConf: SparkConf,    val rpcEnv: RpcEnv)  extends Logging &#123;  import Client._  import YarnSparkHadoopUtil._      &#x2F;&#x2F;创建YARN客户端  private val yarnClient &#x3D; YarnClient.createYarnClient&#x2F;&#x2F;-----------------&gt;YarnClient      ....  &#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="2-ApplicationMaster任务"><a href="#2-ApplicationMaster任务" class="headerlink" title="2.ApplicationMaster任务"></a>2.ApplicationMaster任务</h2><h3 id="ApplicationMaster-main"><a href="#ApplicationMaster-main" class="headerlink" title="ApplicationMaster/main"></a>ApplicationMaster/main</h3><p><em>org.apache.spark.deploy.yarn.ApplicationMaster</em></p><h1 id="2-Spark提交流程"><a href="#2-Spark提交流程" class="headerlink" title="2.Spark提交流程"></a>2.Spark提交流程</h1>]]></content>
      
      
      <categories>
          
          <category> spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark_SparkStreaming_Case</title>
      <link href="2021/01/04/Spark_SparkStreaming_Practice/"/>
      <url>2021/01/04/Spark_SparkStreaming_Practice/</url>
      
        <content type="html"><![CDATA[<h1 id="SparkStreaming-Case-总结"><a href="#SparkStreaming-Case-总结" class="headerlink" title="SparkStreaming_Case_总结"></a>SparkStreaming_Case_总结</h1><p>工具类是必须要学习记忆的</p><p>同时要按照工业化流程写程序</p><p><img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_21_01/%E4%BC%81%E4%B8%9A%E7%BA%A7%E6%96%87%E4%BB%B6%E5%BB%BA%E6%A1%A3.png" alt="企业化程序思路"></p><h2 id="1-配置文件"><a href="#1-配置文件" class="headerlink" title="1.配置文件"></a>1.配置文件</h2><pre class="line-numbers language-properties" data-language="properties"><code class="language-properties">#JDBC配置jdbc.datasource.size&#x3D;10jdbc.url&#x3D;jdbc:mysql:&#x2F;&#x2F;hadoop102:3306&#x2F;spark_2020?useUnicode&#x3D;true&amp;characterEncoding&#x3D;utf8&amp;rewriteBatchedStatements&#x3D;truejdbc.user&#x3D;rootjdbc.password&#x3D;123456# Kafka配置kafka.broker.list&#x3D;hadoop102:9092,hadoop103:9092,hadoop104:9092kafka.topic&#x3D;testTopic<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="2-工具类"><a href="#2-工具类" class="headerlink" title="2.工具类"></a>2.工具类</h2><h3 id="2-1JDBC工具类"><a href="#2-1JDBC工具类" class="headerlink" title="2.1JDBC工具类"></a>2.1JDBC工具类</h3><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">package com.atguigu.utilsimport java.sql.&#123;Connection, PreparedStatement, ResultSet&#125;import java.util.Propertiesimport com.alibaba.druid.pool.DruidDataSourceFactoryimport javax.sql.DataSource&#x2F;** * @author Jinxin Li * @create 2020-11-25 19:49 *&#x2F;object JDBCUtil &#123;  &#x2F;&#x2F;初始化连接池  val dataSource: DataSource &#x3D; init()  &#x2F;&#x2F;初始化连接池方法  def init() &#x3D; &#123;    val properties &#x3D; new Properties()    val config: Properties &#x3D; PropertiesUtil.load(&quot;config.properties&quot;)    properties.setProperty(&quot;driverClassName&quot;, &quot;com.mysql.jdbc.Driver&quot;)    properties.setProperty(&quot;url&quot;, config.getProperty(&quot;jdbc.url&quot;))    properties.setProperty(&quot;username&quot;, config.getProperty(&quot;jdbc.user&quot;))    properties.setProperty(&quot;password&quot;, config.getProperty(&quot;jdbc.password&quot;))    properties.setProperty(&quot;maxActive&quot;, config.getProperty(&quot;jdbc.datasource.size&quot;))    &#x2F;&#x2F;使用Druid创建数据库连接池    val source: DataSource &#x3D; DruidDataSourceFactory.createDataSource(properties)    source  &#125;  &#x2F;&#x2F;获取MySQL连接  def getConnection: Connection &#x3D; &#123;    dataSource.getConnection  &#125;  &#x2F;&#x2F;执行SQL语句,单条数据插入  def executeUpdate(connection: Connection, sql: String, params: Array[Any]): Int &#x3D; &#123;    var rtn &#x3D; 0    var pstmt: PreparedStatement &#x3D; null    try &#123;      connection.setAutoCommit(false)      pstmt &#x3D; connection.prepareStatement(sql)      if (params !&#x3D; null &amp;&amp; params.length &gt; 0) &#123;        for (i &lt;- params.indices) &#123;          pstmt.setObject(i + 1, params(i))        &#125;      &#125;      rtn &#x3D; pstmt.executeUpdate()      connection.commit()      pstmt.close()    &#125; catch &#123;      case e: Exception &#x3D;&gt; e.printStackTrace()    &#125;    rtn  &#125;  &#x2F;&#x2F;判断一条数据是否存在  def isExist(connection: Connection, sql: String, params: Array[Any]): Boolean &#x3D; &#123;    var flag: Boolean &#x3D; false    var pstmt: PreparedStatement &#x3D; null    try &#123;      pstmt &#x3D; connection.prepareStatement(sql)      for (i &lt;- params.indices) &#123;        pstmt.setObject(i + 1, params(i))      &#125;      flag &#x3D; pstmt.executeQuery().next()      pstmt.close()    &#125; catch &#123;      case e: Exception &#x3D;&gt; e.printStackTrace()    &#125;    flag  &#125;  &#x2F;&#x2F;获取MySQL的一条数据  def getDataFromMysql(connection: Connection, sql: String, params: Array[Any]): Long &#x3D; &#123;    var result: Long &#x3D; 0L    var pstmt: PreparedStatement &#x3D; null    try &#123;      pstmt &#x3D; connection.prepareStatement(sql)      for (i &lt;- params.indices) &#123;        pstmt.setObject(i + 1, params(i))      &#125;      val resultSet: ResultSet &#x3D; pstmt.executeQuery()      while (resultSet.next()) &#123;        result &#x3D; resultSet.getLong(1)      &#125;      resultSet.close()      pstmt.close()    &#125; catch &#123;      case e: Exception &#x3D;&gt; e.printStackTrace()    &#125;    result  &#125;  &#x2F;&#x2F;主方法,用于测试上述方法  def main(args: Array[String]): Unit &#x3D; &#123;    &#x2F;&#x2F;1 获取连接    val connection: Connection &#x3D; getConnection    &#x2F;&#x2F;2 预编译SQL    val statement: PreparedStatement &#x3D; connection.prepareStatement(&quot;select * from user_ad_count where userid &#x3D; ?&quot;)    &#x2F;&#x2F;3 传输参数    statement.setObject(1, &quot;a&quot;)    &#x2F;&#x2F;4 执行sql    val resultSet: ResultSet &#x3D; statement.executeQuery()    &#x2F;&#x2F;5 获取数据    while (resultSet.next()) &#123;      println(&quot;111:&quot; + resultSet.getString(1))    &#125;    &#x2F;&#x2F;6 关闭资源    resultSet.close()    statement.close()    connection.close()  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="2-2Kafka工具类"><a href="#2-2Kafka工具类" class="headerlink" title="2.2Kafka工具类"></a>2.2Kafka工具类</h3><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">package com.atguigu.utilsimport org.apache.kafka.clients.consumer.ConsumerRecordimport org.apache.kafka.common.serialization.StringDeserializerimport org.apache.spark.streaming.StreamingContextimport org.apache.spark.streaming.dstream.InputDStreamimport org.apache.spark.streaming.kafka010.&#123;ConsumerStrategies, KafkaUtils, LocationStrategies&#125;&#x2F;** * @author Jinxin Li * @create 2020-11-25 19:28 *&#x2F;object MyKafkaUtil &#123;  &#x2F;&#x2F;创建配置对象]  private val properties &#x3D; PropertiesUtil.load(&quot;config.properties&quot;)  &#x2F;&#x2F;用于初始化链接到集群的地址  private val brokers: String &#x3D; properties.getProperty(&quot;kafka.broker.list&quot;) &#x2F;&#x2F;用于后面设置brokers使用  &#x2F;&#x2F;创建DStream,返回接受的数  def getKafkaStream(topic: String, ssc: StreamingContext) &#x3D; &#123;    val kafkaPara &#x3D; Map(&#x2F;&#x2F;      &quot;bootstrap.servers&quot; -&gt; brokers,&#x2F;&#x2F;      &quot;key.deserializer&quot; -&gt; classOf[StringDeserializer],&#x2F;&#x2F;      &quot;value.deserializer&quot; -&gt; classOf[StringDeserializer],&#x2F;&#x2F;      &quot;group.id&quot; -&gt; &quot;commerce-consumer-group&quot; &#x2F;&#x2F;定义消费者组      &quot;bootstrap.servers&quot; -&gt; brokers,      &quot;key.deserializer&quot; -&gt; classOf[StringDeserializer],      &quot;value.deserializer&quot; -&gt; classOf[StringDeserializer],      &quot;group.id&quot; -&gt; &quot;commerce-consumer-group&quot; &#x2F;&#x2F;消费者组    )    val kafkaDStream: InputDStream[ConsumerRecord[String, String]] &#x3D; KafkaUtils.createDirectStream[String, String](      ssc,      LocationStrategies.PreferConsistent,      ConsumerStrategies.Subscribe[String,String](Array(topic), kafkaPara))    kafkaDStream  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="2-3配置文件工具类"><a href="#2-3配置文件工具类" class="headerlink" title="2.3配置文件工具类"></a>2.3配置文件工具类</h3><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">package com.atguigu.utilsimport java.io.InputStreamReaderimport java.util.Properties&#x2F;** * @author Jinxin Li * @create 2020-11-25 19:15 *&#x2F;object PropertiesUtil &#123;  def main(args: Array[String]): Unit &#x3D; &#123;  &#125;  def load(name:String)&#x3D;&#123;    val properties &#x3D; new Properties()    properties.load(new InputStreamReader(Thread.currentThread().getContextClassLoader.getResourceAsStream(name)))    properties  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="3-日志生成与按比重随机数"><a href="#3-日志生成与按比重随机数" class="headerlink" title="3.日志生成与按比重随机数"></a>3.日志生成与按比重随机数</h2><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">package com.atguigu.dataproductionimport java.util.Propertiesimport com.atguigu.utils.PropertiesUtilimport org.apache.kafka.clients.producer.&#123;KafkaProducer, ProducerConfig, ProducerRecord&#125;import scala.collection.mutable.ArrayBufferimport scala.util.Random&#x2F;** * @author Jinxin Li * @create 2020-11-25 19:24 *&#x2F;&#x2F;&#x2F;作用两秒往kafka里创建一个数据&#x2F;&#x2F;城市信息表： city_id :城市id  city_name：城市名称   area：城市所在大区case class CityInfo(city_id: Long, city_name: String, area: String)object MockerRealTime &#123;  &#x2F;**   * 模拟的数据   * 格式 ：timestamp area city userid adid   * 某个时间点 某个地区 某个城市 某个用户 某个广告   * 1604229363531 华北 北京 3 3   *&#x2F;  def generateMockData(): Array[String] &#x3D; &#123;    val array: ArrayBuffer[String] &#x3D; ArrayBuffer[String]()    val CityRandomOpt &#x3D; RandomOptions(      RanOpt(CityInfo(1, &quot;北京&quot;, &quot;华北&quot;), 30),      RanOpt(CityInfo(2, &quot;上海&quot;, &quot;华东&quot;), 30),      RanOpt(CityInfo(3, &quot;广州&quot;, &quot;华南&quot;), 10),      RanOpt(CityInfo(4, &quot;深圳&quot;, &quot;华南&quot;), 20),      RanOpt(CityInfo(5, &quot;天津&quot;, &quot;华北&quot;), 10)    )    val random &#x3D; new Random()    &#x2F;&#x2F; 模拟实时数据：    &#x2F;&#x2F; timestamp province city userid adid    for (i &lt;- 0 to 50) &#123;      val timestamp: Long &#x3D; System.currentTimeMillis()      val cityInfo: CityInfo &#x3D; CityRandomOpt.getRandomOpt      val city: String &#x3D; cityInfo.city_name      val area: String &#x3D; cityInfo.area      val adid: Int &#x3D; 1 + random.nextInt(6)      val userid: Int &#x3D; 1 + random.nextInt(6)      &#x2F;&#x2F; 拼接实时数据: 某个时间点 某个地区 某个城市 某个用户 某个广告      array +&#x3D; timestamp + &quot; &quot; + area + &quot; &quot; + city + &quot; &quot; + userid + &quot; &quot; + adid    &#125;    array.toArray  &#125;  def main(args: Array[String]): Unit &#x3D; &#123;    &#x2F;&#x2F; 获取配置文件config.properties中的Kafka配置参数    val config: Properties &#x3D; PropertiesUtil.load(&quot;config.properties&quot;)    val brokers: String &#x3D; config.getProperty(&quot;kafka.broker.list&quot;)    val topic: String &#x3D; config.getProperty(&quot;kafka.topic&quot;)    &#x2F;&#x2F; 创建配置对象    val prop &#x3D; new Properties()    &#x2F;&#x2F; 添加配置    prop.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, brokers)    prop.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;)    prop.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;)    &#x2F;&#x2F; 根据配置创建Kafka生产者    val kafkaProducer: KafkaProducer[String, String] &#x3D; new KafkaProducer[String, String](prop)    while (true) &#123;      &#x2F;&#x2F; 随机产生实时数据并通过Kafka生产者发送到Kafka集群中      for (line &lt;- generateMockData()) &#123;        kafkaProducer.send(new ProducerRecord[String, String](topic, line))        println(line)      &#125;      Thread.sleep(2000)    &#125;  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">package com.atguigu.dataproductionimport scala.collection.mutable.ListBufferimport scala.util.Random&#x2F;** * @author Jinxin Li * @create 2020-11-25 19:23 *&#x2F;&#x2F;&#x2F; value值出现的比例，例如：(男，8) (女:2)case class RanOpt[T](value: T, weight: Int)object RandomOptions &#123;  def apply[T](opts: RanOpt[T]*): RandomOptions[T] &#x3D; &#123;    val randomOptions &#x3D; new RandomOptions[T]()    for (opt &lt;- opts) &#123;      &#x2F;&#x2F; 累积总的权重： 8 + 2      randomOptions.totalWeight +&#x3D; opt.weight      &#x2F;&#x2F; 根据每个元素的自己的权重，向buffer中存储数据。权重越多存储的越多      for (i &lt;- 1 to opt.weight) &#123;        &#x2F;&#x2F; 男 男 男 男 男 男 男 男 女 女        randomOptions.optsBuffer +&#x3D; opt.value      &#125;    &#125;    randomOptions  &#125;  def main(args: Array[String]): Unit &#x3D; &#123;    for (i &lt;- 1 to 10) &#123;      println(RandomOptions(RanOpt(&quot;男&quot;, 8), RanOpt(&quot;女&quot;, 2)).getRandomOpt)    &#125;  &#125;&#125;class RandomOptions[T](opts: RanOpt[T]*) &#123;  var totalWeight &#x3D; 0  var optsBuffer &#x3D; new ListBuffer[T]  def getRandomOpt: T &#x3D; &#123;    &#x2F;&#x2F; 随机选择：0-9    val randomNum: Int &#x3D; new Random().nextInt(totalWeight)    &#x2F;&#x2F; 根据随机数，作为角标取数    optsBuffer(randomNum)  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="4-黑名单业务"><a href="#4-黑名单业务" class="headerlink" title="4.黑名单业务"></a>4.黑名单业务</h2><p>实现实时的动态黑名单机制：将每天对某个广告点击超过30次的用户拉黑。</p><p>注：黑名单保存到MySQL中。</p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">package com.atguigu.appimport java.util.Propertiesimport com.atguigu.handler.BlackListHandlerimport com.atguigu.utils.&#123;MyKafkaUtil, PropertiesUtil&#125;import org.apache.kafka.clients.consumer.ConsumerRecordimport org.apache.spark.SparkConfimport org.apache.spark.streaming.dstream.&#123;DStream, InputDStream&#125;import org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;&#x2F;** * @author Jinxin Li * @create 2020-11-25 20:42 *&#x2F;object RealTimeApp &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    val sparkConf: SparkConf &#x3D; new SparkConf().setAppName(&quot;RealTimeApp &quot;).setMaster(&quot;local[*]&quot;)    &#x2F;&#x2F;2.创建StreamingContext    val ssc &#x3D; new StreamingContext(sparkConf, Seconds(3))&#x2F;&#x2F;    val topic &#x3D; PropertiesUtil.load(&quot;config.perproties&quot;).getProperty(&quot;kafka.topic&quot;)    val properties: Properties &#x3D; PropertiesUtil.load(&quot;config.properties&quot;)    val topic: String &#x3D; properties.getProperty(&quot;kafka.topic&quot;)    &#x2F;&#x2F;从kafka中读取数据    val kafkaDStream: InputDStream[ConsumerRecord[String, String]] &#x3D; MyKafkaUtil.getKafkaStream(topic, ssc)    &#x2F;&#x2F;将从kafka独处的数据转换为样例类对象    val adsLogDStream: DStream[Ads_log] &#x3D; kafkaDStream.map(      record &#x3D;&gt; &#123;        &#x2F;&#x2F;kafka得到的是kv值,注意先取出v值        val line: String &#x3D; record.value()        &#x2F;&#x2F;拿出字段信息        val info: Array[String] &#x3D; line.split(&quot; &quot;)        &#x2F;&#x2F;使用伴生对象的apply方法直接包装对象并返回        Ads_log(info(0).toLong, info(1), info(2), info(3), info(4))      &#125;    )    &#x2F;&#x2F;-----------------------------------------------------------    &#x2F;&#x2F;现在已经万事具备,开始处理需求    &#x2F;&#x2F;需求1 BlackListHandler广告黑名单业务,    &#x2F;&#x2F; 将实现实时的动态黑名单机制：将每天对某个广告点击超过30次的用户拉黑。    val filterAdsLogDStream: DStream[Ads_log] &#x3D; BlackListHandler.filterByBlackList(adsLogDStream)    BlackListHandler.addBlackList(filterAdsLogDStream)    filterAdsLogDStream.cache()    filterAdsLogDStream.count().print()    &#x2F;&#x2F;启动任务    ssc.start()    ssc.awaitTermination()  &#125;&#125;&#x2F;&#x2F; 创建数据的样例类对象&#x2F;&#x2F; 时间 地区 城市 用户id 广告idcase class Ads_log(timestamp: Long, area: String, city: String, userid: String, adid: String)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">package com.atguigu.handlerimport java.sql.Connectionimport java.text.SimpleDateFormatimport java.util.Dateimport com.atguigu.app.Ads_logimport com.atguigu.utils.JDBCUtilimport org.apache.spark.streaming.dstream.DStream&#x2F;** * @author Jinxin Li * @create 2020-11-25 20:39 *&#x2F;object BlackListHandler &#123;  &#x2F;&#x2F;实现实时的动态黑名单机制：将每天对某个广告点击超过30次的用户拉黑。  &#x2F;&#x2F;注：黑名单保存到MySQL中。  private val sdf &#x3D; new SimpleDateFormat(&quot;yyyy-MM-dd&quot;)  def addBlackList(filterAdsLogDStream: DStream[Ads_log]) &#x3D; &#123;    val dtAreaCityUserAdDStream: DStream[((String, String, String), Long)] &#x3D; filterAdsLogDStream.map &#123;      case adsLog &#x3D;&gt; &#123;        val dt: String &#x3D; sdf.format(new Date(adsLog.timestamp))        ((dt, adsLog.userid, adsLog.adid), 1L)      &#125;    &#125;.reduceByKey(_ + _)    &#x2F;&#x2F;写出    dtAreaCityUserAdDStream.foreachRDD(      &#x2F;&#x2F;先把流中的foreachRDD写出,然后分流写出      rdd&#x3D;&gt;rdd.foreachPartition(        iter&#x3D;&gt;&#123;          val connection: Connection &#x3D; JDBCUtil.getConnection          iter.foreach&#123;            case ((dt,userid,adid),count)&#x3D;&gt;              JDBCUtil.executeUpdate(                connection,                &quot;&quot;&quot;                  |insert into user_ad_count (dt,userid,adid,count)                  |values (?,?,?,?)                  |on duplicate key                  |update count &#x3D; count + ?                  |&quot;&quot;&quot;.stripMargin,Array(dt,userid,adid,count,count)              )              val ct: Long &#x3D; JDBCUtil.getDataFromMysql(                connection,                &quot;&quot;&quot;                  |select count from user_ad_count where dt&#x3D;? and userid&#x3D;? and adid&#x3D;?                  |&quot;&quot;&quot;.stripMargin,Array(dt,userid,adid)              )              &#x2F;&#x2F;如果大于30就加入黑名单              if (ct&gt;&#x3D;30)&#123;                JDBCUtil.executeUpdate(                  connection,                  &quot;&quot;&quot;                    |insert into black_list (userid)                    |values (?)                    |on duplicate key                    |update userid&#x3D;?                    |&quot;&quot;&quot;.stripMargin,Array(userid,userid)                )              &#125;          &#125;          connection.close()        &#125;      )    )  &#125;  def filterByBlackList(adsLogDStream: DStream[Ads_log]): DStream[Ads_log] &#x3D; &#123;    adsLogDStream.filter(      adsLog&#x3D;&gt;&#123;        &#x2F;&#x2F;获取连接        val connection: Connection &#x3D; JDBCUtil.getConnection        val bool: Boolean &#x3D; JDBCUtil.isExist(          connection,          &quot;&quot;&quot;            |select * from black_list where userid&#x3D;?            |&quot;&quot;&quot;.stripMargin, Array(adsLog.userid)        )        connection.close()        !bool      &#125;    )  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="5-统计点击总流量"><a href="#5-统计点击总流量" class="headerlink" title="5.统计点击总流量"></a>5.统计点击总流量</h2><p>描述：实时统计每天各地区各城市各广告的点击总流量，并将其存入MySQL。</p><p>注意connection与connection.close要同时写</p><p><img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_21_01/%E6%8A%A5%E9%94%99%E4%BF%A1%E6%81%AF1.png" alt="connection报错"></p><p><img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_21_01/%E6%8A%A5%E9%94%99%E4%BF%A1%E6%81%AF2.png" alt="报错信息"></p><p><img src="SparkStreaming_Case_%E6%80%BB%E7%BB%93.assets/1606325292658.png" alt="1606325292658"></p><p><img src="SparkStreaming_Case_%E6%80%BB%E7%BB%93.assets/1606325343070.png" alt="1606325343070"></p><p>功能:DateAreaCityAdCountHandler</p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">package com.atguigu.handlerimport java.sql.Connectionimport java.text.SimpleDateFormatimport java.util.Dateimport com.atguigu.app.Ads_logimport com.atguigu.utils.JDBCUtilimport org.apache.spark.streaming.dstream.DStream&#x2F;** * @author Jinxin Li * @create 2020-11-25 23:58 * 统计每天各大区各个城市广告点击总数并保存至MySQL中 *&#x2F;object DateAreaCityAdCountHandler &#123;  private val sdf &#x3D; new SimpleDateFormat(&quot;yyyy-MM-dd&quot;)  def saveDateAreaCityAdCountToMysql(filterAdsLogDStream: DStream[Ads_log]) &#x3D; &#123;    val dateAreaCityAdToCount&#x3D; filterAdsLogDStream.map(ads_log &#x3D;&gt; &#123;      &#x2F;&#x2F;a.格式化为日期字符串      val dt: String &#x3D; sdf.format(new Date(ads_log.timestamp))      &#x2F;&#x2F;b.组合,返回      ((dt, ads_log.area, ads_log.city, ads_log.adid), 1L)    &#125;).reduceByKey(_ + _)    dateAreaCityAdToCount.foreachRDD(      &#x2F;&#x2F;分配处理,是指遥控在executor进行执行      rdd&#x3D;&gt;&#123;        rdd.foreachPartition(          iter&#x3D;&gt;&#123;            val connection: Connection &#x3D; JDBCUtil.getConnection            iter.foreach &#123;              case ((dt, area, city, adid), ct) &#x3D;&gt;                JDBCUtil.executeUpdate(                  connection,                  &quot;&quot;&quot;                    |INSERT INTO area_city_ad_count (dt)                    |VALUES(?)                    &quot;&quot;&quot;.stripMargin,                  Array(dt)                )            &#125;            connection.close()          &#125;)      &#125;)  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="6-统计两分钟内广告点击量"><a href="#6-统计两分钟内广告点击量" class="headerlink" title="6.统计两分钟内广告点击量"></a>6.统计两分钟内广告点击量</h2><p>LastHourAdCountHandler</p><p>说明：实际测试时，为了节省时间，统计的是2分钟内广告点击量</p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">package com.atguigu.handlerimport java.text.SimpleDateFormatimport java.util.Dateimport com.atguigu.app.Ads_logimport org.apache.spark.streaming.Minutesimport org.apache.spark.streaming.dstream.DStreamobject LastHourAdCountHandler &#123;  &#x2F;&#x2F;时间格式化对象  private val sdf: SimpleDateFormat &#x3D; new SimpleDateFormat(&quot;HH:mm&quot;)  &#x2F;&#x2F; 过滤后的数据集，统计最近一小时(2分钟)广告分时点击总数  def getAdHourMintToCount(filterAdsLogDStream: DStream[Ads_log]): DStream[(String, List[(String, Long)])] &#x3D; &#123;    &#x2F;&#x2F;1.开窗 &#x3D;&gt; 时间间隔为1个小时 window()    val windowAdsLogDStream: DStream[Ads_log] &#x3D; filterAdsLogDStream.window(Minutes(2))    &#x2F;&#x2F;2.转换数据结构 ads_log &#x3D;&gt;((adid,hm),1L) map()    val adHmToOneDStream: DStream[((String, String), Long)] &#x3D; windowAdsLogDStream.map(adsLog &#x3D;&gt; &#123;      val hm: String &#x3D; sdf.format(new Date(adsLog.timestamp))      ((adsLog.adid, hm), 1L)    &#125;)    &#x2F;&#x2F;3.统计总数 ((adid,hm),1L)&#x3D;&gt;((adid,hm),sum) reduceBykey(_+_)    val adHmToCountDStream: DStream[((String, String), Long)] &#x3D; adHmToOneDStream.reduceByKey(_ + _)    &#x2F;&#x2F;4.转换数据结构 ((adid,hm),sum)&#x3D;&gt;(adid,(hm,sum)) map()    val adToHmCountDStream: DStream[(String, (String, Long))] &#x3D; adHmToCountDStream.map &#123; case ((adid, hm), count) &#x3D;&gt;      (adid, (hm, count))    &#125;    &#x2F;&#x2F;5.按照adid分组 (adid,(hm,sum))&#x3D;&gt;(adid,Iter[(hm,sum),...]) groupByKey    adToHmCountDStream      .groupByKey()      .mapValues(iter &#x3D;&gt; iter.toList.sortWith(_._1 &lt; _._1))  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> sparkStreaming </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark_SparkStreaming</title>
      <link href="2021/01/04/Spark_SparkStreaming/"/>
      <url>2021/01/04/Spark_SparkStreaming/</url>
      
        <content type="html"><![CDATA[<p>云盘：<a href="mailto:&#109;&#x69;&#97;&#111;&#x63;&#x68;&#117;&#x61;&#x6e;&#104;&#97;&#x69;&#x40;&#49;&#54;&#x33;&#x2e;&#x63;&#111;&#109;">&#109;&#x69;&#97;&#111;&#x63;&#x68;&#117;&#x61;&#x6e;&#104;&#97;&#x69;&#x40;&#49;&#54;&#x33;&#x2e;&#x63;&#111;&#109;</a><br>暗号：大海哥最帅</p><hr><h1 id="一-概述"><a href="#一-概述" class="headerlink" title="一 概述"></a>一 概述</h1><h2 id="1-Spark架构"><a href="#1-Spark架构" class="headerlink" title="1.Spark架构"></a>1.Spark架构</h2><p><img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_21_01/Spark%E7%BB%93%E6%9E%84.png" alt="SparkStreaming整体架构图"></p><h2 id="2-DStream是什么"><a href="#2-DStream是什么" class="headerlink" title="2.DStream是什么?"></a>2.DStream是什么?</h2><p>每个时间区间收到的数据作为RDD存在</p><p>实时的封装</p><h2 id="3-背压机制"><a href="#3-背压机制" class="headerlink" title="3.背压机制"></a>3.背压机制</h2><p>可以自动调节receiver的接受速率</p><p>通过属性“spark.streaming.backpressure.enabled”来控制是否启用backpressure机制，默认值false，即不启用。</p><h1 id="二-DSteam入门"><a href="#二-DSteam入门" class="headerlink" title="二 DSteam入门"></a>二 DSteam入门</h1><h2 id="1-wordCount"><a href="#1-wordCount" class="headerlink" title="1.wordCount"></a>1.wordCount</h2><p>需求：使用netcat工具向9999端口不断的发送数据，通过SparkStreaming读取端口数据并统计不同单词出现的次数</p><h3 id="1-1使用netcat作为源头wordcount"><a href="#1-1使用netcat作为源头wordcount" class="headerlink" title="1.1使用netcat作为源头wordcount"></a>1.1使用netcat作为源头wordcount</h3><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">object WordCount &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    val sparkConf: SparkConf &#x3D; new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;sparkStreaming&quot;)    &#x2F;&#x2F;初始化配置信息    val ssc &#x3D; new StreamingContext(sparkConf, Seconds(3))    &#x2F;&#x2F;接收数据    val line: ReceiverInputDStream[String] &#x3D; ssc.socketTextStream(&quot;hadoop102&quot;, 9999)    &#x2F;&#x2F;处理    val wordToSumStream: DStream[(String, Int)] &#x3D; line.flatMap(_.split(&quot; &quot;)).map((_, 1)).reduceByKey(_ + _)    wordToSumStream.print()    ssc.start()    ssc.awaitTermination()  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="1-2使用队列源计算wordcount"><a href="#1-2使用队列源计算wordcount" class="headerlink" title="1.2使用队列源计算wordcount"></a>1.2使用队列源计算wordcount</h3><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">object queuePractice &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    val sparkConf: SparkConf &#x3D; new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;opt&quot;)    val ssc &#x3D; new StreamingContext(sparkConf, Seconds(3))    &#x2F;&#x2F;创建RDD队列    val rddQueue &#x3D; new mutable.Queue[RDD[Int]]()    &#x2F;&#x2F;创建queueInputDsteam    val value: InputDStream[Int] &#x3D; ssc.queueStream(rddQueue, oneAtATime &#x3D; false)    val result: DStream[Int] &#x3D; value.reduce(_ + _)    &#x2F;&#x2F;打印结果    result.print()    ssc.start()    for (i &lt;- 1 to 5)&#123;      rddQueue +&#x3D; ssc.sparkContext.makeRDD(1 to 5)      Thread.sleep((2000))    &#125;    ssc.awaitTermination()  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="2-自定义数据源"><a href="#2-自定义数据源" class="headerlink" title="2.自定义数据源"></a>2.自定义数据源</h2><h3 id="2-1自定义数据源receiver"><a href="#2-1自定义数据源receiver" class="headerlink" title="2.1自定义数据源receiver"></a>2.1自定义数据源receiver</h3><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">class defReceiver(host:String,port:Int) extends Receiver[String](StorageLevel.MEMORY_ONLY)&#123;  override def onStart(): Unit &#x3D; &#123;    new Thread(&quot;Socket Receiver&quot;)&#123;      override def run(): Unit &#x3D; &#123;receive()&#125;    &#125;.start()  &#125;  def receive(): Unit &#x3D; &#123;    val socket &#x3D; new Socket(host, port)    &#x2F;&#x2F;创建一个bufferReader    val reader &#x3D; new BufferedReader(new InputStreamReader(socket.getInputStream, StandardCharsets.UTF_8))    &#x2F;&#x2F;读取数据    var input: String &#x3D; reader.readLine()    &#x2F;&#x2F;当receiver没有关闭并且输入数据不为空,则循环发送数据给spark    while(!isStopped()&amp;&amp;input !&#x3D;null)&#123;      store(input)      input&#x3D; reader.readLine()    &#125;    &#x2F;&#x2F;如果循环结束,则关闭资源    reader.close()    socket.close()    &#x2F;&#x2F;重启    restart(&quot;restart&quot;)  &#125;  override def onStop(): Unit &#x3D; &#123;&#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="2-2自定数据源的使用"><a href="#2-2自定数据源的使用" class="headerlink" title="2.2自定数据源的使用"></a>2.2自定数据源的使用</h3><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">object defSource &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    val sparkConf: SparkConf &#x3D; new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;opt&quot;)    val ssc &#x3D; new StreamingContext(sparkConf, Seconds(3))    &#x2F;&#x2F;创建自动receiver的Streaming    val lineDStream: ReceiverInputDStream[String] &#x3D; ssc.receiverStream(new defReceiver(&quot;hadoop102&quot;, 9999))    &#x2F;&#x2F;将每一行数据做切分,形成一个个单词    val result: DStream[(String, Int)] &#x3D; lineDStream.flatMap(_.split(&quot; &quot;)).map((_, 1)).reduceByKey(_ + _)    result.print()    &#x2F;&#x2F;启动并且阻塞    ssc.start()    ssc.awaitTermination()  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="3-kafka数据源"><a href="#3-kafka数据源" class="headerlink" title="3.kafka数据源"></a>3.kafka数据源</h2><p>Kafka数据源在SparkStreaming中存在两种API,一种是ReceiverAPI,另一种是DirectAPI</p><p>ReceiverAPI已经弃用</p><p><strong>DirectAPI</strong>:是由计算的Executor来主动消费Kafka的数据,速度由自身控制</p><h3 id="3-1kafka的命令行代码"><a href="#3-1kafka的命令行代码" class="headerlink" title="3.1kafka的命令行代码"></a>3.1kafka的命令行代码</h3><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">#查看消费topicbin&#x2F;kafka-topics.sh --zookeeper hadoop102:2181&#x2F;kafka -list#创建kafka的topicbin&#x2F;kafka-topics.sh --zookeeper hadoop102:2181&#x2F;kafka  --create --replication-factor 1 --partitions 2 --topic testTopic#查看Topic详情bin&#x2F;kafka-topics.sh --zookeeper hadoop102:2181&#x2F;kafka \--describe --topic testTopic#创建生产者bin&#x2F;kafka-console-producer.sh \--broker-list hadoop102:9092 --topic testTopic#创建消费者[atguigu@hadoop102 kafka]$ bin&#x2F;kafka-console-consumer.sh \--bootstrap-server hadoop102:9092 --from-beginning --topic testTopic#查看_consumer_offsets主题中存储的offsetbin&#x2F;kafka-consumer-groups.sh --bootstrap-server hadoop102:9092 --describe --group atguiguGroup#如何查看消费者组的位置<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="3-2使用kafka源来流数据wordcount处理"><a href="#3-2使用kafka源来流数据wordcount处理" class="headerlink" title="3.2使用kafka源来流数据wordcount处理"></a>3.2使用kafka源来流数据wordcount处理</h3><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">object SparkStreaming_DirectAuto &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    val sparkConf: SparkConf &#x3D; new SparkConf().setAppName(&quot;KakfaSource&quot;).setMaster(&quot;local[*]&quot;)    val scc &#x3D; new StreamingContext(sparkConf, Seconds(3))    &#x2F;&#x2F;配置参数,为map    &#x2F;&#x2F;3.定义Kafka参数：kafka集群地址、消费者组名称、key序列化、value序列化    val kafkaPara: Map[String, Object] &#x3D; Map[String, Object](      ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG -&gt; &quot;hadoop102:9092,hadoop103:9092,hadoop104:9092&quot;,      ConsumerConfig.GROUP_ID_CONFIG -&gt; &quot;atguiguGroup&quot;,      ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG -&gt; &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;,      ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG -&gt; classOf[StringDeserializer]    )    &#x2F;&#x2F;使用DirectAPI创建DStream    val kafkaToDStream: InputDStream[ConsumerRecord[String, String]] &#x3D; KafkaUtils.createDirectStream(scc,      LocationStrategies.PreferConsistent,      ConsumerStrategies.Subscribe[String, String](Set(&quot;testTopic&quot;), kafkaPara))    &#x2F;&#x2F;处理数据    kafkaToDStream.map(record&#x3D;&gt;record.value())      .flatMap(_.split(&quot; &quot;))      .map((_,1))      .reduceByKey(_+_)      .print()    &#x2F;&#x2F;关闭并堵塞    scc.start()    scc.awaitTermination()  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="三-DStream转换"><a href="#三-DStream转换" class="headerlink" title="三 DStream转换"></a>三 DStream转换</h1><p>DStream上的操作与RDD的类似，分为转换和输出两种，此外转换操作中还有一些比较特殊的原语，如：updateStateByKey()、transform()以及各种Window相关的原语。</p><h2 id="3-1无状态转换"><a href="#3-1无状态转换" class="headerlink" title="3.1无状态转换"></a>3.1无状态转换</h2><p>transfrom()是在Driver端开始执行的</p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">&#x2F;&#x2F;DStream其实就是内部封装了一个RDD,因此object SparkStreaming_Transform &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    &#x2F;&#x2F;初始化配置    val sparkConf: SparkConf &#x3D; new SparkConf().setAppName(&quot;KakfaSource&quot;).setMaster(&quot;local[*]&quot;)    val scc &#x3D; new StreamingContext(sparkConf, Seconds(3))    &#x2F;&#x2F;接受参数    val rDStream: ReceiverInputDStream[String] &#x3D; scc.socketTextStream(&quot;hadoop102&quot;, 9999)    &#x2F;&#x2F;使用transform可以将DStream转换成rdd    println(&quot;主程序执行&quot;+Thread.currentThread().getName)&#x2F;&#x2F;JobGenerator Driver    rDStream.transform(rdd&#x3D;&gt;&#123;      println(&quot;transform执行位置&quot;+Thread.currentThread().getName)      rdd.flatMap(line&#x3D;&gt;line.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_)&#125;).print()    &#x2F;&#x2F;关闭并堵塞    scc.start()    scc.awaitTermination()  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="3-2有状态转换"><a href="#3-2有状态转换" class="headerlink" title="3.2有状态转换"></a>3.2有状态转换</h2><p>有状态转化操作：计算当前批次RDD时，需要用到历史RDD的数据。</p><h3 id="1-UpdateStateByKey"><a href="#1-UpdateStateByKey" class="headerlink" title="(1)UpdateStateByKey()"></a>(1)UpdateStateByKey()</h3><p>参数中需要传递一个函数,在函数内部根据需求对==当前数据==与==历史数据==进行整合</p><p>需求:使用UpdateStateByKey计算WordCount</p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">&#x2F;** * @author Jinxin Li * @create 2020-11-24 18:24 * 通过累积历史状态做的实时的wordcount *&#x2F;object sparkStreaming_updateStateByKey &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    &#x2F;&#x2F;这里只会相加key相同的数值,因为是reduce,key相同进来,只要设定value的方法即可    &#x2F;&#x2F; 定义更新状态方法，参数seq为当前批次单词次数，state为以往批次单词次数    val updateFunc &#x3D; (seq:Seq[Int],state:Option[Int])&#x3D;&gt;&#123;      &#x2F;&#x2F;当前批次      val currentCount &#x3D; seq.sum      &#x2F;&#x2F;历史批次      val previousCount &#x3D;  state.getOrElse(0)      &#x2F;&#x2F;总的数据累加      Some(currentCount+previousCount)    &#125;    val sparkConf: SparkConf &#x3D; new SparkConf().setAppName(&quot;KakfaSource&quot;).setMaster(&quot;local[*]&quot;)    val scc &#x3D; new StreamingContext(sparkConf, Seconds(3))    scc.checkpoint(&quot;.&#x2F;SparkStreaming&#x2F;ck&quot;)&#x2F;&#x2F;设定存储点,默认的计算与历史相关的都要设定检查点    val reDStream: ReceiverInputDStream[String] &#x3D; scc.socketTextStream(&quot;hadoop102&quot;, 9999)    val KvDStream: DStream[(String, Int)] &#x3D; reDStream.flatMap(_.split(&quot; &quot;)).map((_, 1))    KvDStream.updateStateByKey[Int](      updateFunc,      new HashPartitioner(2)).print()    &#x2F;&#x2F;关闭并堵塞    scc.start()    scc.awaitTermination()  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>缺点**</strong></p><p>这种UpdateStateByKey在企业中几乎不用,因为存在很多缺点</p><ol><li>使用checkPoint检查点存储记录历史会产生大量的小文件</li></ol><p><img src="../../../MY_POSTS/images/1606214937943.png" alt="1606214937943"></p><ol start="2"><li>当前线程关闭后,下次开启重新开始累积</li><li>即使重新开始累积,但是checkpoint会记录最后一次时间戳，再次启动的时候会把间隔时间的周期再执行一次,容易导致启动卡死</li></ol><h3 id="2-WindowOperations"><a href="#2-WindowOperations" class="headerlink" title="(2)WindowOperations"></a>(2)WindowOperations</h3><p>Window Operations可以设置窗口的大小和滑动窗口的间隔来动态的获取当前Streaming的允许状态。所有基于窗口的操作都需要两个参数，分别为窗口时长以及滑动步长。</p><p><img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_21_01/%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0%E6%93%8D%E4%BD%9C.png" alt="窗口函数操作"></p><p><strong>需求</strong></p><p>基本语法：window(windowLength, slideInterval): 基于对源DStream窗化的批次进行计算返回一个新的DStream。</p><p>需求：统计WordCount:3秒一个批次，窗口12秒，滑步6秒。</p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">object SparkStreaming_window &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    &#x2F;&#x2F;这里只会相加key相同的数值,因为是reduce,key相同进来,只要设定value的方法即可    &#x2F;&#x2F; 定义更新状态方法，参数seq为当前批次单词次数，state为以往批次单词次数    val sparkConf: SparkConf &#x3D; new SparkConf().setAppName(&quot;KakfaSource&quot;).setMaster(&quot;local[*]&quot;)    val scc &#x3D; new StreamingContext(sparkConf, Seconds(3))    scc.checkpoint(&quot;.&#x2F;SparkStreaming&#x2F;ck&quot;)&#x2F;&#x2F;设定存储点,默认的计算与历史相关的都要设定检查点    val reDStream: ReceiverInputDStream[String] &#x3D; scc.socketTextStream(&quot;hadoop102&quot;, 9999)    val lineToTuples: DStream[(String, Int)] &#x3D; reDStream.flatMap(_.split(&quot; &quot;)).map((_, 1))    &#x2F;&#x2F;设定窗口,3秒一个批次,窗口持续12秒,步长6秒    val windowDStream: DStream[(String, Int)] &#x3D; lineToTuples.window(Seconds(12), Seconds(6))    windowDStream.reduceByKey(_+_).print()    &#x2F;&#x2F;关闭并堵塞    scc.start()    scc.awaitTermination()  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_21_01/%E7%AA%97%E5%8F%A3%E8%AE%A1%E7%AE%97%E6%B5%81%E7%A8%8B.png" alt="窗口计算流程图"></p><h3 id="3-reduceByKeyAndWindow"><a href="#3-reduceByKeyAndWindow" class="headerlink" title="(3)reduceByKeyAndWindow"></a>(3)reduceByKeyAndWindow</h3><p>reduceByKeyAndWindow(func, windowLength, slideInterval,[numTasks])：</p><p>当在一个(K,V)对的DStream上调用此函数，会返回一个新(K,V)对的DStream，此处通过对滑动窗口中批次数据使用reduce函数来整合每个key的value值。</p><p>这个是window程序的简化版</p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">object SparkStreaming_reduceByKeyAndWindow &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    &#x2F;&#x2F;初始化配置    val sparkConf: SparkConf &#x3D; new SparkConf().setAppName(&quot;KakfaSource&quot;).setMaster(&quot;local[*]&quot;)    val scc &#x3D; new StreamingContext(sparkConf, Seconds(3))    &#x2F;&#x2F;接受参数    val line: ReceiverInputDStream[String] &#x3D; scc.socketTextStream(&quot;hadoop102&quot;, 9999)    val tuplesDStream: DStream[(String, Int)] &#x3D; line.flatMap(_.split(&quot; &quot;)).map((_, 1))    tuplesDStream.reduceByKeyAndWindow(      (a:Int,b:Int)&#x3D;&gt;a+b,&#x2F;&#x2F;这里要表明类型      Seconds(6),      Seconds(12)    ).print()    scc.start()    scc.awaitTermination()  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="4-reduceByKeyAndWindow（反向Reduce）"><a href="#4-reduceByKeyAndWindow（反向Reduce）" class="headerlink" title="(4)reduceByKeyAndWindow（反向Reduce）"></a>(4)reduceByKeyAndWindow（反向Reduce）</h3><p>reduceByKeyAndWindow(func, invFunc, windowLength, slideInterval,[numTasks])</p><p>这个是上一个函数的升级版</p><p>通过一个减法,减少统计窗口之间的重复部分</p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">object SparkStreaming_reduceByKeyAndWindow_reduce &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    &#x2F;&#x2F;初始化配置    val sparkConf: SparkConf &#x3D; new SparkConf().setAppName(&quot;KakfaSource&quot;).setMaster(&quot;local[*]&quot;)    val scc &#x3D; new StreamingContext(sparkConf, Seconds(3))    &#x2F;&#x2F;接受参数    scc.checkpoint(&quot;.&#x2F;ck&quot;)    val line: ReceiverInputDStream[String] &#x3D; scc.socketTextStream(&quot;hadoop102&quot;, 9999)    val tuplesDStream: DStream[(String, Int)] &#x3D; line.flatMap(_.split(&quot; &quot;)).map((_, 1))    tuplesDStream.reduceByKeyAndWindow(      (a:Int,b:Int)&#x3D;&gt;a+b,&#x2F;&#x2F;这里要表明类型      (x:Int,y:Int)&#x3D;&gt;x-y,      Seconds(12),&#x2F;&#x2F;这个是窗口间隔      Seconds(6),&#x2F;&#x2F;这个是滑动距离      new HashPartitioner(2),      (x:(String, Int)) &#x3D;&gt; x._2 &gt; 0&#x2F;&#x2F;过滤小于0值    ).print()    scc.start()    scc.awaitTermination()  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="3-3其他操作"><a href="#3-3其他操作" class="headerlink" title="3.3其他操作"></a>3.3其他操作</h2><p>（1）countByWindow(windowLength, slideInterval): 返回一个滑动窗口计数流中的元素个数；</p><p>（2）reduceByWindow(func, windowLength, slideInterval): 通过使用自定义函数整合滑动区间流元素来创建一个新的单元素流；</p><h1 id="五-DSteam输出"><a href="#五-DSteam输出" class="headerlink" title="五 DSteam输出"></a>五 DSteam输出</h1><p>企业中常用以下方法进行DSteam输出</p><p>print()：在运行流程序的驱动结点上打印DStream中每一批次数据的最开始10个元素。这用于开发和调试。</p><p>foreachRDD(func)：这是最通用的输出操作，函数func用于产生DStream的每一个RDD。其中参数传入的函数func应该实现将每一个RDD中数据推送到外部系统，如将RDD存入文件或者写入数据库。</p><p>在企业开发中通常采用foreachRDD()，它用来对DStream中的RDD进行任意计算。这和transform()有些类似，都可以让我们访问任意RDD。在foreachRDD()中，可以重用我们在Spark中实现的所有行动操作。比如，常见的用例之一是把数据写到如MySQL的外部数据库中</p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">object SparkStreaming_output &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    &#x2F;&#x2F;初始化配置    val sparkConf: SparkConf &#x3D; new SparkConf().setAppName(&quot;KakfaSource&quot;).setMaster(&quot;local[*]&quot;)    val scc &#x3D; new StreamingContext(sparkConf, Seconds(3))    &#x2F;&#x2F;接受参数    scc.checkpoint(&quot;.&#x2F;ck&quot;)    val line: ReceiverInputDStream[String] &#x3D; scc.socketTextStream(&quot;hadoop102&quot;, 9999)    val tuplesDStream: DStream[(String, Int)] &#x3D; line.flatMap(_.split(&quot; &quot;)).map((_, 1))    &#x2F;&#x2F;tuplesDStream.foreachRDD(rdd&#x3D;&gt;rdd.collect().foreach(println))    &#x2F;&#x2F;常规这样输出是一条一条的输出,但是常规不适合这样操作    tuplesDStream.foreachRDD(rdd&#x3D;&gt;rdd.foreachPartition &#123;      println(Thread.currentThread().getName&#x2F;&#x2F;需要注意的是,转换输出也是在Driver端执行的      item &#x3D;&gt; item.foreach(println)    &#125;)    scc.start()    scc.awaitTermination()  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>（1）连接不能写在Driver层面（序列化）</p><p>（2）如果写在foreach则每个RDD中的每一条数据都创建，得不偿失；</p><p>（3）增加foreachPartition，在分区创建（获取）。还是在Driver端进行?</p><h1 id="六-优雅的关闭"><a href="#六-优雅的关闭" class="headerlink" title="六 优雅的关闭"></a>六 优雅的关闭</h1><p>使用外部文件系统来控制内部程序关闭</p><p>使用object</p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">object close2 &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    val properties &#x3D; new Properties()    val config &#x3D; PropertiesUtil.load(&quot;config.properties&quot;)    val sparkConf: SparkConf &#x3D; new SparkConf().setAppName(&quot;sparkStreaming&quot;).setMaster(&quot;local[*]&quot;)      .set(&quot;spark.streaming.stopGracefullyOnShutdown&quot;, config.getProperty(&quot;spark.streaming.stopGracefullyOnShutdown&quot;))    val ssc &#x3D; new StreamingContext(sparkConf, Seconds(3))    val lineDStream: ReceiverInputDStream[String] &#x3D; ssc.socketTextStream(&quot;hadoop102&quot;, 9999)    lineDStream.flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_).print()    new Thread(new StopMonitor(ssc)).start()    &#x2F;&#x2F;关闭    ssc.start()    ssc.awaitTermination()  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>关闭object</p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">class StopMonitor(ssc: StreamingContext) extends Runnable&#123;  override def run(): Unit &#x3D; &#123;    &#x2F;&#x2F;获得文件系统    val fileSystem: FileSystem &#x3D; FileSystem.get(new URI(&quot;hdfs:&#x2F;&#x2F;hadoop102:8020&quot;), new Configuration(), &quot;atguigu&quot;)    &#x2F;&#x2F;循环监测    while (true)&#123;      val bool: Boolean &#x3D; fileSystem.exists(new Path(&quot;hdfs:&#x2F;&#x2F;hadoop102:8020&#x2F;stopSpark&quot;))      Thread.sleep(5000)      &#x2F;&#x2F;判断      if (bool)&#123;        val state: StreamingContextState &#x3D; ssc.getState()        if (state&#x3D;&#x3D;StreamingContextState.ACTIVE)&#123;          ssc.stop(stopSparkContext &#x3D; true,stopGracefully &#x3D; true)          System.exit(0)        &#125;      &#125;    &#125;  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> sparkstreaming </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark_SparkCore</title>
      <link href="2021/01/03/Spark_SparkCore/"/>
      <url>2021/01/03/Spark_SparkCore/</url>
      
        <content type="html"><![CDATA[<h1 id="0x1-spark概述"><a href="#0x1-spark概述" class="headerlink" title="0x1 spark概述"></a>0x1 spark概述</h1><h2 id="1-历史"><a href="#1-历史" class="headerlink" title="1.历史"></a>1.历史</h2><p>在之前的学习中，Hadoop的MapReduce是大家广为熟知的计算框架，那为什么咱们还要学习新的计算框架Spark呢，这里就不得不提到Spark和Hadoop的关系。</p><p>首先从时间节点上来看:</p><p>Hadoop</p><ol><li>2006年1月，Doug Cutting加入Yahoo，领导Hadoop的开发</li><li>2008年1月，Hadoop成为Apache顶级项目</li><li>2011年1.0正式发布</li><li>2012年3月稳定版发布</li><li>2013年10月发布2.X (Yarn)版本</li></ol><p>Spark</p><ol><li>2009年，Spark诞生于伯克利大学的AMPLab实验室</li><li>2010年，伯克利大学正式开源了Spark项目</li><li>2013年6月，Spark成为了Apache基金会下的项目</li><li>2014年2月，Spark以飞快的速度成为了Apache的顶级项目</li><li>2015年至今，Spark变得愈发火爆，大量的国内公司开始重点部署或者使用Spark</li></ol><h2 id="2-Spark核心模块"><a href="#2-Spark核心模块" class="headerlink" title="2.Spark核心模块"></a>2.Spark核心模块</h2><p><img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_21_01/Spark%E6%A0%B8%E5%BF%83%E6%A8%A1%E5%9D%97.png" alt="spark核心模块"></p><p><strong>Spark Core</strong></p><p>Spark Core中提供了Spark最基础与最核心的功能，Spark其他的功能如：Spark SQL，Spark Streaming，GraphX, MLlib都是在Spark Core的基础上进行扩展的</p><p><strong>Spark SQL</strong></p><p>Spark SQL是Spark用来操作结构化数据的组件。通过Spark SQL，用户可以使用SQL或者Apache Hive版本的SQL方言（HQL）来查询数据。</p><p><strong>Spark Streaming</strong></p><p>Spark Streaming是Spark平台上针对实时数据进行流式计算的组件，提供了丰富的处理数据流的API。</p><p><strong>Spark MLlib</strong></p><p>MLlib是Spark提供的一个机器学习算法库。MLlib不仅提供了模型评估、数据导入等额外的功能，还提供了一些更底层的机器学习原语。</p><p><strong>Spark GraphX</strong></p><p>GraphX是Spark面向图计算提供的框架与算法库。</p><h2 id="3-入门wordCount"><a href="#3-入门wordCount" class="headerlink" title="3.入门wordCount"></a>3.入门wordCount</h2><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql">object wordCountTest &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    &#x2F;&#x2F;spark标准获取流程    val sparkConf: SparkConf &#x3D; new SparkConf().setAppName(&quot;wordCount&quot;).setMaster(&quot;local[1]&quot;)    val sc &#x3D; new SparkContext(sparkConf)    &#x2F;&#x2F;创建RDD,使用textFile的方式创建RDD    val line: RDD[String] &#x3D; sc.textFile(&quot;.&#x2F;word.txt&quot;, 1)    val result: RDD[(String, Int)] &#x3D; line.flatMap(_.split(&quot; &quot;)).map((_, 1)).reduceByKey(_ + _)    &#x2F;&#x2F;打印    result.collect().foreach(println)    sc.close()  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="4-spark运行模式"><a href="#4-spark运行模式" class="headerlink" title="4.spark运行模式"></a>4.spark运行模式</h2><table><thead><tr><th>Spark运行环境</th><th>用法</th></tr></thead><tbody><tr><td>Local模式</td><td>测试</td></tr><tr><td>Standalone模式</td><td>独立部署Matser-Worker<br/>自身提供计算资源,降低了耦合性</td></tr><tr><td>Yarn模式</td><td>提供集群模式</td></tr><tr><td>K8S模式</td><td></td></tr><tr><td>Mesos模式</td><td>Twitter</td></tr><tr><td>Windows模式</td><td></td></tr></tbody></table><h2 id="5-spark端口号"><a href="#5-spark端口号" class="headerlink" title="5.spark端口号"></a>5.spark端口号</h2><table><thead><tr><th>Spark端口号</th><th>详解</th><th>Hadoop</th></tr></thead><tbody><tr><td>内部通信:==7077==</td><td>Spark Master内部通信服务端口号</td><td>8020/9000</td></tr><tr><td>Master资源监控:8080/改==8989==</td><td>Standalone模式下，Spark Master Web端口号</td><td>9870</td></tr><tr><td>Spark-Shell监控:==4040==</td><td>Spark查看当前Spark-shell运行任务情况端口号</td><td></td></tr><tr><td>Spark使用HDFS端口:==8020==</td><td></td><td></td></tr><tr><td>历史服务器UI端口:==18080==</td><td>Spark历史服务器端口号</td><td>19888</td></tr><tr><td>Yarn:==8088==</td><td>Hadoop YARN任务运行情况查看端口号</td><td>8088</td></tr></tbody></table><h2 id="6-Spark运行组件"><a href="#6-Spark运行组件" class="headerlink" title="6.Spark运行组件"></a>6.Spark运行组件</h2><p>Spark框架的核心是一个计算引擎，整体来说，它采用了标准 master-slave 的结构。</p><p><img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_21_01/Spark%E8%BF%90%E8%A1%8C%E7%BB%84%E4%BB%B6.png" alt="Spark运行组件"></p><h3 id="6-1Spark-Executor"><a href="#6-1Spark-Executor" class="headerlink" title="6.1Spark Executor"></a>6.1Spark Executor</h3><p>集群中运行在==工作节点（Worker）中的一个JVM进程==，是整个集群中的专门用于计算的节点。在提交应用中，可以提供参数指定计算节点的个数，以及对应的资源。这里的资源一般指的是工作节点Executor的内存大小和使用的虚拟CPU核（Core）数量。</p><table><thead><tr><th>名称</th><th>说明</th></tr></thead><tbody><tr><td>–num-executors</td><td>配置Executor的数量</td></tr><tr><td>–executor-memory</td><td>配置每个Executor的内存大小</td></tr><tr><td>–executor-cores</td><td>配置每个Executor的虚拟CPU   core数量</td></tr><tr><td></td><td>Executor如果是3核,设备是单核,模拟的多线程操作,其实是并发操作</td></tr></tbody></table><h3 id="6-2并行与并发"><a href="#6-2并行与并发" class="headerlink" title="6.2并行与并发"></a>6.2并行与并发</h3><p>我们会给Executor分配虚拟的核心数量,如果核心不够会触发多线程操作,并发</p><p>如果核心够用,则进行并行操作,可以进行配置</p><h3 id="6-3有向无环图"><a href="#6-3有向无环图" class="headerlink" title="6.3有向无环图"></a>6.3有向无环图</h3><p>表示一种依赖关系,依赖关系形成的拓扑图形称为DAG,有向无环图</p><pre class="mermaid">graph LRA-->BB-->CB-->DD--禁止-->A</pre><p>如图,D向A会形成无环图,有环会形成死循环(与maven类似)</p><h2 id="7-Yarn-Cluster任务提交流程"><a href="#7-Yarn-Cluster任务提交流程" class="headerlink" title="7.Yarn Cluster任务提交流程"></a>7.Yarn Cluster任务提交流程</h2><p><strong>核心</strong>:分两大块 1.资源的申请 2.计算的准备 任务发给资源</p><p>Client与Cluster区别在于Driver程序运行的节点位置</p><p><img src="https://i.loli.net/2020/12/23/JjlbBD6sYXgC3Hx.png" alt="Spark-Yarn提交流程"></p><ol><li>任务提交</li><li>向ResourceManager通讯申请启动ApplicationMaster</li><li>ApplicationMaster选择合适的节点借用NodeManager启动一个container</li><li>在container中运行AppMatser=Driver</li><li>Driver启动后,向RM申请container运行Executor进程</li><li>Executor进程启动后反向向Driver进行注册</li><li>全部注册完成后开始执行main函数</li><li>执行到action算子,触发一个job,根据是否发生shuffle开始划分stage</li><li>每个stage生成对应的TaskSet[task1,task2,task3…]</li><li>然后将task分发到各个Executor上执行</li></ol><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">[shenneng@hadoop102 spark-yarn]$ bin&#x2F;spark-submit \--class org.apache.spark.examples.SparkPi \--master yarn \--deploy-mode cluster \.&#x2F;examples&#x2F;jars&#x2F;spark-examples_2.12-3.0.0.jar \10<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="0x2-Spark框架"><a href="#0x2-Spark框架" class="headerlink" title="0x2 Spark框架"></a>0x2 Spark框架</h1><p>Spark和Hadoop的根本差异是多个作业之间的数据通信问题</p><p><img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_20_12/Spark%E8%BF%90%E8%A1%8C%E6%A8%A1%E5%BC%8F%E7%A4%BA%E6%84%8F%E5%9B%BE.png" alt="Spark运行模式示意图"></p><table><thead><tr><th>Spark运行环境</th><th>用法</th></tr></thead><tbody><tr><td>Local模式</td><td>测试</td></tr><tr><td>Standalone模式</td><td>独立部署Matser-Worker<br/>自身提供计算资源,降低了耦合性</td></tr><tr><td>Yarn模式</td><td>提供集群模式</td></tr><tr><td>K8S模式</td><td></td></tr><tr><td>Mesos模式</td><td>Twitter</td></tr><tr><td>Windows模式</td><td></td></tr></tbody></table><h2 id="1-Yarn模式"><a href="#1-Yarn模式" class="headerlink" title="1.Yarn模式"></a>1.Yarn模式</h2><p><img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_20_12/Yarn-Cluster%E6%8F%90%E4%BA%A4%E6%B5%81%E7%A8%8B.png" alt="Yarn-Cluster提交流程"></p><p>Client与Cluster的主要区别是Driver是否在本地运行</p><h2 id="2-分布式计算模拟"><a href="#2-分布式计算模拟" class="headerlink" title="2.分布式计算模拟"></a>2.分布式计算模拟</h2><p><img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_20_12/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97%E6%A0%B8%E5%BF%83.png" alt="分布式计算核心-拆分Task"></p><p>通过简单的分布式计算模拟,理解任务的拆分,运行的模块,并行的原理,RDD的封装,底层数据结构</p><p><strong>DRIVER</strong></p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">&#x2F;&#x2F;DRIVERpackage com.ecust.saprkcoreimport java.io.&#123;ObjectOutputStream, OutputStream&#125;import java.net.&#123;ServerSocket, Socket&#125;&#x2F;** * @author Jinxin Li * @create 2020-12-31 13:43 *&#x2F;object Driver &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    &#x2F;&#x2F;进行逻辑的封装,计算的准备,数据的提交    val client1 &#x3D; new Socket(&quot;localhost&quot;, 9999)    val client2 &#x3D; new Socket(&quot;localhost&quot;, 8888)    val out1: OutputStream &#x3D; client1.getOutputStream    val out2: OutputStream &#x3D; client2.getOutputStream    val objOut1 &#x3D; new ObjectOutputStream(out1)    val objOut2 &#x3D; new ObjectOutputStream(out2)    val  task &#x3D; new Task()    val subTask1 &#x3D; new SubTask()    subTask1.logic&#x3D;task.logic    subTask1.data&#x3D;task.data.take(2)    val subTask2 &#x3D; new SubTask()    subTask2.logic&#x3D;task.logic    subTask2.data&#x3D;task.data.takeRight(2)    objOut1.writeObject(subTask1)    objOut1.flush()    objOut1.close()    objOut2.writeObject(subTask2)    objOut2.flush()    objOut2.close()    &#x2F;&#x2F;发送,注意在网络中传递的数据要进行序列化,不可能传递对象,必须序列化    println(&quot;任务发送完毕&quot;)    &#x2F;&#x2F;关闭客户端    client1.close()    client2.close()  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>EXECUTOR1</strong></p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">object Executor1 &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    &#x2F;&#x2F;启动服务器,接受数据    val server &#x3D; new ServerSocket(9999)    println(&quot;服务器9999启动,等待接受数据...&quot;)    val client: Socket &#x3D; server.accept()    val in: InputStream &#x3D; client.getInputStream    val objIn &#x3D; new ObjectInputStream(in)    val task &#x3D; objIn.readObject().asInstanceOf[SubTask]    val ints: List[Int] &#x3D; task.compute()    println(&quot;接收到客户端9999接受的数据:&quot;+ints)    objIn.close()    client.close()    server.close()  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>EXECUTOR2</strong></p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">object Executor2 &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    &#x2F;&#x2F;启动服务器,接受数据    val server &#x3D; new ServerSocket(8888)    println(&quot;服务器9999启动,等待接受数据...&quot;)    val client: Socket &#x3D; server.accept()    val in: InputStream &#x3D; client.getInputStream    val objIn &#x3D; new ObjectInputStream(in)    val task&#x3D; objIn.readObject().asInstanceOf[SubTask]    val ints: List[Int] &#x3D; task.compute()    println(&quot;接收到客户端8888接受的数据:&quot;+ints)    objIn.close()    client.close()    server.close()  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>SUBTASK</strong></p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">class SubTask extends Serializable &#123;  &#x2F;&#x2F;这是一种特殊的数据结构,其中包含了数据的格式,数据的计算逻辑与算子转换  &#x2F;&#x2F;接收到数据之后,可以进行计算  &#x2F;&#x2F;RDD 广播变量 累加器 就是类似的数据结构  var data :List[Int] &#x3D; _  var logic:Int&#x3D;&gt;Int &#x3D; _  &#x2F;&#x2F;计算任务  def  compute() &#x3D;&#123;    data.map(logic)  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>TASK</strong></p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">class Task extends Serializable &#123;&#x2F;&#x2F;实现序列化 特质  &#x2F;&#x2F;包含原数据的数据结构  val data &#x3D; List(1, 2, 3, 4)  val function: Int &#x3D;&gt; Int &#x3D; (num: Int) &#x3D;&gt; &#123;    num * 2  &#125;  &#x2F;&#x2F;注意函数的类型是Int&#x3D;&gt;Int  val logic:Int&#x3D;&gt;Int &#x3D; _*2  &#x2F;&#x2F;计算任务  def  compute() &#x3D;&#123;    data.map(logic)  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="3-RDD与IO"><a href="#3-RDD与IO" class="headerlink" title="3.RDD与IO"></a>3.RDD与IO</h2><p><strong>字节流&amp;字符流</strong></p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">InputStream in &#x3D; new FileInputStream(&quot;path&quot;)int i &#x3D; -1while(i &#x3D; in.read()!&#x3D;-1)&#123;    println(i)&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="mermaid">graph LRFile-->FileInputStream--read-->console</pre><p><strong>缓冲流</strong></p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">InputStream in &#x3D; new BufferedInputStream(new FileInputStream(&quot;path&quot;))int i &#x3D; -1while(i &#x3D; in.read()!&#x3D;-1)&#123;    println(i)&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="mermaid">graph LRFile-->FileInputStream-->BufferedInputStream--read-->console</pre><p><img src="C:%5CUsers%5C89388%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20201231224057782.png" alt="缓冲区的缓冲流"></p><p><strong>转换流</strong>InputStreamReader</p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">Reader in &#x3D; new BufferedReader(    new InputStreamReader(        new FileInputStream(&quot;path&quot;),        &quot;UTF-8&quot;        )    )String s &#x3D; nullwhile((s&#x3D;in.readLine())!&#x3D;null)&#123;    println(i);    )<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_20_12/%E6%B5%81%E7%9A%84%E8%A3%85%E9%A5%B0%E8%80%85%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F.png" alt="装饰者设计模式的IO"></p><p>可以看出核心是FileInputFormat,转换流与缓冲流都是包装,这种设计模式成为装饰者设计模式</p><p>哪些inputformat,都是对读取逻辑的封装,没有真正的读取数据</p><p>readLine才会真正的执行,new的过程仅仅是建立连接,但是没有真正的读取,有种延迟加载的感觉</p><p>RDD的组装方式,与IO的封装也是非常的类似</p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">new HadoopRDD&#x2F;&#x2F;textFilenew MapPartitionsRDD()&#x2F;&#x2F;flatMapnew MapPartitionsRDD()&#x2F;&#x2F;mapnew ShuffleRDD()&#x2F;&#x2F;reduceByKey&#x2F;&#x2F;执行rdd.collect()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>一层一层的包装</p><p><img src="C:%5CUsers%5C89388%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20201231232951858.png" alt="RDD的装饰者模式理解"></p><p>RDD的数据处理方式类似于IO流,也有装饰者设计模式</p><p>RDD的数据只有在调用collect方法时,才会真正的执行</p><p>RDD是不保存数据的,但是IO可以临时保存一部分数据</p><h2 id="4-分区"><a href="#4-分区" class="headerlink" title="4. 分区"></a>4. 分区</h2><p>RDD是一个最基本的数据处理模型</p><p>类似于Kafka中的分区,我们将数据进行分区,分区之后分成不成的Task,可以分发至Executor进行计算</p><p>RDD是最小的数据处理单元,里面包含了分区信息,提高并行计算的能力</p><p><img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_20_12/RDD%E6%95%B0%E6%8D%AE%E7%9A%84%E5%88%87%E5%88%86.png" alt="数据的分区"></p><h1 id="0x3-spark核心"><a href="#0x3-spark核心" class="headerlink" title="0x3 spark核心"></a>0x3 spark核心</h1><h2 id="1-spark核心三大数据结构"><a href="#1-spark核心三大数据结构" class="headerlink" title="1. spark核心三大数据结构"></a>1. spark核心三大数据结构</h2><p>RDD : 弹性分布式数据集</p><p>累加器：分布式共享只写变量</p><p>广播变量：分布式共享只读变量</p><h2 id="2-RDD基本概念"><a href="#2-RDD基本概念" class="headerlink" title="2.RDD基本概念"></a>2.RDD基本概念</h2><p>RDD（Resilient Distributed Dataset）叫做弹性分布式数据集，是Spark中最基本的数据处理模型。代码中是一个抽象类，它代表一个弹性的、不可变、可分区、里面的元素可并行计算的集合。</p><p><strong>1.弹性</strong></p><p>存储的弹性：内存与磁盘的自动切换；</p><p>容错的弹性：数据丢失可以自动恢复；</p><p>计算的弹性：计算出错重试机制；</p><p>分片的弹性：可根据需要重新分片。</p><p><strong>2.分布式</strong>：数据存储在大数据集群不同节点上</p><p><strong>3.数据集</strong>：RDD封装了计算逻辑，并不保存数据</p><p><strong>4.数据抽象</strong>：RDD是一个抽象类，需要子类具体实现</p><p><strong>5.不可变</strong>：RDD封装了计算逻辑，是不可以改变的，想要改变，只能产生新的RDD，在新的RDD里面封装计算逻辑</p><p><strong>6.可分区、并行计算</strong></p><h2 id="3-RDD核心属性"><a href="#3-RDD核心属性" class="headerlink" title="3.RDD核心属性"></a>3.RDD核心属性</h2><p><a href="https://data-flair.training/blogs/spark-rdd-tutorial/">RDD详细描述</a></p><p><img src="https://i.loli.net/2020/12/23/PTqaCxnHeBdl91G.png" alt="SparkRDD的核心属性"></p><p>图:Spark RDD核心属性</p><ol><li><p>粗粒度操作(无法对单个元素进行操作)</p></li><li><p>内存中计算</p></li><li><p>懒执行</p></li><li><p>不变性</p></li><li><p>容错性</p></li><li><p>持久性(cache可以选择等级与checkpoint)</p><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql">--数据缓存wordToOneRdd.cache()--可以更改存储级别mapRdd.persist(StorageLevel.MEMORY_AND_DISK_2)--设置检查点路径sc.setCheckpointDir(&quot;.&#x2F;checkpoint1&quot;)--数据检查点：针对wordToOneRdd做检查点计算wordToOneRdd.checkpoint()--一般两者联合使用<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>可分区(分区列表)</p></li><li><p>粘度分区(自定分区)</p></li></ol><h2 id="4-RDD缺点"><a href="#4-RDD缺点" class="headerlink" title="4.RDD缺点"></a>4.RDD缺点</h2><ol><li>没有内置的优化引擎,RDD无法利用Spark的高级优化器（包括catalyst optimizer与Tungsten执行引擎）的优势。开发人员需要根据其属性优化每个RDD</li><li>只能处理结构化数据与DataFrame和数据集不同，RDD不会推断所摄取数据的模式，而是需要用户指定它。</li><li>性能限制,作为内存中的JVM对象，RDD涉及垃圾收集和Java序列化的开销，这在数据增长时非常昂贵。</li><li>没有足够的内存来存储RDD时，它们会拖慢运行速度。也可以将RDD的该分区存储在不适合RAM的磁盘上。结果，它将提供与当前数据并行系统类似的性能。</li></ol><h2 id="5-RDD的来源"><a href="#5-RDD的来源" class="headerlink" title="5.RDD的来源"></a>5.RDD的来源</h2><ol><li>使用集合创建parallelize MakeRDD</li><li>外部存储文件创建RDD textfile</li><li>从其他RDD创建(血缘关系,cache,checkpoint)</li><li>直接创建RDD 内部使用</li></ol><h2 id="6-RDD的-分区分片问题"><a href="#6-RDD的-分区分片问题" class="headerlink" title="6.RDD的==分区分片问题=="></a>6.RDD的==分区分片问题==</h2><p>RDD分区意味着一个分区一个job么,</p><p>RDD分区3意味着要在三个executor里执行么</p><p>重新分区,加入三个executor在不同的container里是如何发生shuffle里的,还是三个分区是一个job,这一个job在一个container里执行</p><h2 id="7-RDD的序列化"><a href="#7-RDD的序列化" class="headerlink" title="7.RDD的序列化"></a>7.RDD的序列化</h2><p>从计算的角度, 算子以外的代码都是在Driver端执行, 算子里面的代码都是在Executor端执行。</p><p>那么在scala的函数式编程中，就会导致算子内经常会用到算子外的数据，这样就形成了闭包的效果，如果使用的算子外的数据无法序列化，就意味着无法传值给Executor端执行，就会发生错误</p><p>所以需要在执行任务计算前，检测闭包内的对象是否可以进行序列化，这个操作我们称之为闭包检测。Scala2.12版本后闭包编译方式发生了改变  </p><p><strong>Kryo序列化框架</strong></p><p>Spark2.0开始支持另外一种Kryo序列化机制。Kryo速度是Serializable的10倍。当RDD在Shuffle数据的时候，简单数据类型、数组和字符串类型已经在Spark内部使用Kryo来序列化。</p><p>即使使用Kryo序列化，也要继承Serializable接口</p><h2 id="8-RDD依赖与血缘"><a href="#8-RDD依赖与血缘" class="headerlink" title="8.RDD依赖与血缘"></a>8.RDD依赖与血缘</h2><h3 id="8-1概述"><a href="#8-1概述" class="headerlink" title="8.1概述"></a>8.1概述</h3><p>RDD的Lineage会记录RDD的元数据信息和转换行为，当该RDD的部分分区数据丢失时，它可以根据这些信息来重新运算和恢复丢失的数据分区。</p><p>相邻的两个RDD之间的关系称为<strong>依赖关系</strong></p><p>多个连续的RDD的依赖关系,称之为<strong>血缘关系</strong></p><p>我们的每一个RDD都会保存我们的血缘关系,会保存之前的血缘关系</p><p>RDD为了提供容错性,需要将RDD间的关系保存下来,一旦出现错误,可以根据血缘关系将数据源重新计算</p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">val rdd1 &#x3D; rdd.map(_.2)&#x2F;&#x2F;新的RDD依赖于旧的RDD<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre class="mermaid">graph LRRDD1--依赖-->RDD2--依赖-->RDD3--依赖-->RDD4RDD4--flatmap-->RDD3RDD3--map-->RDD2RDD2--reduceByKey-->RDD1</pre><h3 id="8-2血缘关系的查看"><a href="#8-2血缘关系的查看" class="headerlink" title="8.2血缘关系的查看"></a>8.2血缘关系的查看</h3><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">&#x2F;&#x2F;血缘关系的演示&#x2F;&#x2F;每个RDD记录了以前所有的血缘关系package com.testimport org.apache.spark.api.java.JavaSparkContext.fromSparkContextimport org.apache.spark.rdd.RDDimport org.apache.spark.&#123;SparkConf, SparkContext&#125;&#x2F;** * @author Jinxin Li * @create 2020-10-26 10:04 *&#x2F;object wordCount &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    val config: SparkConf &#x3D; new SparkConf().setAppName(&quot;MyWordCount&quot;).setMaster(&quot;local[*]&quot;)    val sc &#x3D; new SparkContext(config)    val lines: RDD[String] &#x3D; sc.textFile(&quot;.&#x2F;word.txt&quot;)    println(&quot;++++++++++++++++++++++++++++++++++&#x3D;&quot;)    println(lines.toDebugString)&#x2F;&#x2F;打印血缘关系    val words: RDD[String] &#x3D; lines.flatMap(_.split(&quot; &quot;))    println(&quot;++++++++++++++++++++++++++++++++++&#x3D;&quot;)    println(words.toDebugString)    val pairs: RDD[(String, Int)] &#x3D; words.map((_, 1))    println(&quot;++++++++++++++++++++++++++++++++++&#x3D;&quot;)    println(pairs.toDebugString)    val word: RDD[(String, Int)] &#x3D; pairs.reduceByKey(_ + _)    println(&quot;++++++++++++++++++++++++++++++++++&#x3D;&quot;)    println(word.toDebugString)    println(&quot;++++++++++++++++++++++++++++++++++&#x3D;&quot;)    word.collect().foreach(println(_))    sc.close;  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-java" data-language="java"><code class="language-java">++++++++++++++++++++++++++++++++++&#x3D;(2) .&#x2F;word.txt MapPartitionsRDD[1] at textFile at wordCount.scala:15 [] |  .&#x2F;word.txt HadoopRDD[0] at textFile at wordCount.scala:15 []++++++++++++++++++++++++++++++++++&#x3D;(2) MapPartitionsRDD[2] at flatMap at wordCount.scala:18 [] |  .&#x2F;word.txt MapPartitionsRDD[1] at textFile at wordCount.scala:15 [] |  .&#x2F;word.txt HadoopRDD[0] at textFile at wordCount.scala:15 []++++++++++++++++++++++++++++++++++&#x3D;(2) MapPartitionsRDD[3] at map at wordCount.scala:21 [] |  MapPartitionsRDD[2] at flatMap at wordCount.scala:18 [] |  .&#x2F;word.txt MapPartitionsRDD[1] at textFile at wordCount.scala:15 [] |  .&#x2F;word.txt HadoopRDD[0] at textFile at wordCount.scala:15 []++++++++++++++++++++++++++++++++++&#x3D;(2) ShuffledRDD[4] at reduceByKey at wordCount.scala:24 []    &#x2F;&#x2F;这个地方断开,表示shuffle +- +-(2) MapPartitionsRDD[3] at map at wordCount.scala:21 []    |  MapPartitionsRDD[2] at flatMap at wordCount.scala:18 []    |  .&#x2F;word.txt MapPartitionsRDD[1] at textFile at wordCount.scala:15 []    |  .&#x2F;word.txt HadoopRDD[0] at textFile at wordCount.scala:15 []++++++++++++++++++++++++++++++++++&#x3D;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>可以看出每个RDD会存储所有的血缘关系</p><p>同时使用dependices可以查看依赖关系</p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">object wordCount &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    val config: SparkConf &#x3D; new SparkConf().setAppName(&quot;MyWordCount&quot;).setMaster(&quot;local[*]&quot;)    val sc &#x3D; new SparkContext(config)    val lines: RDD[String] &#x3D; sc.textFile(&quot;.&#x2F;word.txt&quot;)    println(&quot;++++++++++++++++++++++++++++++++++&#x3D;&quot;)    println(lines.dependencies)    val words: RDD[String] &#x3D; lines.flatMap(_.split(&quot; &quot;))    println(&quot;++++++++++++++++++++++++++++++++++&#x3D;&quot;)    println(words.dependencies)    val pairs: RDD[(String, Int)] &#x3D; words.map((_, 1))    println(&quot;++++++++++++++++++++++++++++++++++&#x3D;&quot;)    println(pairs.dependencies)    val word: RDD[(String, Int)] &#x3D; pairs.reduceByKey(_ + _)    println(&quot;++++++++++++++++++++++++++++++++++&#x3D;&quot;)    println(word.dependencies)    println(&quot;++++++++++++++++++++++++++++++++++&#x3D;&quot;)    word.collect().foreach(println(_))    sc.close;  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">++++++++++++++++++++++++++++++++++&#x3D;List(org.apache.spark.OneToOneDependency@1a2bcd56)++++++++++++++++++++++++++++++++++&#x3D;List(org.apache.spark.OneToOneDependency@3c3a0032)++++++++++++++++++++++++++++++++++&#x3D;List(org.apache.spark.OneToOneDependency@5e519ad3)++++++++++++++++++++++++++++++++++&#x3D;List(org.apache.spark.ShuffleDependency@765d55d5)++++++++++++++++++++++++++++++++++&#x3D;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>可以看出存在两种依赖关系,一种OneToOneDependency与ShuffleDependency</p><p>新的RDD的一个分区的数据依赖于旧的RDD的一个分区的数据,这种依赖称之为OneToOne依赖</p><p>新的RDD的一个分区的数据依赖于旧的RDD的多个分区的数据,这种依赖称为Shuffle依赖(数据被打乱重新组合)</p><p>源码中的依赖关系</p><p><img src="https://i.loli.net/2020/12/23/zWdaBIDnpNuxlr7.png" alt="RDD依赖关系的继承关系"></p><p><img src="https://i.loli.net/2020/12/23/w2M9xlshmoVAHL4.png" alt="宽依赖的图"></p><h3 id="8-3阶段划分与源码"><a href="#8-3阶段划分与源码" class="headerlink" title="8.3阶段划分与源码"></a>8.3阶段划分与源码</h3><p>Shuffle划分阶段</p><p>如果是oneToOne不需要划分阶段</p><p>不同的阶段要保证Task执行完毕才能执行下一个阶段</p><p>阶段的数量等于shuffle依赖的数量+1</p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">collectdagScheduler.runjobval waiter &#x3D; submitJob&#x2F;&#x2F;DAGScheduler-681&#x2F;&#x2F;让下翻override def run(): Unit &#x3D; eventProcessLoop.post(JobSubmitted)&#x2F;&#x2F;DAGScheduler-714private[scheduler] def handleJobSubmitted&#x2F;&#x2F;DAGScheduler-975&#123;    var finalStage: ResultStage &#x3D; null&#x2F;&#x2F;判定finalStage是否存在 985    finalStage &#x3D; createResultStage(finalRDD, func, partitions, jobId, callSite)&#x2F;&#x2F;如果不存在则创建一个空的ResultStage 986&#125;&#x2F;&#x2F;也就是说ResultStage只有一个private def createResultStage:ResultStage &#x3D; &#123;    &#x2F;&#x2F;445    val parents &#x3D; getOrCreateParentStages(rdd, jobId)&#x2F;&#x2F;有没有上一个阶段,这个rdd是当前的reduceBykey最后的rdd    val stage &#x3D; new ResultStage(id, rdd, func, partitions, parents, jobId, callSite)    stage  &#125;&#x2F;**获得父阶段列表*&#x2F;private def getOrCreateParentStages(rdd: RDD[_], firstJobId: Int): List[Stage] &#x3D; &#123;&#x2F;&#x2F;466    getShuffleDependencies(rdd).map &#123; shuffleDep &#x3D;&gt;      getOrCreateShuffleMapStage(shuffleDep, firstJobId)&#x2F;&#x2F;一个shuffle就会转换为一个阶段    &#125;.toList  &#125;private[scheduler] def getShuffleDependencies(&#x2F;&#x2F;508      rdd: RDD[_]): HashSet[ShuffleDependency[_, _, _]] &#x3D; &#123;    val parents &#x3D; new HashSet[ShuffleDependency[_, _, _]]    val visited &#x3D; new HashSet[RDD[_]]    val waitingForVisit &#x3D; new ListBuffer[RDD[_]]    waitingForVisit +&#x3D; rdd&#x2F;&#x2F;放入当前rdd reduceByKey的rdd    while (waitingForVisit.nonEmpty) &#123;      val toVisit &#x3D; waitingForVisit.remove(0)      if (!visited(toVisit)) &#123;&#x2F;&#x2F;判断之前是否访问过        visited +&#x3D; toVisit        toVisit.dependencies.foreach &#123;          case shuffleDep: ShuffleDependency[_, _, _] &#x3D;&gt;            parents +&#x3D; shuffleDep&#x2F;&#x2F;模式匹配判断是否是shuffle依赖          case dependency &#x3D;&gt;            waitingForVisit.prepend(dependency.rdd)        &#125;      &#125;    &#125;    parents  &#125;private def getOrCreateShuffleMapStage( &#x2F;&#x2F;338    ... ShuffleMapStage &#x3D; &#123;...        createShuffleMapStage(shuffleDep, firstJobId)                          &#125;     def createShuffleMapStage[K, V, C](&#x2F;&#x2F;384      ... ShuffleMapStage &#x3D; &#123;    val rdd &#x3D; shuffleDep.rdd          ...    val numTasks &#x3D; rdd.partitions.length    val parents &#x3D; getOrCreateParentStages(rdd, jobId)    val id &#x3D; nextStageId.getAndIncrement()    val stage &#x3D; new ShuffleMapStage()      ...    stage  &#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="8-4RDD的任务划分"><a href="#8-4RDD的任务划分" class="headerlink" title="8.4RDD的任务划分"></a>8.4RDD的任务划分</h3><p>行动算子底层是runJob</p><p><strong>Application</strong>:初始化一个SparkContext即生成一个Application</p><p><strong>Job</strong>:一个Action算子就会生成一个Job</p><p><strong>Stage</strong>:Stage等于宽依赖(ShuffleDependency)+1</p><p><strong>Task</strong>:一个Stage阶段中,最后一个RDD分区个数就是Task的个数</p><p>注意:Application-&gt;Job-&gt;Stage-&gt;Task每一层都是一对n的关系</p><p>提交过程是一个阶段一个阶段的提交</p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">private def submitStage(stage: Stage): Unit &#x3D; &#123;&#x2F;&#x2F;1084    val jobId &#x3D; activeJobForStage(stage)    if (jobId.isDefined) &#123;      logDebug(s&quot;submitStage($stage (name&#x3D;$&#123;stage.name&#125;;&quot; +        s&quot;jobs&#x3D;$&#123;stage.jobIds.toSeq.sorted.mkString(&quot;,&quot;)&#125;))&quot;)      if (!waitingStages(stage) &amp;&amp; !runningStages(stage) &amp;&amp; !failedStages(stage)) &#123;        val missing &#x3D; getMissingParentStages(stage).sortBy(_.id)&#x2F;&#x2F;有没有上一级阶段        logDebug(&quot;missing: &quot; + missing)        if (missing.isEmpty) &#123;&#x2F;&#x2F;如果没有上一级的stage,则为空          logInfo(&quot;Submitting &quot; + stage + &quot; (&quot; + stage.rdd + &quot;), which has no missing parents&quot;)          submitMissingTasks(stage, jobId.get)&#x2F;&#x2F;为空就提交stage&#x2F;tasks        &#125; else &#123;          for (parent &lt;- missing) &#123;            submitStage(parent)          &#125;          waitingStages +&#x3D; stage        &#125;      &#125;    &#125; else &#123;      abortStage(stage, &quot;No active job for stage &quot; + stage.id, None)    &#125;  &#125;val tasks: Seq[Task[_]] &#x3D; try &#123;&#x2F;&#x2F;1217      val serializedTaskMetrics &#x3D; closureSerializer.serialize(stage.latestInfo.taskMetrics).array()      stage match &#123;&#x2F;&#x2F;匹配的阶段类型        case stage: ShuffleMapStage &#x3D;&gt;&#x2F;&#x2F;shuffleMaptask          &#x2F;&#x2F;new 几个跟map相关,ShuffleMapStage          stage.pendingPartitions.clear()          partitionsToCompute.map &#123; id &#x3D;&gt;&#x2F;&#x2F;计算分区            val locs &#x3D; taskIdToLocations(id)            val part &#x3D; partitions(id)            stage.pendingPartitions +&#x3D; id            new ShuffleMapTask(stage.id, stage.latestInfo.attemptNumber,              taskBinary, part, locs, properties, serializedTaskMetrics, Option(jobId),              Option(sc.applicationId), sc.applicationAttemptId, stage.rdd.isBarrier())          &#125;        case stage: ResultStage &#x3D;&gt;          partitionsToCompute.map &#123; id &#x3D;&gt;&#x2F;&#x2F;这里面有多少元素            val p: Int &#x3D; stage.partitions(id)            val part &#x3D; partitions(p)            val locs &#x3D; taskIdToLocations(id)&#x2F;&#x2F;到底有多个new            new ResultTask(stage.id, stage.latestInfo.attemptNumber,              taskBinary, part, locs, id, properties, serializedTaskMetrics,              Option(jobId), Option(sc.applicationId), sc.applicationAttemptId,              stage.rdd.isBarrier())          &#125;      &#125;&#125;&#x2F;&#x2F;计算分区&#x2F;&#x2F; Figure out the indexes of partition ids to compute.val partitionsToCompute: Seq[Int] &#x3D; stage.findMissingPartitions()&#123;&#125;    &#x2F;&#x2F;ResultStage 61override def findMissingPartitions(): Seq[Int] &#x3D; &#123;    val job &#x3D; activeJob.get    (0 until job.numPartitions).filter(id &#x3D;&gt; !job.finished(id))&#125;&#x2F;&#x2F;此处的job.numPartitions就是最后一个RDD的分区&#x2F;&#x2F;三个分区就是0-3&#x2F;&#x2F;一个RDD的三个分区,从并行角度就会分配为3个Task&#x2F;&#x2F;SuffleMapStage 91override def findMissingPartitions(): Seq[Int] &#x3D; &#123;    mapOutputTrackerMaster      .findMissingPartitions(shuffleDep.shuffleId)      .getOrElse(0 until numPartitions)  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>一个应用程序会对应多个job(一个行动算子算是一个job)</p><p>ShuffleMapStage=&gt;ShuffleMapTask</p><p>ResultStage=&gt;ResultTask</p><h2 id="9-RDD分区器"><a href="#9-RDD分区器" class="headerlink" title="9.RDD分区器"></a>9.RDD分区器</h2><ol><li>Hash分区(默认)</li><li>Range分区</li><li>自定义分区</li></ol><p>只有Key-Value类型的RDD才有分区器，非Key-Value类型的RDD分区的值是None</p><p>每个RDD的分区ID范围：0 ~ (numPartitions - 1)，决定这个值是属于那个分区的。</p><p><strong>Hash</strong>分区：对于给定的key，计算其hashCode,并除以分区个数取余</p><p><strong>Range分区</strong>：将一定范围内的数据映射到一个分区中，尽量保证每个分区数据均匀，而且分区间有序</p><h3 id="1-自定义分区器"><a href="#1-自定义分区器" class="headerlink" title="1.自定义分区器"></a>1.自定义分区器</h3><h4 id="a-gt-HashPartitioner"><a href="#a-gt-HashPartitioner" class="headerlink" title="a&gt;HashPartitioner"></a>a&gt;HashPartitioner</h4><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">object PartitionBy &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    val sparkConf: SparkConf &#x3D; new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;Main&quot;)    val sc: SparkContext &#x3D; new SparkContext(sparkConf)    val rdd1: RDD[(String, Int)] &#x3D; sc.textFile(&quot;Day06&#x2F;input&#x2F;number&quot;).map((_, 1))    &#x2F;&#x2F;只有k-v值才有分区器    rdd1.saveAsTextFile(&quot;.&#x2F;output&quot;)    &#x2F;&#x2F;使用hash分区器    val rdd2: RDD[(String, Int)] &#x3D; rdd1.partitionBy(new HashPartitioner(3))    rdd2.saveAsTextFile(&quot;.&#x2F;output2&quot;)  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="b-gt-RangePartitioner"><a href="#b-gt-RangePartitioner" class="headerlink" title="b&gt;RangePartitioner"></a>b&gt;RangePartitioner</h4><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">object PartitionBy &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    val sparkConf: SparkConf &#x3D; new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;Main&quot;)    val sc: SparkContext &#x3D; new SparkContext(sparkConf)    val rdd1&#x3D; sc.textFile(&quot;Day06&#x2F;input&#x2F;number&quot;).map((_, 1))    &#x2F;&#x2F;只有k-v值才有分区器    rdd1.saveAsTextFile(&quot;.&#x2F;output&quot;)    &#x2F;&#x2F;使用rangePartitioner    val value &#x3D; new RangePartitioner[String, Int](2, rdd1.sample(false, 0.5))    &#x2F;&#x2F;range分区器的使用,要定义泛型,传递分区,传递sample    &#x2F;&#x2F;首先要传递一个分区,传递一个    rdd1.partitionBy(value)    rdd1.saveAsTextFile(&quot;.&#x2F;output2&quot;)  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="c-gt-自定义分区"><a href="#c-gt-自定义分区" class="headerlink" title="c&gt;自定义分区"></a>c&gt;自定义分区</h3><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">&#x2F;&#x2F;自定义分区器case class MyPartitioner(numPartition:Int) extends Partitioner &#123;    override def numPartitions: Int &#x3D; numPartition    override def getPartition(key: Any): Int &#x3D; (math.random() * numPartition).toInt&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">&#x2F;&#x2F;自定义分区器的使用object PartitionBy &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    val sparkConf: SparkConf &#x3D; new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;Main&quot;)    val sc: SparkContext &#x3D; new SparkContext(sparkConf)    val rdd1&#x3D; sc.textFile(&quot;Day06&#x2F;input&#x2F;number&quot;).map((_, 1))    &#x2F;&#x2F;只有k-v值才有分区器    rdd1.saveAsTextFile(&quot;.&#x2F;output&quot;)    &#x2F;&#x2F;使用rangePartitioner    val value &#x3D; new RangePartitioner[String, Int](2, rdd1.sample(false, 0.5))    &#x2F;&#x2F;range分区器的使用,要定义泛型,传递分区,传递sample    &#x2F;&#x2F;首先要传递一个分区,传递一个    val value1: RDD[(String, Int)] &#x3D; rdd1.partitionBy(MyPartitioner(2))    value1.saveAsTextFile(&quot;.&#x2F;output2&quot;)  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="10-RDD累加器"><a href="#10-RDD累加器" class="headerlink" title="10.RDD累加器"></a>10.RDD累加器</h2><p><strong>系统累加器</strong></p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">object MyAccumulator &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    val sc &#x3D; new SparkContext(new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;myAccumulator&quot;))    val rdd: RDD[Int] &#x3D; sc.makeRDD(List(1, 2, 3, 4, 5))    &#x2F;&#x2F;声明系统累加器    val sum: LongAccumulator &#x3D; sc.longAccumulator(&quot;sum&quot;)    rdd.foreach(      num&#x3D;&gt;&#123;        sum.add(num)      &#125;    )    &#x2F;&#x2F;获取累加器    println(sum.value)  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="自定义累加器"><a href="#自定义累加器" class="headerlink" title="自定义累加器"></a><strong>自定义累加器</strong></h3><p><strong>1.自定wordcount累加器</strong></p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">&#x2F;&#x2F;自定义累加器实现wordcountclass DefineAccumulator extends AccumulatorV2[String,mutable.Map[String,Int]]&#123;  val map: mutable.Map[String, Int] &#x3D; mutable.Map()  &#x2F;&#x2F;判断累加器是否为初始状态  override def isZero: Boolean &#x3D; &#123;map.isEmpty&#125;  &#x2F;&#x2F;复制累加器  override def copy(): AccumulatorV2[String, mutable.Map[String, Int]] &#x3D; &#123;new DefineAccumulator()&#125;  &#x2F;&#x2F;重置累加器  override def reset(): Unit &#x3D; map.clear()  &#x2F;&#x2F;区内相加  override def add(v: String): Unit &#x3D; &#123;    &#x2F;&#x2F;区内相加的定义,如果存在元素,就对key值+1,如果不存在,就添加当前元素,key+1    map(v)&#x3D;map.getOrElse(v,0)+1  &#125;  &#x2F;&#x2F;区间相加  override def merge(other: AccumulatorV2[String, mutable.Map[String, Int]]): Unit &#x3D; &#123;    &#x2F;&#x2F;区间相加,固定    val map1: mutable.Map[String, Int] &#x3D; this.value    val map2: mutable.Map[String, Int] &#x3D; other.value    map2.foreach&#123;      case (k,v) &#x3D;&gt; map1(k)&#x3D;map1.getOrElse(k,0)+v    &#125;  &#125;  override def value: mutable.Map[String, Int] &#x3D; map&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>2.注册并使用定义累加器</strong></p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">object MyAccumulator2 &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    val sc &#x3D; new SparkContext(new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;accumulator&quot;))    val word: RDD[String] &#x3D; sc.textFile(&quot;.&#x2F;word.txt&quot;)    val words &#x3D; word.flatMap(_.split(&quot; &quot;))    &#x2F;&#x2F;new出累加器    val uacc &#x3D; new DefineAccumulator    &#x2F;&#x2F;注册累加器    sc.register(uacc)    &#x2F;&#x2F;使用累加器    words.foreach(uacc.add(_))    println(uacc.value)&#x2F;&#x2F;注意输出为accumulator的值  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="11-广播变量"><a href="#11-广播变量" class="headerlink" title="11.广播变量"></a>11.广播变量</h2><p>广播变量用来高效分发较大的对象。向所有工作节点发送一个较大的只读值，以供一个或多个Spark操作使用。比如，如果你的应用需要向所有节点发送一个较大的只读查询表，广播变量用起来都很顺手。在多个并行操作中使用同一个变量，但是 Spark会为每个任务分别发送。</p><p>在整个队列中,仅仅存在一次</p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">object BoardCast &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    val sc &#x3D; new SparkContext(new SparkConf().setAppName(&quot;emm&quot;).setMaster(&quot;local[*]&quot;))    val rdd1 &#x3D; sc.makeRDD(List( (&quot;a&quot;,1), (&quot;b&quot;, 2), (&quot;c&quot;, 3), (&quot;d&quot;, 4) ),4)    val list &#x3D; List((&quot;a&quot;,4), (&quot;b&quot;, 5), (&quot;c&quot;, 6), (&quot;d&quot;, 7))    val broadcast: Broadcast[List[(String, Int)]] &#x3D; sc.broadcast(list)    val value &#x3D; rdd1.map &#123;      case (key, num) &#x3D;&gt; &#123;        var num1 &#x3D; 0        for ((k, v) &lt;- broadcast.value) &#123;          if (k &#x3D;&#x3D; key)&#123;            num1&#x3D;v          &#125;        &#125;        (key, num+num1)      &#125;    &#125;    value.collect().foreach(println)  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="12-RDD的持久化"><a href="#12-RDD的持久化" class="headerlink" title="12.RDD的持久化"></a>12.RDD的持久化</h2><h3 id="12-1为什么要使用RDD的持久化"><a href="#12-1为什么要使用RDD的持久化" class="headerlink" title="12.1为什么要使用RDD的持久化"></a>12.1为什么要使用RDD的持久化</h3><p>数据不存储在RDD中</p><p><img src="https://i.loli.net/2020/12/25/m4gxMj5cHAh8KaG.png" alt="RDD的重用"></p><p>如果一个RDD需要重复使用,需要从头再次执行来获取数据</p><p>RDD的对象可以重用,但是数据没法重用</p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">object SparkCore02_RDD_Persist &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    val sparkConf: SparkConf &#x3D; new SparkConf().setAppName(&quot;WordCountPersist&quot;).setMaster(&quot;local[1]&quot;)    val sc &#x3D; new SparkContext(sparkConf)    val list &#x3D; List(&quot;Hello Spark&quot;, &quot;Hello Scala&quot;)    &#x2F;&#x2F;生成RDD RDD中不存储数据    val listRDD: RDD[String] &#x3D; sc.makeRDD(list, 1)    val wordRDD: RDD[String] &#x3D; listRDD.flatMap(word &#x3D;&gt; word.split(&quot; &quot;))    val tupleRDD: RDD[(String, Int)] &#x3D; wordRDD.map(word &#x3D;&gt;    &#123; println(&quot;map阶段&quot;)      (word, 1)    &#125;)    &#x2F;&#x2F;分组的操作    val groupRDD: RDD[(String, Iterable[Int])] &#x3D; tupleRDD.groupByKey()    val resultRDD: RDD[(String, Int)] &#x3D; tupleRDD.reduceByKey(_ + _)    resultRDD.collect().foreach(println)    println(&quot;------------华丽分割线----------------&quot;)    groupRDD.collect().foreach(println)    sc.stop()  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">&#x2F;&#x2F;结果map阶段map阶段map阶段map阶段(Spark,1)(Hello,2)(Scala,1)------------华丽分割线----------------map阶段map阶段map阶段map阶段(Spark,CompactBuffer(1))(Hello,CompactBuffer(1, 1))(Scala,CompactBuffer(1))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>发现map阶段运行了两波,所有的执行都会从头开始计算</p><p>这样的执行影响了效率</p><p>要想解决这个问题,数据持久化提高效率</p><p><img src="https://i.loli.net/2020/12/25/MAwm1jeOrELZWqK.png" alt="RDD持久化的作用"></p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">&#x2F;&#x2F;进行缓存tupleRDD.cache() &#x2F;&#x2F;本质是persist&#x2F;&#x2F;tupleRDD.cache()tupleRDD.persist(StorageLevel.MEMORY_AND_DISK)&#x2F;&#x2F;memory_only当内存不够的情况下,数据不能溢写到磁盘,会丢失数据&#x2F;&#x2F;memory_and_disk当内存不够的情况下,会将数据落到磁盘&#x2F;&#x2F;持久化操作,必须在行动算子执行时,完成的sc.setCheckpointDir(&quot;.&#x2F;checkPoint&quot;)&#x2F;&#x2F;一般要保存到分布式存储中tupleRDD.checkpoint()&#x2F;&#x2F;检查点路径,在作业执行完毕之后也是不会删除的<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>RDD对象的持久化操作不一定为了重用,在数据执行较长,或者数据比较重要的场合也可以进行持久化操作</p><h3 id="12-2-三种持久化方法"><a href="#12-2-三种持久化方法" class="headerlink" title="12.2 三种持久化方法"></a>12.2 三种持久化方法</h3><ol><li><p><strong>cache</strong>:将数据临时存储在内存中进行数据重用,会添加新的依赖,出现问题从头开始计算</p></li><li><p><strong>persist</strong>:将数据临时存储在磁盘文件中进行数据重用,涉及到磁盘IO,性能较低,但是数据安全,如果作业执行完毕,临时保存在数据文件就会丢失</p></li><li><p><strong>checkpoint</strong>:将磁盘长久地保存在磁盘文件中进行数据重用,涉及到磁盘IO时,性能较低,但是会切断血缘关系,相当于改变数据源</p><blockquote><p>但是数据安全,为了保证数据安全,所以一般情况下,会独立的执行作业,为了能够提高效率,一般情况下,会跟cache联合使用,先cache在使用checkpoint这个时候会保存cache的文件,而不会独立的跑一个单独的任务</p></blockquote></li></ol><pre class="mermaid">graph LRsc-->map-reduceByKey-->cache--保存-->CheckPointcache-->collect</pre><p><strong>大区别</strong></p><p>cache会添加新的依赖</p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">package com.ecust.rdd.persistimport org.apache.spark.rdd.RDDimport org.apache.spark.storage.StorageLevelimport org.apache.spark.&#123;SparkConf, SparkContext&#125;&#x2F;** * @author Jinxin Li * @create 2020-12-24 14:03 *&#x2F;object SparkCore03_RDD_CheckPoint &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    val sparkConf: SparkConf &#x3D; new SparkConf().setAppName(&quot;WordCountPersist&quot;).setMaster(&quot;local[1]&quot;)    val sc &#x3D; new SparkContext(sparkConf)    val list &#x3D; List(&quot;Hello Spark&quot;, &quot;Hello Scala&quot;)    val listRDD: RDD[String] &#x3D; sc.makeRDD(list, 1)    val wordRDD: RDD[String] &#x3D; listRDD.flatMap(word &#x3D;&gt; word.split(&quot; &quot;))    val tupleRDD: RDD[(String, Int)] &#x3D; wordRDD.map(word &#x3D;&gt; (word, 1))    tupleRDD.cache()    println(tupleRDD.toDebugString)&#x2F;&#x2F;运行之前看血缘关系&#x2F;&#x2F;    tupleRDD.persist(StorageLevel.MEMORY_AND_DISK)&#x2F;&#x2F;    sc.setCheckpointDir(&quot;.&#x2F;checkPoint&quot;)&#x2F;&#x2F;    tupleRDD.checkpoint()    val groupRDD: RDD[(String, Iterable[Int])] &#x3D; tupleRDD.groupByKey()    groupRDD.collect().foreach(println)    println(&quot;----------------------------&quot;)    println(tupleRDD.toDebugString)&#x2F;&#x2F;运行之后看血缘    sc.stop()  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">(1) MapPartitionsRDD[2] at map at SparkCore03_RDD_CheckPoint.scala:21 [Memory Deserialized 1x Replicated] |  MapPartitionsRDD[1] at flatMap at SparkCore03_RDD_CheckPoint.scala:20 [Memory Deserialized 1x Replicated] |  ParallelCollectionRDD[0] at makeRDD at SparkCore03_RDD_CheckPoint.scala:19 [Memory Deserialized 1x Replicated](Spark,CompactBuffer(1))(Hello,CompactBuffer(1, 1))(Scala,CompactBuffer(1))----------------------------(1) MapPartitionsRDD[2] at map at SparkCore03_RDD_CheckPoint.scala:21 [Memory Deserialized 1x Replicated] |       CachedPartitions: 1; MemorySize: 368.0 B; ExternalBlockStoreSize: 0.0 B; DiskSize: 0.0 B&#x2F;&#x2F;这里添加了新的依赖 |  MapPartitionsRDD[1] at flatMap at SparkCore03_RDD_CheckPoint.scala:20 [Memory Deserialized 1x Replicated] |  ParallelCollectionRDD[0] at makeRDD at SparkCore03_RDD_CheckPoint.scala:19 [Memory Deserialized 1x Replicated]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>因此cache会在血缘关系中添加新的依赖,一旦出现问题,可以重头读取数据</p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">&#x2F;&#x2F;使用checkPoint之后(1) MapPartitionsRDD[2] at map at SparkCore03_RDD_CheckPoint.scala:21 [] |  MapPartitionsRDD[1] at flatMap at SparkCore03_RDD_CheckPoint.scala:20 [] |  ParallelCollectionRDD[0] at makeRDD at SparkCore03_RDD_CheckPoint.scala:19 [](Spark,CompactBuffer(1))(Hello,CompactBuffer(1, 1))(Scala,CompactBuffer(1))----------------------------(1) MapPartitionsRDD[2] at map at SparkCore03_RDD_CheckPoint.scala:21 [] |  ReliableCheckpointRDD[4] at collect at SparkCore03_RDD_CheckPoint.scala:31 []<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>使用checkPoint会切断血缘关系,重新建立新的血缘关系等同于改变数据源</p><h3 id="12-3源码解析CheckPoint单独执行任务"><a href="#12-3源码解析CheckPoint单独执行任务" class="headerlink" title="12.3源码解析CheckPoint单独执行任务"></a>12.3源码解析CheckPoint单独执行任务</h3><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">&#x2F;&#x2F;SparkContext.scala 2093-2095dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get)    progressBar.foreach(_.finishAll())    rdd.doCheckpoint()&#x2F;&#x2F;--&gt;&#x2F;&#x2F;runJob之后调用了doCheckPoint()方法&#x2F;&#x2F;-----------------------------&#x2F;&#x2F;RDD.scala 1789-1805 doCheckPointif (checkpointData.isDefined) &#123;    if (checkpointAllMarkedAncestors) &#123;        dependencies.foreach(_.rdd.doCheckpoint())          &#125;    checkpointData.get.checkpoint()&#x2F;&#x2F;如果需要checkPoint然后进行checkPoint&#125; else &#123;    dependencies.foreach(_.rdd.doCheckpoint())&#125;&#x2F;&#x2F;----------------------------------\&#x2F;&#x2F;org.apache.spark.rdd.LocalRDDCheckpointData 53-54if (missingPartitionIndices.nonEmpty) &#123;      rdd.sparkContext.runJob(rdd, action, missingPartitionIndices)&#125;&#x2F;&#x2F;单独执行任务<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="12-4使用CheckPoint恢复计算"><a href="#12-4使用CheckPoint恢复计算" class="headerlink" title="12.4使用CheckPoint恢复计算"></a>12.4使用CheckPoint恢复计算</h3><p>checkpoint会将结果写到hdfs上，当driver 关闭后数据不会被清除。所以可以在其他driver上重复利用该checkpoint的数据。</p><p>checkpoint write data:</p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">sc.setCheckpointDir(&quot;data&#x2F;checkpoint&quot;)val rddt &#x3D; sc.parallelize(Array((1,2),(3,4),(5,6)),2)rddt.checkpoint()rddt.count() &#x2F;&#x2F;要action才能触发checkpoint<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>read from checkpoint data:</p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">package org.apache.sparkimport org.apache.spark.rdd.RDDobject RDDUtilsInSpark &#123;  def getCheckpointRDD[T](sc:SparkContext, path:String) &#x3D; &#123;  &#x2F;&#x2F;path要到part-000000的父目录    val result : RDD[Any] &#x3D; sc.checkpointFile(path)    result.asInstanceOf[T]  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><em>note:因为sc.checkpointFile(path)是private[spark]的，所以该类要写在自己工程里新建的package org.apache.spark中</em></p><p>example:</p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">val rdd : RDD[(Int, Int)]&#x3D; RDDUtilsInSpark.getCheckpointRDD(sc, &quot;data&#x2F;checkpoint&#x2F;963afe46-eb23-430f-8eae-8a6c5a1e41ba&#x2F;rdd-0&quot;)   println(rdd.count())   rdd.collect().foreach(println)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>这样就可以原样复原了。</p><p><strong>Demo</strong></p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">object SparkCore05_RDD_CheckPointUse &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    val sparkConf: SparkConf &#x3D; new SparkConf().setAppName(&quot;WordCountPersist&quot;).setMaster(&quot;local[1]&quot;)    val sc &#x3D; new SparkContext(sparkConf)&#x2F;&#x2F;使用工具类,注意工具类的包,要自己建立,注意泛型    val rdd: RDD[(String, Int)] &#x3D; RDDUtilsInSpark.getCheckpointRDD[RDD[(String, Int)]](sc, &quot;.&#x2F;checkPoint&#x2F;1186c961-ddb4-4af5-b7dc-6cc99776490b&#x2F;rdd-2&quot;)      &#x2F;&#x2F;之前的map之后reduceBykey之前的checkPoint文件    val result: RDD[(String, Int)] &#x3D; rdd.reduceByKey(_ + _)    result.collect().foreach(println)    sc.stop()  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">import org.apache.spark.rdd.RDD&#x2F;&#x2F;可以恢复checkPoint的工具类,注意放置的包object RDDUtilsInSpark &#123;  def getCheckpointRDD[T](sc: SparkContext, path: String) &#x3D; &#123;    &#x2F;&#x2F;path要到part-000000的父目录    val result: RDD[Any] &#x3D; sc.checkpointFile(path)    result.asInstanceOf[T]  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="13-RDD自定义分区器"><a href="#13-RDD自定义分区器" class="headerlink" title="13.RDD自定义分区器"></a>13.RDD自定义分区器</h2><p>Spark目前支持Hash分区、Range分区和用户自定义分区。Hash分区为当前的默认分区。分区器直接决定了RDD中分区的个数、RDD中每条数据经过Shuffle后进入哪个分区和Reduce的个数。</p><p>1）注意：</p><p>（1）只有Key-Value类型的RDD才有分区器，非Key-Value类型的RDD分区的值是None</p><p>（2）每个RDD的分区ID范围：0~numPartitions-1，决定这个值是属于那个分区的。</p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">package com.ecust.rdd.partitionimport org.apache.spark.rdd.RDDimport org.apache.spark.&#123;HashPartitioner, Partitioner, SparkConf, SparkContext&#125;&#x2F;** * @author Jinxin Li * @create 2020-12-26 10:52 * 自定义分区规则 *&#x2F;object SparkCore01_RDD_Partitioner &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    val sparkConf: SparkConf &#x3D; new SparkConf().setMaster(&quot;local&quot;).setAppName(&quot;Partitioner&quot;)    val sc &#x3D; new SparkContext(sparkConf)    val rdd: RDD[(String, String)] &#x3D; sc.makeRDD(List(      (&quot;nba&quot;, &quot;xxxxxxxxxxxxxxx&quot;),      (&quot;wba&quot;, &quot;aaaaaaaaaaaaaa&quot;),      (&quot;cba&quot;, &quot;dddddddddddd&quot;),      (&quot;wcba&quot;, &quot;ppppppppppppppppppppppp&quot;)    ), 3)    &#x2F;*    自动义分区器,决定数据去哪个分区     *&#x2F;    val rddPar: RDD[(String, String)] &#x3D; rdd.partitionBy(new MyPartitioner())    rddPar.saveAsTextFile(&quot;.&#x2F;par&quot;)    sc.stop()  &#125;  class MyPartitioner extends Partitioner&#123;    &#x2F;&#x2F;分区数量    override def numPartitions: Int &#x3D; 3    &#x2F;&#x2F;返回Int类型,返回数据的分区索引 从零开始    &#x2F;&#x2F;Key表示数据的KV到底是什么    &#x2F;&#x2F;根据数据的key值返回数据所在分区索引    override def getPartition(key: Any): Int &#x3D; &#123;      key match &#123;        case &quot;nba&quot; &#x3D;&gt; 0        case &quot;cba&quot; &#x3D;&gt; 1        case _ &#x3D;&gt; 2      &#125;      &#x2F;*if (key &#x3D;&#x3D; &quot;nba&quot;)&#123;        0      &#125; else if(key &#x3D;&#x3D; &quot;cba&quot;)&#123;        1      &#125;else&#123;        2      &#125;*&#x2F;    &#125;  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="14-RDD的存储与保存"><a href="#14-RDD的存储与保存" class="headerlink" title="14.RDD的存储与保存"></a>14.RDD的存储与保存</h2><p>Spark的数据读取及数据保存可以从两个维度来作区分：文件格式以及文件系统。</p><p>文件格式分为：Text文件、Json文件、Csv文件、Sequence文件以及Object文件；</p><p>文件系统分为：本地文件系统、HDFS以及数据库。</p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">&#x2F;&#x2F;集群文件系统存储示例hdfs:&#x2F;&#x2F;hadoop102:8020&#x2F;input&#x2F;1.txt<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h2 id="15-RDD的累加器"><a href="#15-RDD的累加器" class="headerlink" title="15.RDD的累加器"></a>15.RDD的累加器</h2><p>如果没有累加器,我们计算时只能使用reduce,要想把executor的变量拉回到Driver困难</p><p><img src="https://i.loli.net/2020/12/26/b24Xm8BYltu1PF3.png" alt="引入问题"></p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">&#x2F;&#x2F;学前测试object SparkCore02_RDD_accumulator &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    val sparkConf: SparkConf &#x3D; new SparkConf().setMaster(&quot;local&quot;).setAppName(&quot;Partitioner&quot;)    val sc &#x3D; new SparkContext(sparkConf)    val rdd: RDD[Int] &#x3D; sc.makeRDD(List(1, 2, 3, 4),1)    var sum:Int &#x3D; 0    &#x2F;&#x2F;行动算子返回非RDD    rdd.foreach(num&#x3D;&gt;&#123;      sum+&#x3D;num      println(&quot;executor:&quot;+sum)    &#125;)    println(&quot;driver:&quot;+sum)&#x2F;&#x2F;打印结果为零,Driver-&gt;executor,结果返回不了    sc.stop()  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="https://i.loli.net/2020/12/26/9PtGzVBQ8dD3sya.png" alt="累加器的主要目的"></p><p>累加器：分布式共享只写变量。（Executor和Executor之间不能读数据）</p><p>累加器用来把Executor端变量信息聚合到Driver端。在Driver程序中定义的变量，在Executor端的每个task都会得到这个变量的一份新的副本，每个task更新这些副本的值后，传回Driver端进行merge。</p><p><strong>Long累加器Demo</strong></p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">object SparkCore03_RDD_accumulator &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    val sparkConf: SparkConf &#x3D; new SparkConf().setMaster(&quot;local&quot;).setAppName(&quot;Partitioner&quot;)    val sc &#x3D; new SparkContext(sparkConf)    val rdd: RDD[Int] &#x3D; sc.makeRDD(List(1, 2, 3, 4),1)    &#x2F;&#x2F;todo 自定义累加器    val sum: LongAccumulator &#x3D; sc.longAccumulator(&quot;sum&quot;)    &#x2F;&#x2F;系统自带了一些累加器&#x2F;&#x2F;    sc.doubleAccumulator&#x2F;&#x2F;    sc.collectionAccumulator()    rdd.foreach(num&#x3D;&gt;sum.add(num))    println(&quot;driver:&quot;+sum.value)    sc.stop()  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>特殊情况</strong></p><p>少加:转换算子中调用累加器,如果没有行动算子的话,name不会执行</p><p>多加:转换算子中调用累加器,多次行动算子会调用多次,一般会放在行动算子中进行操作</p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">object SparkCore04_RDD_accumulator &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    val sparkConf: SparkConf &#x3D; new SparkConf().setMaster(&quot;local&quot;).setAppName(&quot;Partitioner&quot;)    val sc &#x3D; new SparkContext(sparkConf)    val rdd: RDD[Int] &#x3D; sc.makeRDD(List(1, 2, 3, 4),1)    &#x2F;&#x2F;todo 自定义累加器    val sum: LongAccumulator &#x3D; sc.longAccumulator(&quot;sum&quot;)    &#x2F;&#x2F;系统自带了一些累加器&#x2F;&#x2F;    sc.doubleAccumulator&#x2F;&#x2F;    sc.collectionAccumulator()    val result: RDD[Unit] &#x3D; rdd.map(num &#x3D;&gt; sum.add(num))        result.collect()    result.collect()&#x2F;&#x2F;两个行动算子会多加    println(&quot;driver:&quot;+sum.value)    sc.stop()  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="16-RDD的自定义广播变量"><a href="#16-RDD的自定义广播变量" class="headerlink" title="16.RDD的自定义广播变量"></a>16.RDD的自定义广播变量</h2><p>分布式共享只写变量</p><p>表示累加器的值互相之间是没法访问的,自己能读自己,只有Driver进行读到,然后在Driver端进行合并</p><p>我们可以将一些Shuffle的东西使用累加器来实现(==优化==)</p><p>比方:需要shuffle的方法就不要shuffle了</p><p>闭包数据,都是以Task为单位发送的,每个人物中包含的闭包数据这样可能会导致,一个Executor中含有大量的重复的数据,并且占用大量的内存</p><p>Executor本质其实就是JVM,所以在启动时,会自动分配内存</p><p> 完全可以将任务中的闭包数据放置到Executor的内存中,达到共享的目的</p><p>Spark中的广播变量可以将闭包的数据保存在Executor的内存中</p><p>分布式共享只读变量</p><pre class="mermaid">graph TDmap-->Executor/task1map-->Executor/task2map-->Executor/task3</pre><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">object SparkCore04_RDD_BroadCast &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    val sparkConf: SparkConf &#x3D; new SparkConf().setMaster(&quot;local&quot;).setAppName(&quot;acc&quot;)    val sc &#x3D; new SparkContext(sparkConf)    val rdd1: RDD[(String, Int)] &#x3D; sc.makeRDD(List((&quot;a&quot;, 1), (&quot;b&quot;, 2), (&quot;c&quot;, 3)))    val map: mutable.Map[String, Int] &#x3D; mutable.Map((&quot;a&quot;, 4), (&quot;b&quot;, 5), (&quot;c&quot;, 6))    &#x2F;&#x2F;定义广播变量    val value: Broadcast[mutable.Map[String, Int]] &#x3D; sc.broadcast(map)    &#x2F;&#x2F;每个task都有一份数据    val result: RDD[(String, (Int, Int))] &#x3D; rdd1.map &#123; case (w, c) &#x3D;&gt; &#123;      val i: Int &#x3D; value.value.getOrElse(w, 0)      (w, (c, i))    &#125;&#125;    result.collect().foreach(println)    sc.stop()  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark_SparkSQL</title>
      <link href="2021/01/03/Spark_SparkSQL/"/>
      <url>2021/01/03/Spark_SparkSQL/</url>
      
        <content type="html"><![CDATA[<h1 id="SparkSQL-Abstract"><a href="#SparkSQL-Abstract" class="headerlink" title="SparkSQL_Abstract"></a>SparkSQL_Abstract</h1><h2 id="什么是Spark-SQL"><a href="#什么是Spark-SQL" class="headerlink" title="什么是Spark SQL"></a>什么是Spark SQL</h2><p>Spark SQL是Spark用于结构化数据(Structured Data) 处理的Spark模块</p><p>Spark SQL的底层实现方式是<code>DataFrame API</code> 和 <code>DataSets API</code> </p><p>Spark SQL 运行在 Spark Core 之上。它允许开发人员从 Hive 表和 Parquet 文件中导入关系数据，在导入的数据和现有 rdd 上运行 SQL 查询，并轻松地将 rdd 写到 Hive 表或 Parquet 文件中。</p><p>Spark SQL 引入了称为 <code>Catalyst</code> 的可扩展优化器，因为它有助于在 Bigdata 支持广泛的数据源和算法。</p><h2 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h2><p>DataFrame是一种以RDD为基础的分布式数据集，类似于传统数据库中的二维表格</p><h3 id="0x0-主要区别"><a href="#0x0-主要区别" class="headerlink" title="0x0 主要区别"></a>0x0 主要区别</h3><p>DataFrame也是懒执行的</p><p>DataFrame与RDD的<strong>主要区别</strong>在于，前者带有schema元信息，即DataFrame所表示的二维表数据集的每一列都带有名称和类型。</p><p><img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_20_12/DataFrame%E4%B8%8ERDD%E7%9A%84%E5%8C%BA%E5%88%AB.png" alt="DataFrame与RDD的区别"></p><h3 id="0x1-DataFrame优势"><a href="#0x1-DataFrame优势" class="headerlink" title="0x1 DataFrame优势"></a>0x1 DataFrame优势</h3><p>提供内存管理和执行优化。</p><p>自定义内存管理: 这也被称为项目 <strong>Tungsten</strong>钨。由于数据以二进制格式存储在off-heap memory中，因此节省了大量内存。除此之外，没有垃圾收集开销。</p><p>优化执行计划: 这也称为查询优化器。使用这个，可以为查询的执行创建一个优化的执行计划。一旦创建了优化的计划，最终在 Spark 的 RDDs 上执行。</p><h3 id="0x2-优化器Catalyst"><a href="#0x2-优化器Catalyst" class="headerlink" title="0x2 优化器Catalyst"></a>0x2 优化器Catalyst</h3><ul><li>Analyze logical plan to solve references 分析逻辑计划以解决引用</li><li>Logical plan optimization 逻辑计划优化</li><li>Physical planning 物理规划</li><li>Code generation to compile part of a query to Java bytecode.把一部分代码转换为字节码文件</li></ul><p><img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_20_12/SparkSQL%E4%BC%98%E5%8C%96%E6%A1%88%E4%BE%8B.png" alt="SparkSQL优化案例"></p><p>如图所示,底层执行了一些优化策略,举个最简单的例子,两个RDD的数据源想做个连接,连接之后对数据进行过滤,这个时候可能会有产生笛卡尔乘积,而且再数据量大的情况下存在shuffle来说,性能会大大下降,甚至超过内存无法进行计算</p><p>DF底层会根据逻辑先进行filter然后再进行join,大大减少了数据量</p><h2 id="DataSet"><a href="#DataSet" class="headerlink" title="DataSet"></a>DataSet</h2><p>1.6版本后新的抽象</p><p>DataSet是分布式数据集合。</p><p>DataSet是强类型的。比如可以有DataSet[Car]，DataSet[User]。具有类型安全检查</p><p><code>DataFrame</code>是<code>DataSet</code>的特例,<code>type DataFrame = DataSet[Row]</code> ，<code>Row</code>是一个类型，跟Car、User这些的类型一样，所有的表结构信息都用Row来表示。</p><h3 id="0x0-为什么是DataSet"><a href="#0x0-为什么是DataSet" class="headerlink" title="0x0 为什么是DataSet"></a>0x0 为什么是DataSet</h3><p>DataFrame虽然定义了保存了表结构的原信息</p><pre class="line-numbers language-java" data-language="java"><code class="language-java">ResultSet rs &#x3D; pstat.executeQuery(&quot;select id,name from user&quot;);while (rs.next())&#123;    rs.getInt(1)&#x2F;&#x2F;我们知道是id,但是如果我们sql改变顺序,        &#x2F;&#x2F;比如name,id        &#x2F;&#x2F;调用结果就得修改&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>所以spark就对DataFrame的里面的表结构封装成一个对象,直接使用对象点的方式进行调用,对象的类型就设置为Row类型</p><p>DataSet(row) = DataFrame</p><h3 id="0x1-Encoder"><a href="#0x1-Encoder" class="headerlink" title="0x1 Encoder"></a>0x1 Encoder</h3><p>Encoder编码器是 Spark SQL 中序列化和反序列化(SerDes)框架的基本概念。编码器在<strong>对象</strong>和 Spark 的内部<strong>二进制格式</strong>之间进行转换</p><h2 id="SparkSession"><a href="#SparkSession" class="headerlink" title="SparkSession"></a>SparkSession</h2><p>在老的版本中，SparkSQL提供两种SQL查询起始点：</p><p>一个叫SQLContext，用于Spark自己提供的SQL查询；</p><p>一个叫HiveContext，用于连接Hive的查询。</p><p>SparkSession是Spark最新的SQL查询起始点，实质上是SQLContext和HiveContext的组合，所以在SQLContext和HiveContext上可用的API在SparkSession上同样是可以使用的。</p><p>SparkSession内部封装了SparkContext，所以计算实际上是由SparkContext完成的。当我们使用spark-shell的时候，Spark框架会自动的创建一个名称叫做Spark的SparkSession，就像我们以前可以自动获取到一个sc来表示SparkContext。</p><h2 id="DS-DF-RDD转换"><a href="#DS-DF-RDD转换" class="headerlink" title="DS-DF-RDD转换"></a>DS-DF-RDD转换</h2><p><img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_21_01/DataFrame%E4%B8%8ERDD%E7%9A%84%E8%BD%AC%E6%8D%A2%E5%85%B3%E7%B3%BB%E5%9B%BE.png" alt="DataFrame与RDD的转换关系图"></p><p>￼</p><p>由DataFrame转换过来的RDD的是Row类型</p><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql">--1.RDD &lt;&#x3D;&gt; DF--a. RDD --&gt; DF rdd.toDF(&quot;列名1&quot;，&quot;列名2&quot;，...)--b. DF --&gt; RDDdf.rdd--2.RDD &lt;&#x3D;&gt; DS--a、RDD --&gt; DS--将rdd的数据转换为样例类的格式。    rdd.toDS --这里声明一点--rdd如果是字符串创建来的,是没有能力toDS的--这里要实现把对象准备好val rdd &#x3D; sc.makeRDD(List(Emp(30,&quot;张三&quot;),Emp(40,&quot;李四&quot;))rdd.toDS--这样才能真正的转变--b. DS --&gt; RDDds.rdd-- 3.DF &lt;&#x3D;&gt; DS--a. DF --&gt; DS     df.as[样例类]--该样例类必须存在，而且df中的数据个样例类对应--b. DS --&gt; DSds.toDF<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_21_01/%E4%B8%89%E8%80%85%E8%BD%AC%E6%8D%A2%E5%9B%BE.png" alt="DS与DF,RDD转换图"></p><h1 id="SparkSQL-API"><a href="#SparkSQL-API" class="headerlink" title="SparkSQL_API"></a>SparkSQL_API</h1><h2 id="IDEA中使用SparkSQL"><a href="#IDEA中使用SparkSQL" class="headerlink" title="IDEA中使用SparkSQL"></a>IDEA中使用SparkSQL</h2><h3 id="0x0添加依赖"><a href="#0x0添加依赖" class="headerlink" title="0x0添加依赖"></a>0x0添加依赖</h3><pre class="line-numbers language-xml" data-language="xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>org.apache.spark<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>spark-sql_2.12<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>2.4.5<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="0x1构建sparkSession对象"><a href="#0x1构建sparkSession对象" class="headerlink" title="0x1构建sparkSession对象"></a>0x1构建sparkSession对象</h3><ol><li>重要：连接SparkSQL</li></ol><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">&#x2F;&#x2F; 1. 创建环境val sparkConf: SparkConf &#x3D; new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;sparksql&quot;)&#x2F;&#x2F; 2. 创建SparkSession对象val spark: SparkSession &#x3D; SparkSession.builder().config(sparkConf).getOrCreate()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><ol start="2"><li>添加隐式转换，每次构建完对象以后都需要增加这个 隐式转换的代码</li></ol><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala"> &#x2F;&#x2F; 3. 增加隐式转换    import spark.implicits._&quot;1. 这里的spark不是Scala中的包名，而是创建的sparkSession对象的变量名称 2. spark对象不能使用var声明，因为Scala只支持val修饰的对象的引入&quot;&#x2F;&#x2F;为什么要导入这个对象的隐式转换呢,为了方便调用 $age等<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ol start="3"><li>说明</li></ol><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql">-- 为啥要导入隐式转换sparkSQL是在spark的基础上进行延伸，属于功能的扩展，使用隐式转换，体现了OCP开发原则。--构建对象为什么不直接new呢？因为sparkSession是对sparkContext的包装，创建这个对象时，需要很多步骤，将这些过程进行封装，让开发更容易，使用一个构建器来创建对象。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="0x3代码实现"><a href="#0x3代码实现" class="headerlink" title="0x3代码实现"></a>0x3代码实现</h3><p>直接从SparkSQL里看</p><p>这里仅仅展示一个示例</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">git clone git@github.com:fourgold&#x2F;Spark.git<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">object SparkSQL01_DSL &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    &#x2F;&#x2F;创建SparkSQL的运行环境    val sparkConf: SparkConf &#x3D; new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;BASIC&quot;)    val sparkSession: SparkSession &#x3D; SparkSession.builder().config(sparkConf).getOrCreate()    &#x2F;&#x2F;DataFrame    val dataFrame: DataFrame &#x3D; sparkSession.read.json(&quot;.&#x2F;input&#x2F;user.json&quot;)&#x2F;&#x2F;    dataFrame.show()    &#x2F;&#x2F;DSL语句使用调用方法,类似于Flink总的TABLE API与SQL API    dataFrame.select(&quot;name&quot;,&quot;age&quot;).show()    import sparkSession.implicits._    dataFrame.select($&quot;age&quot;+1).as(&quot;age&quot;).show()    &#x2F;&#x2F;怎么选择两列    &#x2F;&#x2F;可以使用单引号代表引用    dataFrame.select(&#39;age+1).as(&quot;age&quot;).show()    &#x2F;&#x2F;关闭环境    sparkSession.close()  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="自定义UDF函数"><a href="#自定义UDF函数" class="headerlink" title="自定义UDF函数"></a>自定义UDF函数</h2><p>需求:将一个字段的string,加一个前缀</p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">object Demo02_Practice &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    val spark: SparkSession &#x3D; SparkSession.builder().config(new SparkConf().setMaster(&quot;local[1]&quot;).setAppName(&quot;SparkSQL&quot;)).getOrCreate()    &#x2F;&#x2F;用户自定函数    import spark.implicits._    val df: DataFrame &#x3D; spark.read.json(&quot;Day08&#x2F;input&#x2F;person.json&quot;)    df.createOrReplaceTempView(&quot;user&quot;)    df.show()    &#x2F;&#x2F;自定义udf函数    spark.udf.register(&quot;addName&quot;,(name:String)&#x3D;&gt;&quot;name:&quot;+name)    spark.sql(&quot;select age,addName(username) from user&quot;).show()  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="自定义UDAF函数"><a href="#自定义UDAF函数" class="headerlink" title="自定义UDAF函数"></a>自定义UDAF函数</h2><p>需求:</p><p><img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_21_01/%E8%87%AA%E5%AE%9AUDAF%E5%87%BD%E6%95%B0%E5%AE%9E%E7%8E%B0%E9%9C%80%E6%B1%82.png" alt="自定义UDAF函数实现需求"></p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">package com.SparkSqlimport org.apache.spark.sql.Rowimport org.apache.spark.sql.expressions.&#123;MutableAggregationBuffer, UserDefinedAggregateFunction&#125;import org.apache.spark.sql.types.&#123;DataType, DoubleType, LongType, StructField, StructType&#125;class MyAvg extends UserDefinedAggregateFunction&#123;  &#x2F;&#x2F;输入类型  override def inputSchema: StructType &#x3D; StructType(Array(StructField(&quot;age&quot;,LongType)))  &#x2F;&#x2F;缓冲区  override def bufferSchema: StructType &#x3D;StructType(Array(StructField(&quot;sum&quot;,LongType),StructField(&quot;count&quot;,LongType)))  &#x2F;&#x2F;返回值的数据类型  override def dataType: DataType &#x3D; DoubleType  &#x2F;&#x2F;稳定性：对于相同的输入是否一直返回相同的输出。  override def deterministic: Boolean &#x3D; true  &#x2F;&#x2F;缓冲区的初始化  override def initialize(buffer: MutableAggregationBuffer): Unit &#x3D; &#123;    buffer(0) &#x3D; 0L&#x2F;&#x2F;sum    buffer(1) &#x3D; 0L&#x2F;&#x2F;count  &#125;  &#x2F;&#x2F; 更新缓冲区中的数据  override def update(buffer: MutableAggregationBuffer, input: Row): Unit &#x3D; &#123;    if (!input.isNullAt(0)) &#123;      buffer(0) &#x3D; buffer.getLong(0) + input.getLong(0)      buffer(1) &#x3D; buffer.getLong(1) + 1    &#125;  &#125;  &#x2F;&#x2F; 合并缓冲区  override def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit &#x3D; &#123;    buffer1(0) &#x3D; buffer1.getLong(0) + buffer2.getLong(0)    buffer1(1) &#x3D; buffer1.getLong(1) + buffer2.getLong(1)  &#125;  override def evaluate(buffer: Row): Any &#x3D; buffer.getLong(0).toDouble&#x2F;buffer.getLong(1).toDouble&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>聚合函数的使用</p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">&#x2F;&#x2F;使用package com.SparkSqlimport org.apache.spark.SparkConfimport org.apache.spark.sql.expressions.UserDefinedAggregateFunctionimport org.apache.spark.sql.&#123;DataFrame, SparkSession&#125;&#x2F;** * @author Jinxin Li * @create 2020-11-03 16:55 *&#x2F;object Demo02_Practice &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    val spark: SparkSession &#x3D; SparkSession.builder().config(new SparkConf().setMaster(&quot;local[1]&quot;).setAppName(&quot;SparkSQL&quot;)).getOrCreate()    &#x2F;&#x2F;用户自定函数    import spark.implicits._    val df: DataFrame &#x3D; spark.read.json(&quot;Day08&#x2F;input&#x2F;person.json&quot;)    df.createOrReplaceTempView(&quot;user&quot;)    df.show()    &#x2F;&#x2F;自定义udf函数    spark.udf.register(&quot;addName&quot;,(name:String)&#x3D;&gt;&quot;name:&quot;+name)    spark.sql(&quot;select age,addName(username) from user&quot;).show()    &#x2F;&#x2F;在spark中注册聚合函数&#x3D;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;    var MyAvg &#x3D; new MyAvg    spark.udf.register(&quot;avgAge&quot;,MyAvg)    spark.sql(&quot;select avgAge(age) from user&quot;).show()    spark.close()  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="自定义强类型AggregateUDAF函数"><a href="#自定义强类型AggregateUDAF函数" class="headerlink" title="自定义强类型AggregateUDAF函数"></a>自定义强类型AggregateUDAF函数</h2><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">package com.SparkSqlimport org.apache.spark.sql.&#123;Encoder, Encoders&#125;import org.apache.spark.sql.expressions.Aggregator&#x2F;** * @author Jinxin Li * @create 2020-11-03 19:16 * 求user的平均年龄 * 1.继承 * 2.定义泛型 * In 输入数据类型 * Buf * out Double输出的数据类型 * 3.重写方法(6) *&#x2F;case class Buff(var total:Long,var count:Long)class MyAvg2 extends Aggregator[Long,Buff,Double]&#123;  &#x2F;&#x2F;scala用zero,初始值,零值  &#x2F;&#x2F;缓冲区的初始化  override def zero: Buff &#x3D; &#123;    Buff(0L,0L)  &#125;  &#x2F;&#x2F;根据输入的数据更新缓冲区的数据  override def reduce(b: Buff, a: Long): Buff &#x3D; &#123;    b.total +&#x3D; a    b.count +&#x3D; 1    b  &#125;  override def merge(b1: Buff, b2: Buff): Buff &#x3D; &#123;    b1.count +&#x3D; b2.count    b1.total +&#x3D; b2.total    b1  &#125;  override def finish(reduction: Buff): Double &#x3D; reduction.total.toDouble&#x2F;reduction.count.toDouble  override def bufferEncoder: Encoder[Buff] &#x3D; Encoders.product  override def outputEncoder: Encoder[Double] &#x3D; Encoders.scalaDouble&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="0x0注册"><a href="#0x0注册" class="headerlink" title="0x0注册"></a>0x0注册</h3><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">val MyAvg2 &#x3D; new MyAvg2    spark.udf.register(&quot;avgAge1&quot;, functions.udaf(MyAvg2))    spark.sql(&quot;select avgAge1(age) from user&quot;).show()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="0x1使用"><a href="#0x1使用" class="headerlink" title="0x1使用"></a>0x1使用</h3><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">&#x2F;** * 弱类型操作,只有0,1 没有类型的概念 * 没有类型的概念 * 强类型通过属性操作,跟属性没关系 * 自定属性类,定义泛型 *&#x2F;object SparkSQL03_UDAF &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    &#x2F;&#x2F;创建SparkSQL的运行环境    val sparkConf: SparkConf &#x3D; new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;BASIC&quot;)    val sparkSession: SparkSession &#x3D; SparkSession.builder().config(sparkConf).getOrCreate()    &#x2F;&#x2F;聚合函数也是比较重要的,比如,平均值,最大值,最小值    &#x2F;&#x2F;DataFrame    val dataFrame: DataFrame &#x3D; sparkSession.read.json(&quot;.&#x2F;input&#x2F;user.json&quot;)&#x2F;&#x2F;    dataFrame.show()    &#x2F;&#x2F;将数据创建临时表    dataFrame.createOrReplaceTempView(&quot;user&quot;)    &#x2F;&#x2F;view只能查不能改    sparkSession.udf.register(&quot;prefixName&quot;,(name:String)&#x3D;&gt;&#123;&quot;name+&quot;+name&#125;)    &#x2F;&#x2F;将某一字段的名字加上前缀    sparkSession.sql(      &quot;&quot;&quot;        |select prefixName(name),age from user        |&quot;&quot;&quot;.stripMargin).show()    &#x2F;&#x2F;使用udaf-aggregator函数    val myAvg &#x3D; new MyAvg()    sparkSession.udf.register(&quot;myAvg&quot;,functions.udaf(new MyAvgAgg))    sparkSession.sql(      &quot;&quot;&quot;        |select myAvg(age) as avgAge from user        |&quot;&quot;&quot;.stripMargin).show()    &#x2F;&#x2F;关闭环境    sparkSession.close()  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="SparkSQL-Data"><a href="#SparkSQL-Data" class="headerlink" title="SparkSQL_Data"></a>SparkSQL_Data</h1><h2 id="加载数据"><a href="#加载数据" class="headerlink" title="加载数据"></a>加载数据</h2><p>如果不指定加载类型,默认的保存与加载类型是parquet</p><p>spark.read.load 是加载数据的通用方法</p><p>如果读取不同格式的数据，可以对不同的数据格式进行设定</p><p>我们前面都是使用 read API 先把文件加载到 DataFrame 然后再查询，其实，我们也可以直接在文件上查询</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">scala&gt; spark.read. scala&gt; spark.read.format(&quot;…&quot;)[.option(&quot;…&quot;)].load(&quot;…&quot;) #format(&quot;…&quot;)：指定加载的数据类型，包括&quot;csv&quot;、&quot;jdbc&quot;、&quot;json&quot;、&quot;orc&quot;、&quot;parquet&quot;和&quot;textFile&quot;。#load(&quot;…&quot;)：在&quot;csv&quot;、&quot;jdbc&quot;、&quot;json&quot;、&quot;orc&quot;、&quot;parquet&quot;和&quot;textFile&quot;格式下需要传入加载数据的路径。 #option(&quot;…&quot;)：在&quot;jdbc&quot;格式下需要传入 JDBC 相应参数，url、user、password 和 dbtable #直接在文件上进行查询:  文件格式.&#96;文件路径&#96; scala&gt; spark.sql(&quot;select * from json.&#96;&#x2F;opt&#x2F;module&#x2F;data&#x2F;user.json&#96;&quot;).show <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>实战</strong>:JSON与Parquet文件的读取与保存</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">spark.read.load()#默认情况下读取的格式是parquet文件val df &#x3D; spark.read.load(&quot;&#x2F;opt&#x2F;module&#x2F;spark&#x2F;examples&#x2F;src&#x2F;main&#x2F;resources&#x2F;users.parquet&quot;)#格式,例子的地方#&#x2F;opt&#x2F;module&#x2F;spark&#x2F;examples&#x2F;src&#x2F;main&#x2F;resources#保存数据 SparkSQL默认读取或者保存的文件格式parquetdf.write.save(&quot;output&quot;)#就想读json文件val df &#x3D; spark.read.format(&quot;json&quot;).load(&quot;data&#x2F;user.json&quot;)#比较简单的json文件spark.read.json()#保存json文件df.write.format(&quot;json&quot;).save(&quot;output1&quot;)#选择表 转换过程由spark自己完成 注意使用飘号spark.sql(&quot;select * from json.&#96;data&#x2F;user.json&#96;&quot;).show<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="保存数据"><a href="#保存数据" class="headerlink" title="保存数据"></a>保存数据</h2><p>df.write.save 是保存数据的通用方法 </p><p>scala&gt;df.write. csv  jdbc   json  orc   parquet textFile… … 如果保存不同格式的数据，可以对不同的数据格式进行设定 </p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">scala&gt;df.write. csv  jdbc   json  orc   parquet textFile… … <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p><img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_21_01/SparkSQL%E6%95%B0%E6%8D%AE%E4%BF%9D%E5%AD%98%E6%A0%BC%E5%BC%8F.png" alt="SparkSQL数据的保存"></p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">scala&gt;df.write.format(&quot;…&quot;)[.option(&quot;…&quot;)].save(&quot;…&quot;) # format(&quot;…&quot;)：指定保存的数据类型，包括&quot;csv&quot;、&quot;jdbc&quot;、&quot;json&quot;、&quot;orc&quot;、&quot;parquet&quot;和&quot;textFile&quot;。 # save (&quot;…&quot;)：在&quot;csv&quot;、&quot;orc&quot;、&quot;parquet&quot;和&quot;textFile&quot;格式下需要传入保存数据的路径。 # option(&quot;…&quot;)：在&quot;jdbc&quot;格式下需要传入 JDBC 相应参数，url、user、password 和 dbtable # 保存操作可以使用 SaveMode, 用来指明如何处理数据，使用 mode()方法来设置。 有一点很重要: 这些 SaveMode 都是没有加锁的, 也不是原子操作。 #SaveMode 是一个枚举类，其中的常量包括： #SaveMode.ErrorIfExists(default) &quot;error&quot;(default) 如果文件已经存在则抛出异常 #SaveMode.Append &quot;append&quot; 如果文件已经存在则追加 #SaveMode.Overwrite &quot;overwrite&quot; 如果文件已经存在则覆盖 #SaveMode.Ignore &quot;ignore&quot; 如果文件已经存在则忽略 df.write.mode(&quot;append&quot;).json(&quot;&#x2F;opt&#x2F;module&#x2F;data&#x2F;output&quot;) .save(&quot;output&quot;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="JSON-Parquet-CSV"><a href="#JSON-Parquet-CSV" class="headerlink" title="JSON/Parquet/CSV"></a>JSON/Parquet/CSV</h2><h3 id="Parquet"><a href="#Parquet" class="headerlink" title="Parquet"></a>Parquet</h3><p>Spark SQL 的默认数据源为 Parquet 格式。Parquet 是一种能够有效存储嵌套数据的列式存储格式。<br>数据源为 Parquet 文件时，Spark SQL 可以方便的执行所有的操作，不需要使用 format。<br>修改配置项 spark.sql.sources.default，可修改默认数据源格式。 </p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">scala&gt; val df &#x3D; spark.read.load(&quot;examples&#x2F;src&#x2F;main&#x2F;resources&#x2F;users.parquet&quot;) scala&gt; df.showscala&gt; var df &#x3D; spark.read.json(&quot;&#x2F;opt&#x2F;module&#x2F;data&#x2F;input&#x2F;people.json&quot;) #保存为 parquet 格式 scala&gt; df.write.mode(&quot;append&quot;).save(&quot;&#x2F;opt&#x2F;module&#x2F;data&#x2F;output&quot;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="JSON"><a href="#JSON" class="headerlink" title="JSON"></a>JSON</h3><p>Spark SQL 能够自动推测 JSON 数据集的结构，并将它加载为一个 Dataset[Row]. 可以通过 SparkSession.read.json()去加载 JSON 文件。 </p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">#将读取的文件保存val df &#x3D; spark.read.json(&quot;data&#x2F;user.json&quot;)#保存 因为保存模式的原因,再次保存会报错df.write.format(&quot;json&quot;).save(&quot;output&quot;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>注意：Spark 读取的 JSON 文件不是传统的 JSON 文件，每一行都应该是一个 JSON 串。格式如下： </p><pre class="line-numbers language-json" data-language="json"><code class="language-json">&#123;&quot;name&quot;:&quot;Michael&quot;&#125; &#123;&quot;name&quot;:&quot;Andy&quot;， &quot;age&quot;:30&#125; [&#123;&quot;name&quot;:&quot;Justin&quot;， &quot;age&quot;:19&#125;,&#123;&quot;name&quot;:&quot;Justin&quot;， &quot;age&quot;:19&#125;] #因为Spark读取是一行一行读的,所以一行应该是一个标准的json文件<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h3 id="CSV"><a href="#CSV" class="headerlink" title="CSV"></a>CSV</h3><p>Spark SQL 可以配置 CSV 文件的列表信息，读取 CSV 文件,CSV 文件的第一行设置为数据列 spark</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">#option例子是分号 optipn-分隔符 header-表头 inferSchema-??spark.read.format(&quot;csv&quot;).option(&quot;sep&quot;, &quot;;&quot;).option(&quot;inferSchema&quot;, &quot;true&quot;).option(&quot;header&quot;, &quot;true&quot;).load(&quot;data&#x2F;user.csv&quot;) <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h2 id="HIVE"><a href="#HIVE" class="headerlink" title="HIVE"></a>HIVE</h2><p>Apache Hive 是 Hadoop 上的 SQL 引擎，Spark SQL 编译时可以包含 Hive 支持，也可以不包含。</p><p>包含 Hive 支持的 Spark SQL 可以支持 Hive 表访问、UDF (用户自定义函数)以及 Hive 查询语言(HiveQL/HQL)等。</p><p>SparkSQL想连接HIVE有两种连接方式</p><h3 id="内置Hive"><a href="#内置Hive" class="headerlink" title="内置Hive"></a>内置Hive</h3><p>SparkSQL本身也具有元数据,数据仓库,全部都有,通过内置Hive实现</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">#显示全部表spark.sql(&quot;show tables&quot;).show<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>发现内部文件系统自动生成了metadb</p><p><img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_21_01/Spark%E5%86%85%E9%83%A8%E6%96%87%E4%BB%B6.png" alt="Spark内置文件系统"></p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"># 导入数据val df &#x3D; spark.read.json(&quot;data&#x2F;user.json&quot;)# 创建表spark.sql(&quot;create table test(age int)&quot;)#查看表spark.sql(&quot;show tables&quot;).show#加载数据spark.sql(&quot;load data local inpath &#39;data&#x2F;user.text&#39; into table test&quot;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_21_01/spark%E5%86%85%E7%BD%AEhive%E7%9A%84warehouse.png" alt="spark-sql数据仓库"></p><h3 id="外置Hive"><a href="#外置Hive" class="headerlink" title="外置Hive"></a>外置Hive</h3><p>一般使用外置Hive如果想连接外部已经部署好的 Hive，需要通过以下几个步骤： </p><ol><li> Spark 要接管 Hive 需要把 hive-site.xml 拷贝到 conf/目录下 </li><li> 把 Mysql 的驱动 copy 到 jars/目录下 </li><li> 如果访问不到 hdfs，则需要把 core-site.xml 和 hdfs-site.xml 拷贝到 conf/目录下 </li><li> 重启 spark-shell </li></ol><p>然后查看一下数据库</p><p>已经连接到Hive了</p><h3 id="Beeline"><a href="#Beeline" class="headerlink" title="Beeline"></a>Beeline</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">bin&#x2F;spark-sql#为什么带有(default)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p><img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_21_01/spark-sql.png" alt="spark-sql显示bug"></p><p>Spark Thrift Server 是 Spark 社区基于 HiveServer2 实现的一个 Thrift 服务。旨在无缝兼容HiveServer2。</p><p>因为 Spark Thrift Server 的接口和协议都和 HiveServer2 完全一致，因此我们部署好 Spark Thrift Server 后，可以直接使用 hive 的 beeline 访问 Spark Thrift Server 执行相关语句。</p><p>Spark Thrift Server 的目的也只是取代 HiveServer2，因此它依旧可以和 Hive Metastore进行交互，获取到 hive 的元数据。 </p><p>如果想连接 Thrift Server，需要通过以下几个步骤： </p><ol><li><p>Spark 要接管 Hive 需要把 hive-site.xml 拷贝到 conf/目录下  </p></li><li><p> 把 Mysql 的驱动 copy 到 jars/目录下 </p></li><li><p>如果访问不到 hdfs，则需要把 core-site.xml 和 hdfs-site.xml 拷贝到 conf/目录下 </p></li><li><p>启动 Thrift Server</p></li></ol><p><strong>beeline客户端连接</strong></p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">sbin&#x2F;start-thriftserver.sh # 使用 beeline 连接 Thrift Server bin&#x2F;beeline -u jdbc:hive2:&#x2F;&#x2F;linux1:10000 -n root <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>代码操作</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">#注意：在开发工具中创建数据库默认是在本地仓库，通过参数修改数据库仓库的地址: config(&quot;spark.sql.warehouse.dir&quot;, &quot;hdfs:&#x2F;&#x2F;linux1:8020&#x2F;user&#x2F;hive&#x2F;warehouse&quot;) <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>权限问题</p><pre class="line-numbers language-java" data-language="java"><code class="language-java">System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;root&quot;)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> sparkSQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Jetbrains系列产品重置试用方法</title>
      <link href="2020/12/28/Jetbrains%E7%B3%BB%E5%88%97%E4%BA%A7%E5%93%81%E9%87%8D%E7%BD%AE%E8%AF%95%E7%94%A8%E6%96%B9%E6%B3%95/"/>
      <url>2020/12/28/Jetbrains%E7%B3%BB%E5%88%97%E4%BA%A7%E5%93%81%E9%87%8D%E7%BD%AE%E8%AF%95%E7%94%A8%E6%96%B9%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h2 id="0x0-项目背景"><a href="#0x0-项目背景" class="headerlink" title="0x0. 项目背景"></a>0x0. 项目背景</h2><p>Jetbrains家的产品有一个很良心的地方，他会允许你试用<code>30</code>天（这个数字写死在代码里了）以评估是否你真的需要为它而付费。<br>但很多时候会出现一种情况：<strong>IDE并不能按照我们实际的试用时间来计算。</strong><br>我举个例子：如果我们开始了试用，然后媳妇生孩子要你回去陪产！陪产时我们并无空闲对IDE试用评估，它依旧算试用时间。（只是举个例子，或许你并没有女朋友）<br>发现了吗？你未能真的有<code>30</code>天来对它进行全面的试用评估，你甚至无法作出是否付费的决定。此时你会想要延长试用时间，然而Jetbrains并未提供相关功能，该怎么办？</p><p>事实上有一款插件可以实现这个功能，你或许可以用它来重置一下试用时间。<strong>但切记不要无休止的一直试用，这并不是这个插件的本意！</strong></p><h2 id="0x1-如何安装"><a href="#0x1-如何安装" class="headerlink" title="0x1. 如何安装"></a>0x1. 如何安装</h2><h3 id="1-插件市场安装："><a href="#1-插件市场安装：" class="headerlink" title="1). 插件市场安装："></a>1). 插件市场安装：</h3><ul><li>在<code>Settings/Preferences...</code> -&gt; <code>Plugins</code> 内手动添加第三方插件仓库地址：<code>https://plugins.zhile.io</code></li><li>搜索：<code>IDE Eval Reset</code>插件进行安装。如果搜索不到请注意是否做好了上一步？网络是否通畅？</li><li>插件会提示安装成功。</li></ul><h3 id="2-下载安装："><a href="#2-下载安装：" class="headerlink" title="2). 下载安装："></a>2). 下载安装：</h3><ul><li>点击这个<a href="https://plugins.zhile.io/files/ide-eval-resetter-2.1.6.zip">链接(v2.1.6)</a>下载插件的<code>zip</code>包（macOS可能会自动解压，然后把<code>zip</code>包丢进回收站）</li><li>通常可以直接把<code>zip</code>包拖进IDE的窗口来进行插件的安装。如果无法拖动安装，你可以在<code>Settings/Preferences...</code> -&gt; <code>Plugins</code> 里手动安装插件（<code>Install Plugin From Disk...</code>）</li><li>插件会提示安装成功。</li></ul><h2 id="0x2-如何使用"><a href="#0x2-如何使用" class="headerlink" title="0x2. 如何使用"></a>0x2. 如何使用</h2><ul><li>一般来说，在IDE窗口切出去或切回来时（窗口失去/得到焦点）会触发事件，检测是否长时间（<code>25</code>天）没有重置，给通知让你选择。（初次安装因为无法获取上次重置时间，会直接给予提示）</li><li>也可以手动唤出插件的主界面：<ul><li>如果IDE没有打开项目，在<code>Welcome</code>界面点击菜单：<code>Get Help</code> -&gt; <code>Eval Reset</code></li><li>如果IDE打开了项目，点击菜单：<code>Help</code> -&gt; <code>Eval Reset</code></li></ul></li><li>唤出的插件主界面中包含了一些显示信息，<code>2</code>个按钮，<code>1</code>个勾选项：<ul><li>按钮：<code>Reload</code> 用来刷新界面上的显示信息。</li><li>按钮：<code>Reset</code> 点击会询问是否重置试用信息并<strong>重启IDE</strong>。选择<code>Yes</code>则执行重置操作并<strong>重启IDE生效</strong>，选择<code>No</code>则什么也不做。（此为手动重置方式）</li><li>勾选项：<code>Auto reset before per restart</code> 如果勾选了，则自勾选后<strong>每次重启/退出IDE时会自动重置试用信息</strong>，你无需做额外的事情。（此为自动重置方式）</li></ul></li></ul><h2 id="0x3-如何更新"><a href="#0x3-如何更新" class="headerlink" title="0x3. 如何更新"></a>0x3. 如何更新</h2><h3 id="1-插件更新机制（推荐）："><a href="#1-插件更新机制（推荐）：" class="headerlink" title="1). 插件更新机制（推荐）："></a>1). 插件更新机制（推荐）：</h3><ul><li>IDE会自行检测其自身和所安装插件的更新并给予提示。如果本插件有更新，你会收到提示看到更新日志，自行选择是否更新。</li><li>点击IDE的<code>Check for Updates...</code> 菜单手动检测IDE和所安装插件的更新。如果本插件有更新，你会收到提示看到更新日志，自行选择是否更新。</li><li>插件更新可能会需要<strong>重启IDE</strong>。</li></ul><h3 id="2-手动更新："><a href="#2-手动更新：" class="headerlink" title="2). 手动更新："></a>2). 手动更新：</h3><ul><li>从本页面下载最新的插件<code>zip</code>包安装更新。参考本文：<code>下载安装</code>小节。</li><li>插件更新需要<strong>重启IDE</strong>。</li></ul><h2 id="0x4-一些说明"><a href="#0x4-一些说明" class="headerlink" title="0x4. 一些说明"></a>0x4. 一些说明</h2><ul><li><p>本插件默认不会显示其主界面，如果你需要，参考本文：<code>如何使用</code>小节。</p></li><li><p>市场付费插件的试用信息也会<strong>一并重置</strong>。</p></li><li><p>对于某些付费插件（如:<code>Iedis 2</code>,<code>MinBatis</code>）来说，你可能需要去取掉<code>javaagent</code></p><p>配置（如果有）后重启IDE：</p><ul><li>如果IDE没有打开项目，在<code>Welcome</code>界面点击菜单：<code>Configure</code> -&gt; <code>Edit Custom VM Options...</code> -&gt; 移除 <code>-javaagent:</code> 开头的行。</li><li>如果IDE打开了项目，点击菜单：<code>Help</code> -&gt; <code>Edit Custom VM Options...</code> -&gt; 移除 <code>-javaagent:</code> 开头的行。</li></ul></li><li><p>重置需要<strong>重启IDE生效</strong>！</p></li><li><p>重置后并不弹出<code>Licenses</code>对话框让你选择输入License或试用，这和之前的重置脚本/插件不同（省去这烦人的一步）。</p></li><li><p>如果长达<code>25</code>天不曾有任何重置动作，IDE会有<strong>通知询问</strong>你是否进行重置。</p></li><li><p>如果勾选：<code>Auto reset before per restart</code> ，重置是静默无感知的。</p></li><li><p>简单来说：勾选了<code>Auto reset before per restart</code>则无需再管，一劳永逸。</p></li></ul><h2 id="0x5-开源信息"><a href="#0x5-开源信息" class="headerlink" title="0x5. 开源信息"></a>0x5. 开源信息</h2><ul><li>插件是学习研究项目，源代码是开放的。源码仓库地址：<a href="https://gitee.com/pengzhile/ide-eval-resetter">Gitee</a>。</li><li>如果你有更好的想法，欢迎给我提<code>Pull Request</code>来共同研究完善。</li><li>插件源码使用：<code>GPL-2.0</code>开源协议发布。</li><li>插件使用<code>PHP</code>编写，毕竟<code>PHP</code>是世界上最好的编程语言！</li></ul><h2 id="0x6-支持的产品"><a href="#0x6-支持的产品" class="headerlink" title="0x6. 支持的产品"></a>0x6. 支持的产品</h2><ul><li><strong>IntelliJ IDEA</strong></li><li><strong>AppCode</strong></li><li><strong>CLion</strong></li><li><strong>DataGrip</strong></li><li><strong>GoLand</strong></li><li><strong>PhpStorm</strong></li><li><strong>PyCharm</strong></li><li><strong>Rider</strong></li><li><strong>RubyMine</strong></li><li><strong>WebStorm</strong></li></ul><p><strong>转载于：</strong> <a href="https://zhile.io/2020/11/18/jetbrains-eval-reset.html">https://zhile.io/2020/11/18/jetbrains-eval-reset.html</a></p>]]></content>
      
      
      <categories>
          
          <category> tips </category>
          
      </categories>
      
      
        <tags>
            
            <tag> idea </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Git的配置与在idea中的应用</title>
      <link href="2020/10/27/Git/"/>
      <url>2020/10/27/Git/</url>
      
        <content type="html"><![CDATA[<h1 id="Git"><a href="#Git" class="headerlink" title="Git"></a>Git</h1><h1 id="0x0-GitHub用户信息"><a href="#0x0-GitHub用户信息" class="headerlink" title="0x0. GitHub用户信息"></a>0x0. GitHub用户信息</h1><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">Username: fourgoldEmail address: lijinxinok@163.com密码：IELTS-rise-to-6.5-!验证邮箱：lijinxinok@163.com邮箱密码：qinni123！<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="0x1-Github常用命令"><a href="#0x1-Github常用命令" class="headerlink" title="0x1. Github常用命令"></a>0x1. Github常用命令</h1><h3 id="2-1初始化操作"><a href="#2-1初始化操作" class="headerlink" title="2.1初始化操作"></a>2.1初始化操作</h3><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">#初始化本地仓库git init#查看隐藏文件.&#x2F;gitls -lAll .git&#x2F;#.git存放的是本地库相关的子目录以及文件#初始化本地配置 设置签名 这个签名和登录远程库和账号密码没有任何关系#global代表的是系统用户级别git config --global user.name JInxinLi#初始化本地邮箱$ git config --global user.email Jinxin@atguigu.com#签名的作用是区分不同操作者身份。用户的签名信息在每一个版本的提交信息中能够看到，以此确认本次提交是谁做的。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>就近原则:项目级别的优先于系统用户级别,采用项目级别的签名</p><p><strong>签名信息保存在哪里?</strong></p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">.&#x2F;git&#x2F;config<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><strong>系统用户保存在哪里?</strong></p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">~&#x2F;.gitconfig<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="2-2实际操作"><a href="#2-2实际操作" class="headerlink" title="2.2实际操作"></a>2.2实际操作</h3><p>git add添加到暂存区还可以撤销</p><p>HEAD是一个指针</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">#查看主文档 暂存区与工作区的状态 默认matser分支git status#添加暂存区git add#提交到本地库 hello.txt是文件名1 git commit -m &quot;version1.0&quot; hello.txt#推送matser开发线到远程仓库git push sparkStreaming master#将远程仓库的内容克隆到本地git clone http#拉取远程仓库内容git pull sparkStreaming master#查看历史版本 (HEAD -&gt; matser) HEAD是一个指针git loggit reflog#切换版本号git reset --hard 087a1a7#创建分支git branch hot-fix#查看分支git branch -v#切换分支git checkout hot-fix#合并分支#注意当前分支为mastergit merge hot-fix#冲突解决后提交git commit -m &quot;merge hot-fix&quot;####新建别名urlgit remote -vgit remote add origin https:&#x2F;&#x2F;url....git remote -v##推送git push origin master##克隆操作git clone url#回车#完整的把远程库下载到本地#创建origin远程地址别名#初始化本地库#邀请别人加入到协作组中#点击邀请,就变成了团队成员#pull &#x3D; fetch+mergegit fetch origin master#将远程库的master拉取下来#切换到origin master#查看远程的master分支[远程地址][远程分支]git checkout origin&#x2F;master#合并远程的master分支[远程地址][远程分支]git merge origin&#x2F;mastergit pull origin&#x2F;master<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>冲突解决</strong></p><p>当主分支已经提升了版本,而我们的clone的是旧版本,这个时候就已经无法推送</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">#解决,先拿到远程的修改 如果不是最新版所做的修改,不能修改,必须先拉取git pull origin&#x2F;master#文件里东西修改git add [filename]git commit -m &quot;version2.0&quot;git push origin&#x2F;master<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>跨团队协作</strong></p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">#以第三者的身份先进行fork fork到自己的远程库#克隆到本地git clone url[自己的地址]#然后进行增加内容git commit -m &quot;...&quot; [filename]git push origin master#现在已经推送到自己的远程库#本地修改,然后推送到远程#new pull request#create new pull result#发送消息进行提交#经理 打开pull request#点击内容#两人可以聊天#点commits files changed#审核代码#merge pull request#点这里进行合并#合并的时候也要添加相关信息<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>展示的时候显示了来源</p><h3 id="2-3查看历史记录log的方式"><a href="#2-3查看历史记录log的方式" class="headerlink" title="2.3查看历史记录log的方式"></a>2.3查看历史记录log的方式</h3><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">#最完整的形式git log#日志以一个漂亮的格式进行显示git log --pretty&#x3D;oneline#hash值显示一部分git log --oneline#多屏幕显示控制方法#空格向下翻页#b向上翻页#q退出#在oneline的基础上显示了移动到某一个版本要移动几步git reflog<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="https://i.loli.net/2020/12/24/mvKrNGdHQwWt9Ty.png" alt="注意HEAD"></p><h3 id="2-4版本回退穿梭"><a href="#2-4版本回退穿梭" class="headerlink" title="2.4版本回退穿梭"></a>2.4版本回退穿梭</h3><p>管理历史记录的时候存在一个<strong>指针</strong>(HEAD)</p><p>我们可以把HEAD指针进行移动</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">#切换版本号 git reset --hard 087a1a7<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="2-5三种操作的参数"><a href="#2-5三种操作的参数" class="headerlink" title="2.5三种操作的参数"></a>2.5<strong>三种操作</strong>的参数</h3><ol><li>基于索引值操作[推荐]</li><li>使用^符号:只能往后退</li></ol><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">#回退一步git reset --hard HEAD^#回退三步git reset --hard HEAD^^^<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><ol><li>使用~符号</li></ol><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">#回退3步git reset --hard HEAD~3<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="2-6参数说明"><a href="#2-6参数说明" class="headerlink" title="2.6参数说明"></a>2.6参数说明</h3><p>reset命令的三个参数对比</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">#查看本地帮助文档git help reset#命令--soft不会动index file(暂存区) and work tree(工作区)仅仅在本地库移动HEAD指针#将本地库后退--mixed在本地移动指针重置暂存区#将暂存区与本地库后退--hard移动指针重置缓存区重置工作区#全部后退<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="2-7永久删除文件的保存"><a href="#2-7永久删除文件的保存" class="headerlink" title="2.7永久删除文件的保存"></a>2.7永久删除文件的保存</h3><p>删除仅仅是一条记录,可以回退版本进行恢复</p><p>前提:删除前,文件存在时的状态提交到了本地库</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">git reset -hard[指针位置]<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="2-8添加到暂存区的删除文件找回"><a href="#2-8添加到暂存区的删除文件找回" class="headerlink" title="2.8添加到暂存区的删除文件找回"></a>2.8添加到暂存区的删除文件找回</h3><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">#暂存区与工作都是git reset --hard HEAD<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="2-9比较文件差异"><a href="#2-9比较文件差异" class="headerlink" title="2.9比较文件差异"></a>2.9比较文件差异</h3><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">git diff test.txtgit diff [本地区中的历史版本][文件名]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p><img src="https://i.loli.net/2020/12/24/GsheZUHS9PQ3zWN.png" alt="image-20201224171304996"></p><p>将工作区的文件个暂存区的文件进行比较</p><h3 id="2-10分支管理"><a href="#2-10分支管理" class="headerlink" title="2.10分支管理"></a>2.10分支管理</h3><p>在版本控制过程中,使用多条线控制任务的分支</p><p>分支的命名以feature开头 feature_bule</p><p>热修复的命名 hot_fix </p><p>分支能够同时并行推进多个功能的开发,提高开发效率</p><p>如果分支在开发过程中,如果某一个分支开发失败,不会对其他分支有任何影响,失败的分支删除重新开始</p><h3 id="2-11分支的具体操作"><a href="#2-11分支的具体操作" class="headerlink" title="2.11分支的具体操作"></a>2.11分支的具体操作</h3><p>master是默认分支</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">#查看分支git branch -v#创建分支git branch hot_fix#切换分支git checkout hot_fix#合并分支的步骤#第一步:切换到接受修改的分支git checkout master#第二步:执行merge命令git merge hot_fix#解决冲突#第一步编辑文件,删除特殊符号#第二步把文件修改到满意#第三步 git add[文件名]#第四部 git commit<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="0x3-Git基本原理"><a href="#0x3-Git基本原理" class="headerlink" title="0x3. Git基本原理"></a>0x3. Git基本原理</h1><h2 id="3-1哈希算法"><a href="#3-1哈希算法" class="headerlink" title="3.1哈希算法"></a>3.1哈希算法</h2><pre class="mermaid">graph LR明文-->加密算法-->密文</pre><p>同一个数保证加密后得到同一个结果</p><p>输入数据细微变化会引起Hash巨大的变化</p><p>哈希算法不可逆</p><p>不管输入的数据的数据量有多大,输入同一个哈希算法,得到的加密结果长度固定</p><p>很多内容也会加密成得到的长度相同32位16进制数</p><p>Git底层采用的是SHA-1算法</p><p><strong>用途</strong>:哈希算法用于校验文件</p><h2 id="3-2-Git保存版本的机制"><a href="#3-2-Git保存版本的机制" class="headerlink" title="3.2 Git保存版本的机制"></a>3.2 Git保存版本的机制</h2><p>每个版本都会保存当前版本的文件状态</p><p>Git把数据看做是小型文件系统的一组快照,每次提交更新时Git都会对当前的全部文件制作一个快照并保存这个快照的索引</p><p>为了高效,如果文件没有修改,Git不再重新存储该文件,而是只保留一个链接指向之前的存储的文件,所以Git的工作方式可以称为快照流</p><h2 id="3-3Git如何管理分支"><a href="#3-3Git如何管理分支" class="headerlink" title="3.3Git如何管理分支"></a>3.3Git如何管理分支</h2><p>第一次提交是rootcommit</p><p>master与testing都算是指针,指向原来的对象</p><h1 id="0x4-idea使用GitHub"><a href="#0x4-idea使用GitHub" class="headerlink" title="0x4. idea使用GitHub"></a>0x4. idea使用GitHub</h1><h3 id="4-1创建同步忽略文件"><a href="#4-1创建同步忽略文件" class="headerlink" title="4.1创建同步忽略文件"></a>4.1创建同步忽略文件</h3><p>创建忽略规则文件xxxx.ignore（前缀名随便起）</p><p>这个文件的存放位置原则上在哪里都可以，</p><p>为了便于让~/.gitconfig文件引用，</p><p>建议也放在用户家目录下</p><p>xxxx.ignore文件内容如下：</p><h4 id="idea-ignore"><a href="#idea-ignore" class="headerlink" title="idea.ignore"></a>idea.ignore</h4><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"># Compiled class file*.class# Log file*.log# BlueJ files*.ctxt# Mobile Tools for Java (J2ME).mtj.tmp&#x2F;# Package Files #*.jar*.war*.nar*.ear*.zip*.tar.gz*.rar# virtual machine crash logs, see http:&#x2F;&#x2F;www.java.com&#x2F;en&#x2F;download&#x2F;help&#x2F;error_hotspot.xmlhs_err_pid*.classpath.project.settingstarget.idea*.iml<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>2）在.gitconfig文件中引用忽略配置文件（此文件在Windows的家目录中）</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">[user]name &#x3D; ZhangJYemail &#x3D; ZhangJY@atguigu.com[core]excludesfile &#x3D; C:&#x2F;Users&#x2F;ZhangJY&#x2F;SH0720.ignore注意：这里要使用“正斜线（&#x2F;）”，不要使用“反斜线（\）”<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="4-2使用免密登录连接远程仓库"><a href="#4-2使用免密登录连接远程仓库" class="headerlink" title="4.2使用免密登录连接远程仓库"></a>4.2使用免密登录连接远程仓库</h2><p>配置免密登录时非常有必要的</p><p>首先要明白配置免密登录使用的SSH登录方式,使用RSA</p><p>免密登陆地址</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">#1.进入家目录cd<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><blockquote><p>89388@DESKTOP-CEH28KV MINGW64 ~</p></blockquote><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">#2.删除.ssh目录rm -rvf .ssh<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><blockquote><p>removed ‘.ssh/known_hosts’</p><p>removed directory ‘.ssh’</p></blockquote><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">#3.生成免密密钥 -C +github账号ssh-keygen -t rsa -C lijinxinok@163.com<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">#4.查看公钥并复制公钥cat id_rsa.pub#复制公钥,注意有坑,在命令行复制容易出问题#可以去源文件的地方使用nodpad++打开<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">#5.添加公钥到github#如图<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p><img src="https://i.loli.net/2020/12/25/am5XnUMoeWY7G2x.png" alt=" 1606477780968"></p><p>然后将公钥复制进去就可以了</p><p>注意push的时候要使用SSH地址哦</p><p><img src="https://i.loli.net/2020/12/25/4PqjRpcbdrewtGv.png" alt="免密登陆地址"></p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">#6.查看当前所有远程地址别名git remote -v <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>然后将远程登录的SSH复制,添加别名</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">#7.别名 远程地址git remote add SparkStreaming http:ssh登录地址<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>注意有坑,这里时ssh登录地址</p><p>接下来就可以测试了</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">#8.测试一下git add hello.txtgit push SparkStreaming master<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h2 id="4-3-Git结构"><a href="#4-3-Git结构" class="headerlink" title="4.3.Git结构"></a>4.3.Git结构</h2><pre class="mermaid">graph TD历史版本---本地库临时存储---暂存区写代码---工作区工作区--git add-->暂存区暂存区--git commit-->本地库</pre><h2 id="4-4Git和代码托管中心"><a href="#4-4Git和代码托管中心" class="headerlink" title="4.4Git和代码托管中心"></a>4.4Git和代码托管中心</h2><h4 id="局域网环境"><a href="#局域网环境" class="headerlink" title="局域网环境"></a>局域网环境</h4><p>GitLab</p><h4 id="外网络环境"><a href="#外网络环境" class="headerlink" title="外网络环境"></a>外网络环境</h4><p>GitHub</p><p>码云</p><h2 id="4-5-本地与内部协作"><a href="#4-5-本地与内部协作" class="headerlink" title="4.5 本地与内部协作"></a>4.5 本地与内部协作</h2><h4 id="团队内部协作"><a href="#团队内部协作" class="headerlink" title="团队内部协作"></a>团队内部协作</h4><p>加入团队可以增加权限</p><h4 id="跨团队协作"><a href="#跨团队协作" class="headerlink" title="跨团队协作"></a>跨团队协作</h4><h2 id="4-6从零到一使用IDEA"><a href="#4-6从零到一使用IDEA" class="headerlink" title="4.6从零到一使用IDEA"></a>4.6从零到一使用IDEA</h2><h3 id="创建本地库"><a href="#创建本地库" class="headerlink" title="创建本地库"></a>创建本地库</h3><p>将一个文件添加到ignore忽视</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">#将数据添加到ignoregit add to .gitignore<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>将文件添加exclude</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">git add to exclude<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="创建版本"><a href="#创建版本" class="headerlink" title="创建版本"></a>创建版本</h3><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">commitversion-0.1 Copy Revison Number<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="回退版本"><a href="#回退版本" class="headerlink" title="回退版本"></a>回退版本</h3><p><img src="https://i.loli.net/2020/12/25/oyQbqeJjfGzdlF5.png" alt="回退版本图"></p><p>然后将版本哈希值粘贴到HEAD地方</p><h3 id="创建分支以及合并分支"><a href="#创建分支以及合并分支" class="headerlink" title="创建分支以及合并分支"></a>创建分支以及合并分支</h3><p>创建分支</p><p><img src="https://i.loli.net/2020/12/25/48awkZFMH3X7q2B.png" alt="image-20201225163340577"></p><p>合并分支</p><p><img src="https://i.loli.net/2020/12/25/DyPaoRHQ5Ud2Y63.png" alt="image-20201225163935837"></p><h3 id="如何在idea里解决分支冲突"><a href="#如何在idea里解决分支冲突" class="headerlink" title="如何在idea里解决分支冲突"></a>如何在idea里解决分支冲突</h3><p>merge</p><h3 id="添加合作伙伴"><a href="#添加合作伙伴" class="headerlink" title="添加合作伙伴"></a>添加合作伙伴</h3><p>setting</p><p>manage access</p><h1 id="0x5-HEXO个人博客"><a href="#0x5-HEXO个人博客" class="headerlink" title="0x5. HEXO个人博客"></a>0x5. HEXO个人博客</h1><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">git config --global user.name &quot;godweiyang&quot;git config --global user.email &quot;792321264@qq.com&quot;ssh-keygen -t rsa -C &quot;lijinxinok@163.com&quot;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h1 id="0x6-配置图床"><a href="#0x6-配置图床" class="headerlink" title="0x6. 配置图床"></a>0x6. 配置图床</h1><p><img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_20_12/%E6%B5%8B%E8%AF%95.png" alt="测试图"></p><p>加油</p>]]></content>
      
      
      <categories>
          
          <category> git </category>
          
      </categories>
      
      
        <tags>
            
            <tag> git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark_SparkOperator</title>
      <link href="2020/08/29/Spark_operator/"/>
      <url>2020/08/29/Spark_operator/</url>
      
        <content type="html"><![CDATA[<h1 id="算子"><a href="#算子" class="headerlink" title="算子"></a>算子</h1><h3 id="reduceBykey"><a href="#reduceBykey" class="headerlink" title="reduceBykey"></a>reduceBykey</h3><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">val dataRDD1 &#x3D; sparkContext.makeRDD(List((&quot;a&quot;,1),(&quot;b&quot;,2),(&quot;c&quot;,3)))val dataRDD2 &#x3D; dataRDD1.reduceByKey(_+_)val dataRDD3 &#x3D; dataRDD1.reduceByKey(_+_, 2)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="aggregateByKey"><a href="#aggregateByKey" class="headerlink" title="aggregateByKey"></a>aggregateByKey</h3><pre class="line-numbers language-java" data-language="java"><code class="language-java">val dataRDD1 &#x3D;    sparkContext.makeRDD(List((&quot;a&quot;,1),(&quot;b&quot;,2),(&quot;c&quot;,3)))val dataRDD2 &#x3D;    dataRDD1.aggregateByKey(0)(_+_,_+_)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h3 id="combineByKey"><a href="#combineByKey" class="headerlink" title="combineByKey"></a>combineByKey</h3><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala"> * @author Jinxin Li * @create 2020-10-30 8:53 * 第一个初始值(但是传递函数),把每个分区的第一个值通过一个函数转化为初始值 * 第二个是区内聚合 * 第三个是区间聚合函数.跟上一个一样 * * 没有经过shuffle数据不会改变排序 * 需求,区内字符串相加,区间字符串长度相乘 *&#x2F;object CombineByKey &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    val sc &#x3D; new SparkContext(new SparkConf().setMaster(&quot;local[3]&quot;).setAppName(&quot;project1&quot;))    val list&#x3D; List(      (1, &quot;aa&quot;), (1,&quot;b&quot;), (1,&quot;a&quot;),      (2,&quot;ccc&quot;), (2, &quot;95&quot;), (1,&quot;b&quot;))&#x2F;&#x2F;这里(1,&quot;b&quot;)是第二个分区,但是位置是3,但是key为1的时候他是第一个,用这个第一个数指    val input &#x3D; sc.makeRDD(list, 2)    val combineRdd&#x3D; input.combineByKey(      x&#x3D;&gt;x.length,&#x2F;&#x2F;将每一个分区内的第一位数的进行函数作为初始值      &#x2F;&#x2F;这里区内第一个数指的是,相同的key的值的第一个      (len:Int,str:String)&#x3D;&gt;len + str.length,&#x2F;&#x2F;len是首值,也就是第一个函数传递的分区      (len1:Int,len2:Int) &#x3D;&gt; len1*len2&#x2F;&#x2F;不是函数的柯里化无法使用首值进行类型推断    ).saveAsTextFile(&quot;Day05&#x2F;output&quot;)  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="sortByKey"><a href="#sortByKey" class="headerlink" title="sortByKey"></a>sortByKey</h3><p>稳定排序:数字相同不会改变顺序</p><p>函数时间复杂度O(c),O(n),O(nlogn),O(n2)</p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">object SortByKey &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    val sc &#x3D; new SparkContext(new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;project1&quot;))    val list&#x3D; List(      (1, &quot;aa&quot;), (1,&quot;b&quot;), (1,&quot;a&quot;),      (2,&quot;ccc&quot;), (2, &quot;95&quot;), (1,&quot;b&quot;))    val input &#x3D; sc.makeRDD(list, 2)    input.sortByKey().collect().foreach(println)  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="join"><a href="#join" class="headerlink" title="join"></a>join</h3><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">object Join &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    val sc &#x3D; new SparkContext(new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;project1&quot;))    val list&#x3D; List(      (1, &quot;aa&quot;), (1,&quot;b&quot;), (1,&quot;a&quot;),      (2,&quot;ccc&quot;), (2, &quot;95&quot;), (1,&quot;b&quot;))&#x2F;&#x2F;这里(1,&quot;b&quot;)是第二个分区,但是位置是3,但是key为1的时候他是第一个,用这个第一个数指    val input1 &#x3D; sc.makeRDD(list, 2)    val list2&#x3D; List(      (1, &quot;aa&quot;), (1,&quot;b&quot;), (1,&quot;a&quot;),      (2,&quot;ccc&quot;), (2, &quot;95&quot;), (3,&quot;b&quot;))    val input2 &#x3D; sc.makeRDD(list, 2)    input1.join(input2).collect().foreach(println)    &#x2F;&#x2F;结果是相同的Key进行join 1号list有3个key1,2号有4个key1,两者会产生12个数据(key1)    &#x2F;&#x2F;同时这个是内连接    &#x2F;&#x2F;也就是3这个key会消失  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="PartitionBy-包含关于分区的一些说明"><a href="#PartitionBy-包含关于分区的一些说明" class="headerlink" title="PartitionBy(包含关于分区的一些说明)"></a>PartitionBy(包含关于分区的一些说明)</h3><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">object PartitionBy &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    val sc &#x3D; new SparkContext(new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;project1&quot;))    val rdd &#x3D; sc.makeRDD(Array((1,&quot;aaa&quot;),(2,&quot;bbb&quot;),(3,&quot;ccc&quot;),(1,&quot;aaa&quot;),(2,&quot;bbb&quot;),(3,&quot;ccc&quot;)),3)    &#x2F;*import org.apache.spark.HashPartitioner    val rdd2 &#x3D; rdd.partitionBy(new HashPartitioner(3))*&#x2F;    &#x2F;&#x2F;关于makeRDD的分区事项,默认是根据range与核心数进行切分,分区,添加hash分区器之后按照hash进行分区    val fileRDD: RDD[String] &#x3D; sc.textFile(&quot;Day05&#x2F;input&quot;)    fileRDD.saveAsTextFile(&quot;Day05&#x2F;output&quot;)    &#x2F;&#x2F;textFile的分区默认按照2个分区.保证最少两个分区    &#x2F;&#x2F;默认按照HDFS来进行分区,但是至少两个分区  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="count"><a href="#count" class="headerlink" title="count"></a>count</h3><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">&#x2F;&#x2F;行动算子的主要特点就是返回一个非RDD,经过shuffleobject count &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    val sc &#x3D; new SparkContext(new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;project1&quot;))    val rdd&#x3D; sc.textFile(&quot;Day02&#x2F;input&#x2F;word.txt&quot;)    &#x2F;&#x2F; 返回RDD中元素的个数    val countResult: Long &#x3D; rdd.count()    println(countResult)  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="cogroup"><a href="#cogroup" class="headerlink" title="cogroup"></a>cogroup</h3><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">object cogroup &#123;  &#x2F;&#x2F;coGroup主要是对每一个key进行分组,然后针对于这些分组每一个分组作为一个buffer  def main(args: Array[String]): Unit &#x3D; &#123;    val sc &#x3D; new SparkContext(new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;project1&quot;))    val dataRDD1 &#x3D; sc.makeRDD(List((&quot;a&quot;,1),(&quot;a&quot;,2),(&quot;c&quot;,3)))    val dataRDD2 &#x3D; sc.makeRDD(List((&quot;a&quot;,1),(&quot;c&quot;,2),(&quot;c&quot;,3)))    val value: RDD[(String, (Iterable[Int], Iterable[Int]))] &#x3D; dataRDD1.cogroup(dataRDD2)    value.foreach(println)    &#x2F;&#x2F;(a,(CompactBuffer(1, 2),CompactBuffer(1)))    &#x2F;&#x2F;(c,(CompactBuffer(3),CompactBuffer(2, 3)))  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="Scala%E7%AE%97%E5%AD%90%E6%80%BB%E7%BB%93.assets/1604105832445.png" alt="1604105832445"></p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">sc.longAccumulator<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="Scala%E7%AE%97%E5%AD%90%E6%80%BB%E7%BB%93.assets/1604111699745.png" alt="累加器"></p><p><img src="Scala%E7%AE%97%E5%AD%90%E6%80%BB%E7%BB%93.assets/1604112475954.png" alt="Wc的累加器"></p><p><img src="Scala%E7%AE%97%E5%AD%90%E6%80%BB%E7%BB%93.assets/1604112819436.png" alt="Scala算子总结"></p><p><img src="Scala%E7%AE%97%E5%AD%90%E6%80%BB%E7%BB%93.assets/1604113095891.png" alt="1604113095891"></p><p><img src="Scala%E7%AE%97%E5%AD%90%E6%80%BB%E7%BB%93.assets/1604113150988.png" alt="1604113150988"></p><p><img src="Scala%E7%AE%97%E5%AD%90%E6%80%BB%E7%BB%93.assets/1604129808171.png" alt="1604129808171"></p><p><img src="Scala%E7%AE%97%E5%AD%90%E6%80%BB%E7%BB%93.assets/1604130528847.png" alt="1604130528847"></p><p><img src="Scala%E7%AE%97%E5%AD%90%E6%80%BB%E7%BB%93.assets/1604133613281.png" alt="1604133613281"></p><p><img src="Scala%E7%AE%97%E5%AD%90%E6%80%BB%E7%BB%93.assets/1604279277784.png" alt="1604279277784"></p><p><img src="Scala%E7%AE%97%E5%AD%90%E6%80%BB%E7%BB%93.assets/1604279808204.png" alt="1604279808204"></p><h2 id="算子实战1-1"><a href="#算子实战1-1" class="headerlink" title="算子实战1.1"></a>算子实战1.1</h2><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">&#x2F;&#x2F;6.2.1object Case1_1 &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    val Conf: SparkConf &#x3D; new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;project&quot;)    val sc: SparkContext &#x3D; new SparkContext(Conf)    val source: RDD[Array[String]] &#x3D; sc.textFile(&quot;Day07&#x2F;input&#x2F;user_visit_action.txt&quot;).map(_.split(&quot;_&quot;))    &#x2F;&#x2F;品类点击总数    &#x2F;&#x2F;思路:filter-&gt;map(6,1)-&gt;reduceBykey    val require1RDD: RDD[(String, Int)] &#x3D; source.filter(fields &#x3D;&gt; fields(6) !&#x3D; &quot;-1&quot;).map(fields &#x3D;&gt; (fields(6), 1)).reduceByKey(_ + _)    require1RDD.collect().foreach(println)    &#x2F;&#x2F;思路:filter-&gt;flatMap-&gt;reduceBykey    val require2RDD: RDD[(String, Int)] &#x3D; source.filter(fields &#x3D;&gt; fields(8) !&#x3D; &quot;null&quot;).flatMap(t &#x3D;&gt; t(8).split(&quot;,&quot;))      .map((_, 1)).reduceByKey(_ + _)    require2RDD.collect().foreach(println)    val require3RDD: RDD[(String, Int)] &#x3D; source.filter(fields &#x3D;&gt; fields(10) !&#x3D; &quot;null&quot;).flatMap(t &#x3D;&gt; t(10).split(&quot;,&quot;))      .map((_, 1)).reduceByKey(_ + _)    require3RDD.collect().foreach(println)    val result: RDD[(String, Int, Int, Int)] &#x3D; require1RDD.join(require2RDD).join(require3RDD).map &#123;      case (a, ((b, c), d)) &#x3D;&gt; (a, b, c, d)    &#125;.sortBy(t&#x3D;&gt;(t._2,t._3,t._4),false)    result.collect().foreach(println)  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="算子实战1-2-Case-If-多级筛选-多级排序"><a href="#算子实战1-2-Case-If-多级筛选-多级排序" class="headerlink" title="算子实战1.2 [Case - If 多级筛选,多级排序]"></a>算子实战1.2 [Case - If 多级筛选,多级排序]</h2><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">&#x2F;** * @author Jinxin Li * @create 2020-11-02 19:57 *&#x2F;object Case1_2 &#123;  def main(args: Array[String]): Unit &#x3D; &#123;    val Conf1: SparkConf &#x3D; new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;project1.2&quot;)    val sc: SparkContext &#x3D; new SparkContext(Conf1)    val source: RDD[Array[String]] &#x3D; sc.textFile(&quot;Day07&#x2F;input&#x2F;user_visit_action.txt&quot;).map(_.split(&quot;_&quot;))    &#x2F;&#x2F;filter-&gt;flatmap-&gt;map-&gt;reducebykey    source.flatMap&#123;      case info if info(6) !&#x3D; &quot;-1&quot; &#x3D;&gt; Array(((info(6),&quot;click&quot;),1))      case info if info(8) !&#x3D; &quot;null&quot; &#x3D;&gt; info(8).split(&quot;,&quot;).map(t&#x3D;&gt;((t,&quot;order&quot;),1))      case info if info(10) !&#x3D; &quot;null&quot; &#x3D;&gt; info(10).split(&quot;,&quot;).map(t&#x3D;&gt;((t,&quot;pay&quot;),1))      case _ &#x3D;&gt; Nil    &#125;.reduceByKey(_+_).map&#123;      case ((cateId,action),count) &#x3D;&gt; (cateId,(action,count))    &#125;.groupByKey.mapValues(t&#x3D;&gt;&#123;      val map: Map[String, Int] &#x3D; t.toMap      (map.getOrElse(&quot;click&quot;,0),map.getOrElse(&quot;order&quot;,0),map.getOrElse(&quot;pay&quot;,0))    &#125;).sortBy(_._2,false).collect().foreach(println)  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="算子实战1-3-样例类-拓展样例类-重要"><a href="#算子实战1-3-样例类-拓展样例类-重要" class="headerlink" title="算子实战1.3 [样例类,拓展样例类(重要)]"></a>算子实战1.3 [样例类,拓展样例类(重要)]</h2><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala"><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="Scala的集合与泛型"><a href="#Scala的集合与泛型" class="headerlink" title="Scala的集合与泛型"></a>Scala的集合与泛型</h3><h3 id="算子实战1-4-1-5-2-3-自定义累加器"><a href="#算子实战1-4-1-5-2-3-自定义累加器" class="headerlink" title="算子实战1.4,1.5,2,3,自定义累加器"></a>算子实战1.4,1.5,2,3,自定义累加器</h3><h3 id="源码解析"><a href="#源码解析" class="headerlink" title="源码解析"></a>源码解析</h3>]]></content>
      
      
      <categories>
          
          <category> spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> operator </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark_Kernel</title>
      <link href="2020/04/29/Spark_Kernel/"/>
      <url>2020/04/29/Spark_Kernel/</url>
      
        <content type="html"><![CDATA[<h1 id="Spark内核"><a href="#Spark内核" class="headerlink" title="Spark内核"></a>Spark内核</h1><hr><blockquote><p>所谓的内核，就是Spark内部核心原理。</p></blockquote><h2 id="一、内核解析的分解"><a href="#一、内核解析的分解" class="headerlink" title="一、内核解析的分解"></a>一、内核解析的分解</h2><ol><li>Spark应用的提交</li><li>Spark内部的通信</li><li>Spark作业的调度</li><li>任务的执行</li><li>spark内存管理</li></ol><h2 id="二、-SparkSubmit"><a href="#二、-SparkSubmit" class="headerlink" title="二、 SparkSubmit"></a>二、 SparkSubmit</h2><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql">--本章节讲述job提交应用以后，环境的准备工作。主要包含以下：1. spark向yarn提交job的过程2. yarn中application、driver、executor、container是如何相互响应<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><ul><li>提交应用</li></ul><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql">bin&#x2F;spark-submit \--class org.apache.spark.examples.SparkPi \--master yarn \--deploymode cluster \   表示yarn的集群模式.&#x2F;examples&#x2F;jars&#x2F;spark-examples_2.12-2.4.5.jar \10-- 说明：--master yarn 默认是采用yarn的客户端模式，但是在实际过程中，我们都是使用yarn的集群模式。所以增加：--deploymode cluster \<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="2-1-Spark向yarn提交"><a href="#2-1-Spark向yarn提交" class="headerlink" title="2.1  Spark向yarn提交"></a>2.1  Spark向yarn提交</h3><h4 id="2-1-1-SparkSubmit"><a href="#2-1-1-SparkSubmit" class="headerlink" title="2.1.1 SparkSubmit"></a>2.1.1 SparkSubmit</h4><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql">--作用：1. 解析参数2. 提交参数，初始数环境，并获取&quot;org.apache.spark.deploy.yarn.YarnClusterApplication&quot;的对象，调用对象的start方法<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql">1. 执行SparkSubmit的mian方法2. 在main方法中：   1）、 new SparkSubmit   2）、 submit.doSubmit(args) --&gt;执行提交程序,点击doSubmit          ①、 val appArgs &#x3D; parseArguments(args)  --&gt;解析参数，解析应用提交的参数，点击parseArguments                  a、parse(args.asJava)   --&gt;具体进行参数的解析，点击parse，返回参数的解析，方法的内部调用了handle方法                     action &#x3D; Option(action).getOrElse(SUBMIT)，--&gt;默认值为submit                  b、handle(opt: String, value: String) --&gt;opt:参数的名称，value：参数的值。                      左边是参数  &#x3D;&gt; 右边是赋值的变量                     &#x2F;&#x2F; --master yarn &#x3D;&gt; master                     &#x2F;&#x2F; --deploy-mode cluster &#x3D;&gt; deployMode                     &#x2F;&#x2F; --class SparkPI(WordCount) &#x3D;&gt; 【mainClass】                               &quot;如上为解析参数&quot;       ②、appArgs.action match &#123;case SparkSubmitAction.SUBMIT &#x3D;&gt; submit(appArgs, uninitLog)--&gt;点击submit          a、submit中又调用了doRunMain()，doRunMain()中调用了runMain()方法              -- runMain(args, uninitLog)，运行主程序，在runmain()方法中：                  1.准备提交环境                  -- val (childArgs, childClasspath, sparkConf, childMainClass) &#x3D; prepareSubmitEnvironment(args)                                    2.设定当前类的加载器                  -- Thread.currentThread.setContextClassLoader(loader)                                    3.通过类名加载这个类，&#39;反射的方式&#39;                  -- mainClass &#x3D; Utils.classForName(childMainClass)                                    4.创建第3步类的实例，并将类型转换为SparkApplication                  -- app: SparkApplication &#x3D; mainClass.newInstance().asInstanceOf[SparkApplication]                                     childMainClass到底是谁？                       cluster模式：childMainClass &#x3D; YARN_CLUSTER_SUBMIT_CLASS                                   &#x3D;org.apache.spark.deploy.yarn.YarnClusterApplication                       client模式：childMainClass &#x3D; args.mainClass&#x3D;class SparkPI(WordCount)                                       5.YarnClusterApplication.start                   --  app.start(childArgs.toArray, sparkConf)                                &quot;如上为提交环境，并启动org.apache.spark.deploy.yarn.YarnClusterApplication&quot;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="2-2-2-yarn-YarnClusterApplication"><a href="#2-2-2-yarn-YarnClusterApplication" class="headerlink" title="2.2.2 yarn.YarnClusterApplication"></a>2.2.2 yarn.YarnClusterApplication</h4><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql">--作用：1. 调用YarnClusterApplication的start方法，创建yarn的resourcemanagerClient，RM的客户端2. 执行RM客户端执行run方法3. 在run方法中，启动一个应用程序application，也就是一个进程，并提交应用程序，则会执行这个进程的main方法。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql">1. 通过反射调用start()方法，在start()方法中：   -- 1）new Client(new ClientArguments(args), conf).run()          ①new ClientArguments(args)，是配置参数的封装          ②new Client，在client类中的属性有：              --val yarnClient &#x3D; YarnClient.createYarnClient，点击createYarnClient方法，在这个方法中：                  -- YarnClient client &#x3D; new YarnClientImpl()，点击YarnClientImpl类，在类中有一个属性                      rmclient：resourcemanagerClient                      -- protected ApplicationClientProtocol rmClient          &quot;如上就是创建RM客户端对象&quot;，接下来执行run方法          ③run()，RM客户端对象执行run方法，点击run，在run方法的内部：              1. 提交应用，返回应用的id。              -- this.appId &#x3D; submitApplication()，点击submitApplication(),查看具体提交的过程                     1. 初始化hadoop的环境                   --yarnClient.init(hadoopConf)                   2. 启动yarn客户端,与yarn之间进行连接      -- yarnClient.start()      3. yarn客户端创建一个应用application      --val newApp &#x3D; yarnClient.createApplication()                     4. 获取应用的id，在yarn应用程序中，每一个应用都是有唯一的应用id      -- appId &#x3D; newAppResponse.getApplicationId()      5. 提交yarn应用程序，提交的是什么呢？      --yarnClient.submitApplication(appContext)，点击appContext         --&#x2F;&#x2F; Set up the appropriate contexts to launch our AM               配置java虚拟机的启动参数，点击createContainerLaunchContext，               在这个方法的内部进行了command的封装：               【集群模式】command &#x3D; bin&#x2F;java org.apache.spark.deploy.yarn.ApplicationMaster                              【client模式】command &#x3D; bin&#x2F;java org.apache.spark.deploy.yarn.ExecutorLauncher                              --val containerContext &#x3D; createContainerLaunchContext(newAppResponse)                              基本参数配置的封装                              --val appContext &#x3D; createApplicationSubmissionContext(newApp, containerContext)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="2-2-3-yarn-ApplicationMaster"><a href="#2-2-3-yarn-ApplicationMaster" class="headerlink" title="2.2.3 yarn.ApplicationMaster"></a>2.2.3 yarn.ApplicationMaster</h4><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql">-- 作用1. 封装ApplicationMaster的参数2. 根据参数，创建ApplicationMaster对象3. 执行ApplicationMaster的run方法，在run方法中，最后调用到runDriver方法，在这个方法中：   a、启动用户的应用，并返回这个应用的&quot;线程&quot;，具体实现如下：           a、启动用户提交的应用程序；           b、在ApplicationMaster中创建一个线程，线程的名称就是&quot;Driver&quot;           c、启动这个线程，并执行run方法，在run方法中，就是执行我们提交的应用程序类的main方法           d、返回这个&quot;Driver&quot;线程    b、 执行一个方法，用于返回&quot;sparkContext&quot;的对象，如果没有返回，就不会执行下面的代码，当返回了这个上下文的对象以后：    c、 ApplicationMaster通过ApplicationMaste的客户端，向ResourceManager注册自己，并申请资源    d、 分配资源，具体实现如下：             a、在ResourceManager端获取一个ApplicationMaster的客户端，返回一个分配器            b、分配器进行资源的分配：                 a、ApplicationMaster的客户端申请一个分配器响应                 b、分配器响应返回所有被分配的容器container(资源列表)给到ApplicationMaster                 c、如果分配的资源列表的数量大于0，则对容器进行处理，处理的方式为：                        1.AM内部会创建一个线程，并调用线程的run方法，在run方法中循环遍历RM返回的可用容器，然后进行                        对每个容器进行匹配，此时涉及到首选位置，根据请求匹配选择哪些容器.首选位置的选择规则见首选位置说明。                        2. 运行匹配后的资源，挨个遍历可用的容器，如果运行执行器的数量小于目标执行器的数量&quot;假如需要4个执行                        器，即为目标执行器，此时已经运行了2个执行器，即为运行执行器的数量，此时会启动下面的逻辑&quot;，                        那么在这个容器中会创建一个线程池，一个线程池container对应一个ExecutorRunnable，并调用了这个对象的                        run方法，在这个线程池中，有一个nmClient(nameManagClient),说明AM能够找到NM，在这个run方法中，创建                        NM的客户端，初始化NM，并启动容器container，在启动容器中，封装一个指令，   command：&#x2F;bin&#x2F;java                        &#x2F;org.apache.spark.executor.CoarseGrainedExecutorBackend，并且启动了这个指令，显然是一个进程                        ，CoarseGrainedExecutorBackend，粗粒度的执行器后台。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql">1. main方法，在main方法中，分三步骤：    1） 封装参数    --val amArgs &#x3D; new ApplicationMasterArguments(args)    2）创建ApplicationMaster的对象    --master &#x3D; new ApplicationMaster(amArgs)    3）执行run方法,点击run方法    --System.exit(master.run())           ①run方法的实现，点击runImpl       --runImpl()            &#x2F;&#x2F; 如果是client模式，执行：            -- runExecutorLauncher()             &#x2F;&#x2F; 如果是集群模式，执行，点击runDriver            -- runDriver               1. 启动用户的程序,返回一个线程，点击startUserApplication               --userClassThread &#x3D; startUserApplication()                    1. 通过类加载器加载一个类，并获取这个类的main方法                    -- val mainMethod &#x3D; userClassLoader.loadClass(args.userClass).getMethod(&quot;main&quot;, classOf[Array[String]])                    2. 创建一个线程                    -- val userThread &#x3D; new Thread                    3.                     -- userThread.setContextClassLoader(userClassLoader)                    4. 设定线程的名字为driver，说明driver就是一个applicationMaster的一个线程                    -- userThread.setName(&quot;Driver&quot;)                    5. 启动线程，执行线程的run方法，其实就是执行类userClass的main方法，userClass是哪个类呢？                       通过查到，就是我们提交应用的--class，sparkpi，或者是我们自定的类                    -- userThread.start()                        -- mainMethod.invoke                          6. 返回用户线程                    -- userThread               2. awaitResult等待结果，线程阻塞，等待对象(SparkContext)的返回               --val sc &#x3D; ThreadUtils.awaitResult(sparkContextPromise.future,Duration(totalWaitTime, TimeUnit.MILLISECONDS))                                3. 返回sparkContext以后，向rm进行注册AM：ApplicationMaster，点击registerAM()                --registerAM(host, port, userConf, sc.ui.map(_.webUrl))                    ApplicationMaster的客户端向RM注册自己，并申请资源                    --client.register(host, port, yarnConf, _sparkConf, uiAddress, historyAddress)               4. 返回RM分配的容器               --createAllocator(driverRef, userConf)                   &#x2F;&#x2F; 1.AM的客户端，&#39;在RM端&#39;，创建分配器，返回一个分配器                   -- allocator &#x3D; client.createAllocator                   &#x2F;&#x2F; 2.分配器分配资源，点击allocateResources                   -- allocator.allocateResources()                          &#x2F;&#x2F; 1.AM的客户端，申请一个分配响应                          --val allocateResponse &#x3D; amClient.allocate(progressIndicator)                          &#x2F;&#x2F; 2.分配器响应获取所有被分配的容器container(资源列表)                         --val allocatedContainers &#x3D; allocateResponse.getAllocatedContainers()                         &#x2F;&#x2F; 3.如果可分配的容器数量大于0，则调用处理可用容器的方法，点击handle方法                          --if (allocatedContainers.size &gt; 0) &#x3D;&gt;                            handleAllocatedContainers(allocatedContainers.asScala)                               &#x2F;&#x2F; 1.内部会创建一个线程，并调用线程的run方法，在run方法中循环遍历RM返回的可用容器，然后进行                                  对每个容器进行匹配，此时涉及到首选位置，根据请求匹配选择哪些容器.首选位置的选择规则见                                  首选位置说明。                               &#x2F;&#x2F; 2. 运行匹配后的资源，点击runAllocatedContainers                               --runAllocatedContainers(containersToUse)                                      &#x2F;&#x2F; 1. 挨个遍历可用的容器资源                                       --for (container &lt;- containersToUse)                                       &#x2F;&#x2F; 2. 每个容器中，如果运行执行器的数量小于目标执行器的数量，执行如下代码                                       --runningExecutors.size() &lt; targetNumExecutors                                       &#x2F;&#x2F; 3. 线程池，在线程池的内部有：                                       --launcherPool.execute(new Runnable                                             &#x2F;&#x2F; 1.执行的池子是一个线程池                                            --launcherPool &#x3D; ThreadUtils.newDaemonCachedThreadPool                                       &#x2F;&#x2F; 2.一个线程container对应一个ExecutorRunnable，并调用了这个对象的run方法                                       --new ExecutorRunnable...run()                                            &#x2F;&#x2F; a、在ExecutorRunnable中：说明AM能够找到NM                                            --nmClient，nodeManager                                            &#x2F;&#x2F; b、run()中：其实就是AM与NM建立连接                                                     &#x2F;&#x2F; 创建NM的客户端                                                     --nmClient &#x3D; NMClient.createNMClient()                                                     &#x2F;&#x2F; 初始化NM                                                     --nmClient.init(conf)                                                      &#x2F;&#x2F; 启动NM                                                     -- nmClient.start()                                                      &#x2F;&#x2F; 启动容器，点击                                                      --startContainer()                                                           &#x2F;&#x2F; NM启动容器，启动executor                                                           --nmClient.startContainer(container.get, ctx)                                                           &#x2F;&#x2F; 封装指令，点击prepareCommand                                                           --val commands &#x3D; prepareCommand()                                                                 commands&#x3D;&#x2F;bin&#x2F;java&#x2F;org.apache.spark.executor.CoarseGrainedExecutorBackend--&gt;粗粒度的执行器后台，是一个进程                                                           &#x2F;&#x2F;将封装好的指令传递到参数中                                                           --ctx.setCommands(commands.asJava)   <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql">-- 首选位置说明        --1. 移动数据不如移动计算。         --2. 首选位置：有多个，和本地化级别有关。        --3. 本地化级别：将数据和计算所在的位置称之为本地化               1. 计算和数据在同一个Executor中，称之进程本地化               2. 计算和数据在同一个节点中，称之节点本地化               3. 计算和数据在同一个机架中，称之机架本地化               4. 任意<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200618002757.png" alt="image-20200618002757465" style="zoom:50%;" /><img src="https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200618002929.png" alt="image-20200618002929263" style="zoom:50%;" /><p>![image-20200619202433592](<a href="https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic">https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic</a> GO/20200619202433.png)</p><h4 id="2-2-4-CoarseGrainedExecutorBackend"><a href="#2-2-4-CoarseGrainedExecutorBackend" class="headerlink" title="2.2.4 CoarseGrainedExecutorBackend"></a>2.2.4 CoarseGrainedExecutorBackend</h4><blockquote><p>执行一次bin/java就会执行一个新的进程，则是属于并行执行的感觉，和之前执行的内容是分开的。类似我们在Windows中开了一个微信和qq程序一样，各自执行，互不影响。</p></blockquote><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql">-- 作用：   执行CoarseGrainedExecutorBackend&quot;执行器后台&quot;的main方法，在main方法中：   1. 首先封装一些参数   2. 执行run方法，在run方法中：        1. 通过driver的URI，使得CoarseGrainedExecutorBackend与Driver进行关联        2. 通过通信环境创建了一个终端，名字为executor，创建一个CoarseGrainedExecutorBackend对象并调用onstart方法：             1. 获取driver的引用             2. ExecutorBackend向driver发送消息，注册executor的消息，也称之为反向注册             3. 在driver端会接收到这个消息，通过executor的引用，发送消息给到ExecutorBackend，注册executor成功              4. ExecutorBackend接收driver返回的executor注册成功的消息，            -- 说明：   executor是一个计算对象，在这个对象里面有一个线程池，每一个线程来处理一个从driver端发送过来的任务 <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql">1. commands&#x3D;&#x2F;bin&#x2F;java&#x2F;org.apache.spark.executor.CoarseGrainedExecutorBackend,执行这个指令，那么是调用这个类的main方法。2. main方法中：       &#x2F;&#x2F; 1. 首先是对一些参数进行封装       &#x2F;&#x2F; 2. 执行run方法        -- run(driverUrl, executorId, hostname, cores, appId, workerUrl, userClassPath)           &#x2F;&#x2F; 1.通过driver的uri和Driver进行关联            --driver &#x3D; fetcher.setupEndpointRefByURI(driverUrl)            &#x2F;&#x2F; 2.通过通信环境创建了一个终端，名字为executor，            在底层：Executor启动后会注册通信，并收到信息onStart，收到消息后，会执行通信对象CoarseGrainedExecutorBackend            的onStart方法，点击CoarseGrainedExecutorBackend            --env.rpcEnv.setupEndpoint(&quot;Executor&quot;, new CoarseGrainedExecutorBackend(        env.rpcEnv, driverUrl, executorId, hostname, cores, userClassPath, env))                &#x2F;&#x2F; 1.获取driver的引用                -- driver &#x3D; Some(ref)                &#x2F;&#x2F; 2.ExecutorBackend向driver发送消息，注册executor的消息，也称之为反向注册                --ref.ask[Boolean](RegisterExecutor(executorId, self, hostname, cores, extractLogUrls))                &#x2F;&#x2F; 3.在driver端会接收到这个消息，因为在driver端，有一个上下文的对象，sparkcontext，在这个类有一个属性：                   private var _schedulerBackend: SchedulerBackend &#x3D; _，点击SchedulerBackend，是一个trait，找到                   实现类：CoarseGrainedSchedulerBackend，在这个类中，有一个方法：receiveAndReply()：                      &#x2F;&#x2F; executor的引用，在driver端，发送消息给到ExecutorBackend，注册executor成功                      --executorRef.send(RegisteredExecutor)                                            &#x2F;&#x2F; ExecutorBackend类中有一个recive方法，用来接收driver返回的executor注册成功的消息，executor是一                         个计算对象，在这个对象里面有一个线程池，每一个线程来处理一个从driver端发送过来的任务                     --executor &#x3D; new Executor(executorId, hostname, env, userClassPath, isLocal &#x3D; false)                       <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>![image-20200618150421861](<a href="https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic">https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic</a> GO/20200618150421.png)</p><p>![image-20200618150442390](<a href="https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic">https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic</a> GO/20200618150442.png)</p><h4 id="2-2-5-总结"><a href="#2-2-5-总结" class="headerlink" title="2.2.5 总结"></a>2.2.5 总结</h4><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql">-- 1. application是在一个nodemanager中container中，并且在这个container中创建了一个driver线程-- 2. 在一个nodemanager中，可以创建多个container，在每个container中，会创建ExecutorBackend对象，在这个对象中，会创建一个executor对象，在这个对象中一个线程池，一个线程用来处理driver发来的一个task，至于能同时执行多少个task，和executor中的core数量有关。-- 3. ApplicationMaster周旋于Driver和ResourceManager之间-- 4. spark有两个进程，也就是两个分支    创建RM的客户端，创建AM，在AM中，创建Driver的线程    &quot;分支1&quot;：此时会执行Driver线程的run方法，在run方法中就是执行了应用程序的main方法    &quot;分支2&quot;：构建SparkContext上下文的对象，再向RM注册AM，然后申请资源和返回可用的资源，最后Driver进行资源的选择，按照首选位置的原则。    所以如下图片有一个错误：资源满足以后才执行main方法，实际上是创建了driver线程，还没有申请资源就已经开始执行main方法了。-- 5. 进程、线程、对象   &quot;进程&quot;：SparkSubmit、ApplicationMaster和CoarseGrainedExecutorBackend   &quot;线程&quot;：Driver，但是我们一般称SparkContext称之为Driver   &quot;对象&quot;：Executor和YarnClusterApplication   -- 6. client和cluster模式的区别：      Driver的位置不同，其余的逻辑是一样的。      Cluster：在集群中，在nodemanager中的AM对象中，是一个线程      client：在集群之外<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200618155642.png" alt="image-20200618155642818" style="zoom:150%;" /><h2 id="三、Spark内部组件及通信"><a href="#三、Spark内部组件及通信" class="headerlink" title="三、Spark内部组件及通信"></a>三、Spark内部组件及通信</h2><h3 id="3-1-通信原理"><a href="#3-1-通信原理" class="headerlink" title="3.1 通信原理"></a>3.1 通信原理</h3><p>Netty:通信框架/AIO</p><p>为什么要采用Netty==&gt;<strong>AIO</strong></p><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql">-- 通信原理 - IO - RPC    1. 基本的网络通信：Socket, ServerSocket    2. 通信框架：AKKA(旧),  Netty(新)(AIO)    3. 三种IO方式：BIO（阻塞式）, NIO（非阻塞式）, AIO（异步非阻塞）<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h4 id="0x1-如何理解三种IO"><a href="#0x1-如何理解三种IO" class="headerlink" title="0x1 如何理解三种IO?"></a>0x1 如何理解三种IO?</h4><p>BIO: 阻塞式IO 饭馆点餐,一直等待上餐,</p><p>NIO:非阻塞式IO 不干等着,让老板先做饭,去干别的,时不时回去询问,饭好没好</p><p>这种方式没有阻塞,但是要时不时的回头看饭有没有做好,性能提高但是有损耗</p><p>AIO:异步非阻塞式IO 一个小时后送到指定位置,性能最好</p><p><strong>注意</strong></p><p>Netty就是基于AIO开发的通信框架</p><p>但是Linux对AIO支持不够好,不支持</p><p>Linux采用Epoll方式模仿AIO进行操作</p><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql">--Linux与windows通信框架的对比在Linux系统上，AIO的底层实现仍使用EPOLL，与NIO相同，因此在性能上没有明显的优势；Windows的AIO底层实现良好，但是Netty开发人员并没有把Windows作为主要使用平台考虑。微软的windows系统提供了一种异步IO技术：IOCP（I&#x2F;O CompletionPort，I&#x2F;O完成端口）；Linux下由于没有这种异步IO技术，所以使用的是epoll（一种多路复用IO技术的实现）对异步IO进行模拟。所以在Linux上不建议使用AIO<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="3-2-组件之间通信"><a href="#3-2-组件之间通信" class="headerlink" title="3.2 组件之间通信"></a>3.2 组件之间通信</h3><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql">1. 组件：Driver、executor2. 通信环境：NettyRpcEnvFactory() -- TransportServer(通信服务器 EPOLL) 服务器初始化3. 通信终端: RpcEndPoint[receive*] --用于接收数据收件箱: inbox --按顺序读取.4. 通信终端: RpcEndPointRef[ask*] --用于发送数据发件箱: outboxes[transportClient*] --根据地址(Host + Port),会有多个发件箱.-- 一个终端的生命周期：The life-cycle of an endpoint is:创建终端-&gt; 启动终端 -&gt; 接收消息 -&gt; 停止终端* &#123;@code constructor -&gt; onStart -&gt; receive* -&gt; onStop&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_21_01/Spark%E5%86%85%E9%83%A8%E9%80%9A%E4%BF%A1%E5%8E%9F%E7%90%86.png" alt="Spark内部组件通信原理"></p><p><img src="https://cdn.jsdelivr.net/gh/fourgold/images/fourgold/images/img_21_01/%E5%86%85%E9%83%A8%E9%80%9A%E4%BF%A1%E5%9F%BA%E4%BA%8Eactor%E6%A8%A1%E5%9E%8B.png" alt="基于actor模型额内部通信架构"></p><h2 id="四、作业的调度"><a href="#四、作业的调度" class="headerlink" title="四、作业的调度"></a>四、作业的调度</h2><h3 id="4-1-Application"><a href="#4-1-Application" class="headerlink" title="4.1 Application"></a>4.1 Application</h3><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql">1. Yarn中会有application，提交任务以后，就会产生一个应用，并有一个唯一的应用id2. 在SparkConf中配置了setAppName(xxxx),设置应用的名字<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>![image-20200618203801740](<a href="https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic">https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic</a> GO/20200618203801.png)</p><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql">3. SparkContext，是spark核心的对象，核心类，在这个核心类中的一些重要的参数有：  private var _conf: SparkConf &#x3D; _  -- spark的关键参数  private var _env: SparkEnv &#x3D; _    -- spark的环境，内部有NettyRpcEnv  private var _schedulerBackend: SchedulerBackend &#x3D; _   -- spark的调度后台，Rpc后台信息交互对象  private var _taskScheduler: TaskScheduler &#x3D; _         -- 任务调度器  private var _heartbeatReceiver: RpcEndpointRef &#x3D; _    -- 指心跳接收器，通信终端的引用   @volatile private var _dagScheduler: DAGScheduler &#x3D; _ -- 有向无环图调度器，负责job内部调度，负责阶段划分和任务的切分。  -- _conf：下滑线开头，表示内部的变量，不是规范，是早期程序员默认遵守的规范。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql">4. DAGScheduler ，spark非常核心的调度器。      1.内部有一个对象,DAGSchedulerEventProcessLoop,&quot;指事件调度的规则&quot;，点击这个类：    --private[spark] val eventProcessLoop &#x3D; new DAGSchedulerEventProcessLoop(this)        1.上面类继承于EventLoop,这个类中有一个属性：事件队列，用来存放事件           BlockingQueue[E]：阻塞式队列           LinkedBlockingDeque：双端队列        -- private val eventQueue: BlockingQueue[E] &#x3D; new LinkedBlockingDeque[E]()       <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>![image-20200618211748700](<a href="https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic">https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic</a> GO/20200618211748.png)</p><h3 id="4-2-逻辑代码"><a href="#4-2-逻辑代码" class="headerlink" title="4.2 逻辑代码"></a>4.2 逻辑代码</h3><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql">1. RDD的创建： 从内存中&#x2F;从文件中2. RDD的转换： 转换算子(单value类型、双value类型、kv类型)3. RDD的行动： 行动算子<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="4-3-job"><a href="#4-3-job" class="headerlink" title="4.3 job"></a>4.3 job</h3><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql">1. 触发作业的执行，在行动算子的内部会执行过程：    1.sparkContext提交作业--&gt; sc.runjob 2. 有向无环图的调度器执行runjob--&gt; dagScheduler.runJob 3. 提交job--&gt; submitjob4. 消息队列进行存放消息--&gt; eventProcessLoop.post5. 消息队列将消息放进队列中，这个消息是：JobSubmitted--&gt; eventQueue.put(event) 6. 在eventQueue有一个线程，线程中有一个run方法--&gt; eventThread 7.  负责取出消息，因为这个队列是一个阻塞式队列，队列中没有消息，那么就处于阻塞式状态--&gt; val event &#x3D; eventQueue.take() 8. 取到消息--&gt; onReceive(event)9. 执行处理消息--&gt; doOnReceive(event)10. 使用模式匹配的的方式处理消息--&gt;  def doOnReceive(event: DAGSchedulerEvent): Unit &#x3D; event match &#123;      case JobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties) &#x3D;&gt;      dagScheduler.handleJobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties)    11. 有向无环图调度器处理任务的提交     --&gt; dagScheduler.handleJobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties)    12. 创建一个活动的job    --&gt; val job &#x3D; new ActiveJob<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql">-- 总结：启动一个行动算子 --&gt; runjob  -&gt; 将执行事件放进阻塞式队列中 -&gt; 创建一个线程取出队列中的消息 -&gt; 进行模式匹配，处理任务的提交--&gt; 创建一个运行job<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="4-4-stage"><a href="#4-4-stage" class="headerlink" title="4.4 stage"></a>4.4 stage</h3><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql">-- 1. 阶段的划分，取决于转换算子的依赖类型。-- 2. 宽依赖：ShuffleDependency-- 3. 窄依赖：OneToOneDependency extends NarrowDependency-- 4. 分区的数量     a、窄依赖：分区数量保持不变          1. 获取窄依赖的分区数量，点击 firstParent          -- override def getPartitions: Array[Partition] &#x3D; firstParent[T].partitions               1. 获取依赖关系的第一个rdd分区数量               -- dependencies.head.rdd.asInstanceOf[RDD[U]]     b、宽依赖：            1. 获取宽依赖的分区数量            partitioner：是一个分区器，partitioner，由上一个RDD传递过来的，在传递的时候，会进行判断，如果当前的RDD的分区器            和上一级的分区器一样，那么是不会创建shuffleRDD，只有当前RDD的分区器和上一级的分区器不一样时，才会创建            ShuffledRDD            --Array.tabulate[Partition](part.numPartitions)(i &#x3D;&gt; new ShuffledRDDPartition(i))                        2. 默认情况下，默认的分区器将上一级的RDD传入            --  reduceByKey(defaultPartitioner(self), func)                1. 默认的分区数量等于上级RDD的最大值，因为上一级RDD可能有多个                -- val defaultNumPartitions &#x3D; rdds.map(_.partitions.length).max                2. 构造分区器的时候，将默认的分区数量传入，分区器的作用是指定数据去到哪个分区，分区的数量默认和上一级RDD                   保持一致                -- new HashPartitioner(defaultNumPartitions) -- 5. 总结：        a、窄依赖默认分区数量保持不变       b、宽依赖，默认和上一级RDD最大的分区数量保持一致，如果上一级RDD只有一个，那就和上一级RDD保持一致                 但是Shuffle的算子一般都会有改变分区数量的参数<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql">-- 6. 从文件中创建RDD时默认的分区数量      1. 取(defaultParallelism, 2)的最小值，点击defaultParallelism      --math.min(defaultParallelism, 2)      2. 选择yarn模式中的默认平行度。      --defaultParallelism &#x3D; conf.getInt(&quot;spark.default.parallelism&quot;, math.max(totalCoreCount.get(), 2))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>![image-20200620130734058](<a href="https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic">https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic</a> GO/20200620130734.png)</p><h3 id="4-5-task的切分"><a href="#4-5-task的切分" class="headerlink" title="4.5 task的切分"></a>4.5 task的切分</h3><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql">--1. 任务和阶段stage的关系     定位：DAGScheduler类     1. 处理任务的提交handleJobSubmitted，在这个方法的内部：        1. 将整个job作为一个finalStage    -- var finalStage: ResultStage &#x3D; null    2. 创建一个结果阶段，并赋值给finalStage        finalRDD：最后提交job时的RDD，点击createResultStage    -- finalStage &#x3D; createResultStage(finalRDD, func, partitions, jobId, callSite)        1. 通过当前的RDD获取其上一级的阶段，点击getOrCreateParentStages        -- val parents &#x3D; getOrCreateParentStages(rdd, jobId)             1.获取最后一个RDD的shuffle依赖，每一个shuffle依赖创建一个shufflemapStage             --getShuffleDependencies(rdd).map &#123; shuffleDep &#x3D;&gt;getOrCreateShuffleMapStage(shuffleDep, firstJobId)&#125;.toList              a. 在getShuffleDependencies方法中，找到resultStage的上一级shuffleRDD               val parents &#x3D; new HashSet[ShuffleDependency[_, _, _]] -- 存放宽依赖                    val visited &#x3D; new HashSet[RDD[_]] --创建一个hashSet集合，用来存放已经被访问过的RDD                    val waitingForVisit &#x3D; new ArrayStack[RDD[_]] -- 集合的栈，创建一个集合，用来存放待访问的RDD                    waitingForVisit.push(rdd)    -- 将最后的一个RDD传到这个集合中                    while (waitingForVisit.nonEmpty) &#123; -- 集合是否为空，刚放进去，肯定不是空                      val toVisit &#x3D; waitingForVisit.pop()  -- pop，弹栈，将刚刚放进去的RDD弹出来，并准备去访问                      if (!visited(toVisit)) &#123;  -- 当前放进去的RDD是否被访问过，如果没有，则继续向下执行                        visited +&#x3D; toVisit      -- 将当前获取的RDD放进已经被访问的RDD集合中                        toVisit.dependencies.foreach &#123;  -- 获取RDD与直接上级的RDD的依赖关系，并循环遍历。                          case shuffleDep: ShuffleDependency[_, _, _] &#x3D;&gt; -- 如果是宽依赖                            parents +&#x3D; shuffleDep  -- 则将依赖加入parents集合中                          case dependency &#x3D;&gt;                            waitingForVisit.push(dependency.rdd) -- 如果是窄依赖，将上级RDD放进等待访问的RDD中，并                                                                 进行循环，判断其与上级RDD的依赖关系，直到当前的RDD为                                                                 shuffleRDD                             &#125;                      &#125;                    &#125;                    parents   -- 将上一级shuffleRDD放进parents的集合中                                        获取当前RDD与直接上级的RDD的依赖关系，返回一个seq序列集合，因为当前的RDD的直接上级的RDD可能有多个                    -- toVisit.dependencies                 b、通过map方法，对resultStage上级的shuffleRDD进行遍历，调用如下方法：返回获取的ShuffleDependency，执行获取或创建shuffleMapStage，点击这个方法                    -- getOrCreateShuffleMapStage                        创建shuffleMapStage，每一个shuffleDep创建一个shuffleMapStage                        -- createShuffleMapStage(shuffleDep, firstJobId)                            new出一个shuffleMapStage                            &#x2F;&#x2F; 将依赖的上一级RDD赋值给rdd                            --val rdd &#x3D; shuffleDep.rdd                            &#x2F;&#x2F; 又调用了创建或获取上一级阶段                            -- val parents &#x3D; getOrCreateParentStages(rdd, jobId)                            -- val stage &#x3D; new ShuffleMapStage<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql">--2. 阶段的类型   ResultStage 和 shuffleMapStage--3. 阶段的数量   &#x3D; ResultStage  + n *  shuffleMapStage--4. 任务和分区的关系     1. 提交最后一个阶段：       --submitStage(finalStage)           1. 获取当前阶段的上一级阶段           --  val missing &#x3D; getMissingParentStages(stage).sortBy(_.id)           2. 如果有上一级阶段不为空，则循环遍历上一阶段，先假如上一级阶段只有一个，则提交上一个阶段，又调用提交阶段           --for (parent &lt;- missing) &#123;submitStage(parent)&#125;             &quot;总结：在提交阶段时，从最后一个阶段往前找，直到最前面的一个阶段，然后再依次从前往后进行提交阶段&quot;。                 2. 当没有上一级阶段以后，提交任务       -- submitMissingTasks(stage, jobId.get)          &#x2F;&#x2F; 1.对当前阶段进行模式匹配，确认是shuffleMapSrage还是ResultStage，返回结果为taskIdToLocations,任务本地化路径          &#x2F;&#x2F; 2. 如果当前阶段是ShuffleMapStage，则创建ShuffleMapTask                如果当前阶段是ResultStage ，则创建ResultTask           val tasks: Seq[Task[_]] &#x3D; try &#123;            case stage: ShuffleMapStage            partitionsToCompute.map  --&gt; 计算分区的数量，每一个分区，会执行如下创建任务的代码。            &#123;........            new ShuffleMapTask(stage.id, stage.latestInfo.attemptNumber            .....&#125;           case stage: ResultStage &#x3D;&gt;            &#123;                               .......            new ResultTask(stage.id, stage.latestInfo.attemptNumber,            .......                   &#125;-- 5. task的类型：      a、如果当前阶段是ShuffleMapStage，则创建ShuffleMapTask      b、如果当前阶段是ResultStage ，则创建ResultTask    -- 6 .任务的总数量      &#x3D; 每个阶段的任务总和<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql">--总结：1. 通过resultStage最后一个RDD，进行循环依次向上找，获取resultStage阶段，上一级为shuffleDep的ShuffleDependency，   存放到一个parents集合中2. 采用map算子，parents集合中的每个ShuffleDependency，获取到所有上级依赖为shuffleDep的RDD，然后每一个shuffleDep会创建一个ShuffleMapStage阶段。3. 当找到job最前面一个RDD以后，开始从第一个阶段提交阶段，提交阶段时，首先获取当前阶段最后一个RDD的分区数量，在一个阶段中，每一个分区就会创建一个task，task的类型和阶段的类型匹配：      a、如果当前阶段是ShuffleMapStage，则创建ShuffleMapTask      b、如果当前阶段是ResultStage ，则创建ResultTask4. 当前阶段提交完成以后，就提交下一个阶段，依次类推，最后就会提交resultStage。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="五、任务的执行"><a href="#五、任务的执行" class="headerlink" title="五、任务的执行"></a>五、任务的执行</h2><h3 id="5-1-任务包含的内容"><a href="#5-1-任务包含的内容" class="headerlink" title="5.1 任务包含的内容"></a>5.1 任务包含的内容</h3><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql">1.任务的提交：--new ShuffleMapTask(stage.id, stage.latestInfo.attemptNumber,taskBinary, part, locs, properties, serializedTaskMetrics, Option(jobId),Option(sc.applicationId), sc.applicationAttemptId, stage.rdd.isBarrier())2. 提交的重要几个参数有：   a、&quot;stage.id&quot;：任务从属的阶段id   b、&quot;taskBinary&quot;：是一个广播变量，内容为：阶段的&quot;RDD&quot;和&quot;依赖关系&quot;序列化以后的二进制字节码，因为RDD是不保存数据，一旦任务执行失败，需要知道RDD的元数据信息以及依赖关系，才能进行重新计算。       1. 是一个广播变量       --var taskBinary: Broadcast[Array[Byte]] &#x3D; null       2. 将任务的二进制的字节码赋值给了这个广播变量       --taskBinary &#x3D; sc.broadcast(taskBinaryBytes)       3. 任务的二进制的字节码是通过对阶段匹配，如果是shuffle阶段，就会采用闭合的序列化器将阶段的RDD和阶段的依赖进行序列化       --taskBinaryBytes &#x3D; stage match &#123;         case stage: ShuffleMapStage &#x3D;&gt;              JavaUtils.bufferToArray(                closureSerializer.serialize((stage.rdd, stage.shuffleDep): AnyRef))           case stage: ResultStage &#x3D;&gt;          JavaUtils.bufferToArray(closureSerializer.serialize((stage.rdd, stage.func): AnyRef))          &#125;    c、 &quot;part&quot; ：分区，指当前的task和哪个partition有关        -- val part &#x3D; partitions(id)    d、 &quot;locs&quot; ： 任务的首选位置        -- val locs &#x3D; taskIdToLocations(id)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="5-2-序列化"><a href="#5-2-序列化" class="headerlink" title="5.2 序列化"></a>5.2 序列化</h3><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql">1. 默认的序列化：&quot;JavaSerializer&quot;    1. 在SparkContext中创建了SparkEnv，点击创建的方法，一层一层往里点：    -- _env &#x3D; createSparkEnv(_conf, isLocal, listenerBus)        1. 最终看到了默认的序列化器为：JavaSerializer        --val serializer &#x3D; instantiateClassFromConf[Serializer](      &quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.JavaSerializer&quot;)    logDebug(s&quot;Using serializer: $&#123;serializer.getClass&#125;&quot;)2. kryo序列化:      --1.特点：          a、性能优          b、序列化结果文件的字节数少          c、可以绕过java的序列化，将不能序列的对象也能进行序列化          d、但是，我们在实际的情况下，并不是所有的对象都会采用kryo序列化。     --2. 那么哪些对象采用kryo序列化会比较有优势呢？          &quot;总结：在shuffle阶段，当为kv类型时，k、v的数据类型如果都支持kryo序列，则会采用kryo进行序列化。                支持ktyo序列化的数据类型有：String和值类型(anyVal)&quot;                   底层：当有shuffle阶段时，会选择最好的序列化器         -- Pick the best serializer for shuffling an RDD of key-value pairs.         2. 判断选择的规则：            如果kv的k和v都能使用kryo序列化器时，则选择kryo序列化器，否则选择默认的序列化器：javaSerializer            当为如下类型（值类型）或者是string类型的时候，则可以使用kyro序列化器            --if (canUseKryo(keyClassTag) &amp;&amp; canUseKryo(valueClassTag)) &#123;              kryoSerializer            &#125; else &#123;              defaultSerializer            &#125;                          --  ClassTag.Boolean,                  ClassTag.Byte,                  ClassTag.Char,                  ClassTag.Double,                  ClassTag.Float,                  ClassTag.Int,                  ClassTag.Long,                  ClassTag.Null,                  ClassTag.Short<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="5-3-任务的调度"><a href="#5-3-任务的调度" class="headerlink" title="5.3  任务的调度"></a>5.3  任务的调度</h3><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql">-- 1. driver生成的任务以后存放在哪里了？   a、当driver生成任务以后，并不是立即将任务task就发送给executor，因为可能发送过程有异常，也可能发送过去的时候，executor对象还没有创建，都会导致任务task发送失败        1. 一个阶段stage生成tasks以后，如果这个阶段的tasks的数量大于0，那么这个任务调度器就会提交任务，在提交任务中，会将这个          stage的任务封装成一个TaskSet,任务集进行提交，点击submitTasks      -- if (tasks.size &gt; 0)，taskScheduler.submitTasks(new TaskSet( tasks.toArray, stage.id, stage.latestInfo.attemptNumber, jobId, properties))              1.首先取出任务              --val tasks &#x3D; taskSet.tasks              2. 创建一个任务集taskset的管理者manager              -- val manager &#x3D; createTaskSetManager(taskSet, maxTaskFailures)              3. 构建调度器，将刚刚创建的任务集管理者放到调度器中，点击addTaskSetManager              --schedulableBuilder.addTaskSetManager(manager, manager.taskSet.properties)                  1.是一个抽象方法，类是一个trait，有两个实现类，分别是：                        FIFOSchedulableBuilder --&gt; 先进先出调度器                        FairSchedulableBuilder --&gt; 公平调度器                        那么我们新增加进去的manager是采用什么调度器呢？                            a、通过源码可知，默认的调度模式为FIFO模式                            -- private val schedulingModeConf &#x3D; conf.get(SCHEDULER_MODE_PROPERTY, SchedulingMode.FIFO.toString)                            b、创建一个任务调度池，当driver生成任务以后，会将任务放进任务池中，由manager来进行调度                            val rootPool: Pool &#x3D; new Pool(&quot;&quot;, schedulingMode, 0, 0)                   2. 将manager直接放进调度池中，                    rootPool.addSchedulable(manager)                4. 点击.reviveOffers：恢复当前的操作               --backend.reviveOffers()                   1.driver的终端，自己给自己发消息                    -- driverEndpoint.send(ReviveOffers)                    2.在DriverEndpoint中，就有一个receive方法，在这个方法中，匹配获取的消息，如果是ReviveOffers,                    则执行makeOffers()方法，点击makeOffers()方法                     -- case ReviveOffers &#x3D;&gt;makeOffers()                         a、DriverEndpoint调度器从任务池中取出任务，取任务的具体方式：点击resourceOffers                          -- val taskDescs &#x3D; scheduler.resourceOffers(workOffers)                               a、获取一个排好序的任务集合，实现方式，点击getSortedTaskSetQueue                                --val sortedTaskSets &#x3D; rootPool.getSortedTaskSetQueue                                     a、如下为任务集的调度的算法，依据算法对任务集进行比较排序，返回排好序的任务集，然后将                                     返回任务集存放到一个arraybuffer集合中，并返回给到sortedTaskSets，不同的调度的算法                                     是不一样的。                                     &quot;FIFO调度算法&quot;：先比较优先级，优先级高的先调度，如果优先级相等，则比较阶段id，阶段                                                    id小的先执行。                                     &quot;Fair调度算法&quot;：根据运行任务的数量、权重【默认值为1】、最小分配数量【默认值为0】，                                                    进行综合分配                                      -- val sortedSchedulableQueue &#x3D;      schedulableQueue.asScala.toSeq.sortWith(taskSetSchedulingAlgorithm.comparator)                                 b、如果任务不为空，则driver发射任务                          -- if (!taskDescs.isEmpty) &#123;launchTasks(taskDescs)&#125;             <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql">-- 1.总结   1. 一个stage生成tasks以后，由taskSchedule负责任务的调度   2. 一个stage就会有一个任务集，taskSet   3. 每一个taskSet都会被封装成TaskSetManager，负责监控管理同一个Stage中的Tasks，TaskScheduler调度模式有两种：        a、FIFOSchedulableBuilder --&gt; 先进先出调度器【默认调度模式】        b、FairSchedulableBuilder --&gt; 公平调度器   4. TaskScheduler初始化过程中会实例化rootPool任务池，driver准备的任务和管理者会发送到这个任务池中，      由TaskScheduler负责将任务调度结果发送给executor   5. driver的终端自己给自己发送一个消息&quot;ReviveOffers&quot;，driverEndpoint收到ReviveOffer消息后调用makeOffers方法，TaskScheduler就开始进行任务集的调度   6. 根据&quot;调度算法&quot;对任务集进行排序，获取一个排好序的队列&quot;排序在前的就先执行，排序在后的就后执行&quot;，将排好序的队列放到一个arraybuffer集合中，并返回给到sortedTaskSets              &quot;FIFO调度算法&quot;：先比较优先级，优先级高的先调度，如果优先级相等，则比较阶段id，阶段 id小的先执行。        &quot;Fair调度算法&quot;：根据运行任务的数量、weight【默认值为1】、minShare【默认值为0】，进行综合分配        minShare、weight的值均在公平调度配置文件&quot;fairscheduler.xml&quot;中被指定，调度池在构建阶段会读取此文件的相关配置   7. &quot;driverEndpoint&quot;调度器就从这个排好序的任务队列的数组中取任务tasks。   8. 如果获取的任务不为空，则dirver开始发射任务   -- 2.说明：   1. 从任务池中取出的任务，包含了本地化级别信息以及等待的时长(&quot;默认每个级别等待时间为3s，也可以单独设置每个级别的等待时间&quot;)，当在driver在发送任务的时候，会根据本地化级别进行发送任务.   -- 3.区分本地化级别和调度算法    调度算法：是指driverEndpoint在调度任务集时，确定哪个任务集先执行，哪个任务集后执行    本地化级别：是指driver在发送向executor发送任务的首选位置，确定任务发送到哪个executor中，如果发送不成功，并进行降级处理                                           <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="5-4-任务的计算"><a href="#5-4-任务的计算" class="headerlink" title="5.4  任务的计算"></a>5.4  任务的计算</h3><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql"> 1. driver发送任务前，会将任务进行编码： --val serializedTask &#x3D; TaskDescription.encode(task)    2. 然后向executor发送已经编码和序列化的任务task-- executorData.executorEndpoint.send(LaunchTask(new SerializableBuffer(serializedTask))3. 在executorbackend就会收到任务(receive)并启动任务,首先是对任务进行解码，然后executor启动任务，点击launchTask --val taskDesc &#x3D; TaskDescription.decode(data.value)     logInfo(&quot;Got assigned task &quot; + taskDesc.taskId)     executor.launchTask(this, taskDesc)4.  来一个task就使用一个线程来接收     --val tr &#x3D; new TaskRunner(context, taskDescription)     runningTasks.put(taskDescription.taskId, tr)     threadPool.execute(tr) 5. 线程中有一个run方法，方法中有一个逻辑为：task.run，通过底层发现，其实调用的是具体task对象的runTask()方法<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="5-5-shuffle"><a href="#5-5-shuffle" class="headerlink" title="5.5 shuffle"></a>5.5 shuffle</h3><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql">【在&quot;shuffleMapTask类&quot;中的runTask()方法中】1. shuffle&quot;写操作&quot;--var writer: ShuffleWriter[Any, Any] &#x3D; null2. 在写操作之前，也会调用迭代器的方式，所以也可以实现&quot;读的操作&quot;--writer.write(rdd.iterator(partition, context)......【在&quot;resultTask类&quot;中的runTask()方法中，那么就得有读数据的操作】1. RDD中不保存数据，所以操作的时候数据是一条一条的执行，则会调用迭代器的方法，点击iterator方法-- func(context, rdd.iterator(partition, context))    1. 一层一层的调，在shuffleRDD中的computer中有：&quot;读的操作&quot;     -- SparkEnv.get.shuffleManager.getReader(dep.shuffleHandle, split.index, split.index + 1, context).read()   <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql">&quot;分支1&quot;： Shuffle map(Write)      1. 点击getWrite      -- writer &#x3D; manager.getWriter[Any, Any](dep.shuffleHandle, partitionId, context)            1. getWriter是一个抽象方法，所在的类为：ShuffleManager，&#39;shuffle管理器&#39;，获取其实现类：&quot;SortShuffleManager&quot;               是一个可排序的shuffleManager管理器。查询这个管理类的getWriter方法，在这个方法中，对handle的类型进行模式匹               配，所以现在handle就很很重要了，从模式匹配项，可以知道有3种不同类型的handle，而且handle来自&quot;getWriter方法&quot;               -- handle match &#123;                  case unsafeShuffleHandle: SerializedShuffleHandle                  case bypassMergeSortHandle: BypassMergeSortShuffleHandle                  case other: BaseShuffleHandle      2.在 &quot;manager.getWriter&quot;方法中的handle到底是什么？看源码             1. 是shuffle管理器注册shuffle获取的，点击registerShuffle             --val shuffleHandle: ShuffleHandle &#x3D; _rdd.context.env.shuffleManager.registerShuffle(        shuffleId, _rdd.partitions.length, this)             2. 是一个抽象方法，获取抽象类&quot;ShuffleManager&quot;的实现类&quot;SortShuffleManager&quot;,查询&quot;registerShuffle&quot;方法                    从这里发现，确实有三种handle：                    a、如果忽略索引文件的排序 --&gt; 创建BypassMergeSortShuffleHandle                    b、如果可以实现序列化    --&gt; 创建SerializedShuffleHandle                    c、如果不是以上两种      --&gt; 创建BaseShuffleHandle                   --if (SortShuffleWriter.shouldBypassMergeSort(conf, dependency)) &#123;                            new BypassMergeSortShuffleHandle[K, V](                            shuffleId, numMaps, dependency.asInstanceOf[ShuffleDependency[K, V, V]])                        &#125; else if (SortShuffleManager.canUseSerializedShuffle(dependency)) &#123;                            new SerializedShuffleHandle[K, V](                            shuffleId, numMaps, dependency.asInstanceOf[ShuffleDependency[K, V, V]])                        &#125; else &#123;                               new BaseShuffleHandle(shuffleId, numMaps, dependency)                        &#125;                      &#125;                      1. 点击&quot;shouldBypassMergeSort&quot;,查看什么情况下忽略排序，如果当前rdd的map端有预聚合功能，就                         不能忽略排序，如reduceByKey算子                        -- if (dep.mapSideCombine) &#123;false&#125;                        如果map端没有预聚合功能，首先获取忽略合并的阈值，如果没有显示设置，就会默认给200，如果当前RDD的                        分区器的分区数量小于这个阈值，那么就返回true，则此时创建&quot;BypassMergeSortShuffleHandle&quot;                        --else &#123;                        val bypassMergeThreshold: Int &#x3D; conf.getInt(&quot;spark.shuffle.sortbypassMergeThreshold&quot;, 200)                        dep.partitioner.numPartitions &lt;&#x3D; bypassMergeThreshold                        -- 所以总结就是当rdd的map端没有预聚合功能，且分区器的分区数量小于阈值，那么就会创建                            &quot;BypassMergeSortShuffleHandle&quot;                     2. 点击&quot;canUseSerializedShuffle&quot;,Spark的内存优化后的解决方案,对象序列化后不需要反序列化。                          &#x2F;&#x2F; 通过以下代码可知，创建&quot;SerializedShuffleHandle&quot;的条件为,满足以下三个条件即可：                             a、序列化对象需要&quot;支持&quot;重定义                             b、依赖的map端&quot;没有&quot;预聚合功能                             c、分区数量&quot;小于&quot;(1 &lt;&lt; 24) - 1 &#x3D; 16777215                          if (!dependency.serializer.supportsRelocationOfSerializedObjects) &#123; false&#125;                           else if (dependency.mapSideCombine) &#123;false &#125;                           else if (numPartitions &gt; MAX_SHUFFLE_OUTPUT_PARTITIONS_FOR_SERIALIZED_MODE) &#123; false&#125;                           else &#123;true &#125;                     3. 如果以上两个handle都不满足，则选择最后一个handle：&quot;BaseShuffleHandle&quot; --&gt;默认的handle                                             &quot;分支2&quot;：Shuffle reduce(Read)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql">-- 总结： shuffle的handle有三种：     1. BypassMergeSortShuffleHandle  --&gt; BypassMergeSortShuffleWriter        &quot;条件&quot;：        a、当前rdd的map端没有预聚合功能，如groupBy        b、分区器的分区数量小于阈值,默认为200             2. SerializedShuffleHandle      --&gt; UnsafeShuffleWriter        &quot;条件&quot;：        a、序列化对象需要&quot;支持&quot;重定义        b、依赖的map端&quot;没有&quot;预聚合功能        c、分区数量&quot;小于&quot;(1 &lt;&lt; 24) - 1 &#x3D; 16777215     3. BaseShuffleHandle           --&gt; SortShuffleWriter        &quot;默认的handle&quot;如果前两种都不满足，那么就使用默认的write拿着这三种handle，再来看这个&quot;getWrite&quot;方法<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql"> -- handle match &#123;    -- case unsafeShuffleHandle: SerializedShuffleHandle &#x3D;&gt;       new UnsafeShuffleWriter....      -- case bypassMergeSortHandle: BypassMergeSortShuffleHandle &#x3D;&gt;       new BypassMergeSortShuffleWriter....    -- case other: BaseShuffleHandle &#x3D;&gt;       new SortShuffleWriter....&quot;不同的handle对应不同的writer&quot;    1. BypassMergeSortShuffleHandle  --&gt; BypassMergeSortShuffleWriter       &#x2F;&#x2F; 点击&quot;BypassMergeSortShuffleWriter&quot;中的write方法，如下代码，根据分区的数量进行循环，&#39;每一个分区就向磁盘写一个文       件&#39;。 即map端的每一个task会为reduce端的每一个task都创建一个临时磁盘文件,根据key的hashcode%分区数量，决定数据去到       哪个分区文件中。       -- for (int i &#x3D; 0; i &lt; numPartitions; i++) &#123;     partitionWriters[i] &#x3D; blockManager.getDiskWriter(blockId, file, serInstance, fileBufferSize, writeMetrics);&#125;          2. SerializedShuffleHandle       --&gt; UnsafeShuffleWriter       3. BaseShuffleHandle,&quot;重要&quot;       --&gt; SortShuffleWriter        &#x2F;&#x2F; 点击&quot;SortShuffleWriter&quot;中的write方法，如下代码：       &#x2F;&#x2F; 1. &quot;写文件过程&quot;：写磁盘文件时，首先将数据写到内存中，并在内存中的进行排序，如果内存（5M）不够，会溢写磁盘，       生成临时文件(一个数据文件，一个索引文件)，最终将所有的临时文件合并(原来的数据文件和索引文件会被删除)成数据       文件和索引文件。          2. &quot;预聚和的原理&quot;：在排序时，构造了一种类似于hashtable的结构，所以相同的key就聚合在一起。          3. &quot;排序规则&quot;：首先会按照分区进行排序，然后按照key.          4. &quot;数据进入不同分区的原则&quot;：按照分区器的原则，默认是hashpartition，根据key的hash%分区数量。        val partitionLengths &#x3D; sorter.writePartitionedFile(blockId, tmp)        shuffleBlockResolver.writeIndexFileAndCommit...<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>![image-20200621180817513](<a href="https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic">https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic</a> GO/20200621180817.png)</p><p>![image-20200620004312766](<a href="https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic">https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic</a> GO/20200620004312.png)</p><pre class="line-numbers language-sqlite" data-language="sqlite"><code class="language-sqlite">-- 面试中常见shuffle的两个问题：1. 我们现在spark使用了哪种shuffle，哪一种类型的？   a、sortshuffle。2. 忽略排序过程的shuffle什么时候会触发？   a、map 端没有预聚合功能   b、reduce端的分区数量小于一个阈值，默认是200<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="六-、-Spark内存管理"><a href="#六-、-Spark内存管理" class="headerlink" title="六 、 Spark内存管理"></a>六 、 Spark内存管理</h2><h3 id="6-1-堆内内存和堆外内存"><a href="#6-1-堆内内存和堆外内存" class="headerlink" title="6.1 堆内内存和堆外内存"></a>6.1 堆内内存和堆外内存</h3><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql">--1. &quot;堆内内存&quot;：    是指jvm所能使用的内存，并不是完全可以控制，如GC垃圾回收器的执行时间是不可控的，当你需要内存进行数据处理时，GC并不能立    马释放内存给你使用。jvm虚拟机默认使用的内存大小是可用内存的1&#x2F;64，最大值是1&#x2F;4--2. &quot;堆外内存&quot;：     在jvm虚拟机之外的内存，可以存储我们的数据，这个内存是咱们向操作系统申请过来的，完全可控。&quot;默认是不启用堆外内存&quot;--3. 设置堆外内存的参数：    a、启动堆外内存参数：spark.memory.offHeap.enabled    b、设定堆外内存的大小： spark.memory.offHeap.size --4. 在spark中，堆内和堆外内存可以进行统一的管理。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="6-2-内存空间分配"><a href="#6-2-内存空间分配" class="headerlink" title="6.2 内存空间分配"></a>6.2 内存空间分配</h3><h4 id="6-2-1-早期内存管理"><a href="#6-2-1-早期内存管理" class="headerlink" title="6.2.1 早期内存管理"></a>6.2.1 早期内存管理</h4><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql">&quot;早期各个区域的内存分配好了以后，就需要严格遵守这个规则，内存大小不可变。&quot;--1. 内存空间的分配：1. Storage：缓存RDD数据和广播变量的数据， &quot;内存大小占比60%&quot;2. Execution：用于缓存在shuffle过程中的中间数据， &quot;内存大小占比20%&quot;3. Other：用户自定义的一些数据结构或者是Spark内部的元数据 ： &quot;内存大小占比20%&quot;-- 2. Storage内存和Execution内存都有预留空间，目的是防止OOM，因为Spark堆内内存大小的记录是不准确的，需要留出保险区域。-- 3. 当前不同区域内存大小分配存在的问题：      Execution的内存过小，而Storage内存大小过多。       从而就产生了新的内存分配原则<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>堆内内存</li></ul><p>![image-20200620012427321](<a href="https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic">https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic</a> GO/20200620012427.png)</p><ul><li>堆外内存</li></ul><p>![image-20200620015214858](<a href="https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic">https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic</a> GO/20200620015214.png)</p><h4 id="6-2-2-统一内存管理"><a href="#6-2-2-统一内存管理" class="headerlink" title="6.2.2 统一内存管理"></a>6.2.2 统一内存管理</h4><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql">-- 1. 什么是统一内存管理？   Spark1.6 之后引入的统一内存管理机制，各个区域内存的大小是可变的. --2.与静态内存管理的区别:   统一内存管理&quot;存储内存&quot;和&quot;执行内存共享&quot;同一块空间，可以动态占用对方的空闲区域-- 3. 当前spark默认的内存分配是按照统一内存管理的模式。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>堆内内存</li></ul><p>![image-20200620015026859](<a href="https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic">https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic</a> GO/20200620015026.png)</p><ul><li>堆外内存</li></ul><p>![image-20200620015251558](<a href="https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic">https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic</a> GO/20200620015251.png)</p><h4 id="6-2-3-同一管理内存的优点"><a href="#6-2-3-同一管理内存的优点" class="headerlink" title="6.2.3 同一管理内存的优点"></a>6.2.3 同一管理内存的优点</h4><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql">-- 1. 优点1)设定基本的存储内存和执行内存区域（spark.storage.storageFraction参数），该设定确定了双方各自拥有的空间的范围；2)双方的空间都不足时，则存储到硬盘；若己方空间不足而对方空余时，可借用对方的空间;（存储空间不足是指不足以放下一个完整的Block）3)执行内存的空间被对方占用后，可让对方将占用的部分转存到硬盘，然后”归还”借用的空间；4)存储内存的空间被对方占用后，无法让对方”归还”，因为需要考虑 Shuffle过程中的很多因素，实现起来较为复杂。-- 2. 统一内存管理的动态占用机制图如下：<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>![image-20200620015447725](<a href="https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic">https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic</a> GO/20200620015447.png)</p><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql">-- 注意事项1. 如果是storage借了Execution的内存，那么当Execution需使用时，storage占用Execution的内存就要想办法还给Execution，一般可以进行落盘，但是在内存中的数据有一个存储级别，如果仅仅是Memory_Only的话，那么此时占用内存的数据就会丢失。2.  如果是Execution借了storage的内存，那么当storage需使用时，Execution并不会把内存还给storage，那么此时storage的数据就会溢写磁盘，如果不能溢写的话，那么就会丢失或淘汰。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql">-- 面试题：1. 动态占用机制图是什么情况？2. 为什么cache为丢失数据？3. 阶段的划分4. task的发送<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
