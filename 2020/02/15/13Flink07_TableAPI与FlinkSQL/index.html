<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Table API 和 Flink SQL, 觉浅">
    <meta name="description" content="Jinxin Li的个人博客">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Table API 和 Flink SQL | 觉浅</title>
    <link rel="icon" type="image/png" href="/favicon.png">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">

    <script src="/libs/jquery/jquery.min.js"></script>

<meta name="generator" content="Hexo 5.3.0"></head>




<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">觉浅</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/contact" class="waves-effect waves-light">
      
      <i class="fas fa-comments" style="zoom: 0.6;"></i>
      
      <span>留言板</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>友情链接</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">觉浅</div>
        <div class="logo-desc">
            
            Jinxin Li的个人博客
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/contact" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-comments"></i>
			
			留言板
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			友情链接
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="http://github.com/fourgold/" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="http://github.com/fourgold/" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/featureimages/6.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Table API 和 Flink SQL</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        width: 345px;
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        padding: 35px 0 15px 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content {
        padding-bottom: 30px;
        overflow: auto;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;

        position: absolute;
        right: 23.5vw;
        display: block;
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 15px;
        bottom: 76px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                          <div class="article-tag">
                            <span class="chip bg-color">无标签</span>
                          </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2020-02-15
                </div>
                

                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    14.7k
                </div>
                

                

                
            </div>
        </div>
        <hr class="clearfix">

        
        <!-- 是否加载使用自带的 prismjs. -->
        <link rel="stylesheet" href="/libs/prism/prism.css">
        

        
        <!-- 代码块折行 -->
        <style type="text/css">
            code[class*="language-"], pre[class*="language-"] { white-space: pre-wrap !important; }
        </style>
        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <h1 id="第一章-整体介绍"><a href="#第一章-整体介绍" class="headerlink" title="第一章 整体介绍"></a>第一章 整体介绍</h1><h2 id="1-1-什么是-Table-API-和-Flink-SQL"><a href="#1-1-什么是-Table-API-和-Flink-SQL" class="headerlink" title="1.1 什么是 Table API 和 Flink SQL"></a>1.1 什么是 Table API 和 Flink SQL</h2><p>Flink本身是批流统一的处理框架，所以Table API和SQL，就是批流统一的上层处理API。</p>
<p>目前功能尚未完善，处于活跃的开发阶段。</p>
<p>Table API是一套内嵌在Java和Scala语言中的查询API，它允许我们以非常直观的方式，组合来自一些关系运算符的查询（比如select、filter和join）。</p>
<p>而对于Flink SQL，就是直接可以在代码中写SQL，来实现一些查询（Query）操作。Flink的SQL支持，基于实现了SQL标准的Apache Calcite（Apache开源SQL解析工具）。</p>
<p>无论输入是批输入还是流式输入，在这两套API中，指定的查询都具有相同的语义，得到相同的结果。</p>
<h2 id="1-2-需要引入的依赖"><a href="#1-2-需要引入的依赖" class="headerlink" title="1.2 需要引入的依赖"></a>1.2 需要引入的依赖</h2><p>Table API和SQL需要引入的依赖有两个：planner和bridge。</p>
<p>&lt;**dependency**&gt;<br>   &lt;**groupId**&gt;org.apache.flink&lt;/**groupId**&gt;<br>   &lt;**artifactId**&gt;flink-table-planner_2.12&lt;/**artifactId**&gt;<br>   &lt;**version**&gt;1.10.1&lt;/**version**&gt;<br> &lt;/**dependency**&gt;</p>
<p>&lt;**dependency**&gt;<br>   &lt;**groupId**&gt;org.apache.flink&lt;/**groupId**&gt;<br>   &lt;**artifactId**&gt;flink-table-api-java-bridge_2.12&lt;/**artifactId**&gt;<br>   &lt;**version**&gt;1.10.1&lt;/**version**&gt;<br> &lt;/**dependency**&gt;</p>
<p>flink-table-planner：planner计划器，是table API最主要的部分，提供了运行时环境和生成程序执行计划的planner；</p>
<p>flink-table-api-scala-bridge, flink-table-api-java-bridge：bridge桥接器，主要负责table API和 DataStream/DataSet API的连接支持，按照语言分java和scala。</p>
<p>这里的两个依赖，是IDE环境下运行需要添加的；如果是生产环境，lib目录下默认已经有了planner，就只需要有bridge就可以了。</p>
<p>当然，如果想使用用户自定义函数，或是跟kafka做连接，需要有一个SQL client，这个包含在flink-table-common里。</p>
<h3 id="1-2-1-牛刀小试"><a href="#1-2-1-牛刀小试" class="headerlink" title="1.2.1 牛刀小试"></a>1.2.1 牛刀小试</h3><p>  <strong>import</strong> com.atguigu.bean.SensorReading;   <strong>import</strong> org.apache.flink.streaming.api.datastream.DataStreamSource;   <strong>import</strong> org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;   <strong>import</strong> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;   <strong>import</strong> org.apache.flink.table.api.Table;   <strong>import</strong> org.apache.flink.table.api.java.StreamTableEnvironment;   <strong>import</strong> org.apache.flink.types.Row;      <strong>public class</strong> FlinkSQL_Test01 {        <strong>public static void</strong> main(String[]  args) <strong>throws</strong> Exception {          <em>//1.**获取执行环境并设置并行度 *      StreamExecutionEnvironment  env = StreamExecutionEnvironment.*getExecutionEnvironment</em>();       env.setParallelism(1);          <em>//2.**读取文本数据创建流 *      DataStreamSource<String>  readTextFile = env.readTextFile(*</em>“sensor”<strong>);          *//3.<strong>将每一行数据转换为</strong>JavaBean *      SingleOutputStreamOperator<SensorReading>  sensorDataStream = readTextFile.map(line -&gt; {         String[] fields =  line.split(</strong>“,”<strong>);         **return new</strong> SensorReading(fields[0], Long.<em>parseLong</em>(fields[1]), Double.<em>parseDouble</em>(fields[2]));       });          <em>//4.<strong>创建</strong>TableAPI**执行环境 *      StreamTableEnvironment  tableEnv = StreamTableEnvironment.*create</em>(env);          <em>//5.**从流中创建表 *      Table table =  tableEnv.fromDataStream(sensorDataStream);          *//6.**转换数据 *      *//6.1</em> <em>使用<strong>TableAPI</strong>转换数据 *      Table result =  table.select(*</em>“id,temp”<strong>).filter(</strong>“id =  ‘sensor_1’”<strong>);          <em>//6.2</em> *使用<strong>FlinkSQL</strong>转换数据 *      tableEnv.createTemporaryView(</strong>“sensor”<strong>,  sensorDataStream);       Table sqlResult =  tableEnv.sqlQuery(</strong>“select id,temp from sensor where  id=’sensor_1’”<strong>);          <em>//7.**转换为流输出数据 *      tableEnv.toAppendStream(result,  Row.*</em>class</strong>).print(<strong>“result”</strong>);         tableEnv.toAppendStream(sqlResult, Row.<strong>class</strong>).print(<strong>“sql”</strong>);          *//8.**启动任务 *      env.execute();     }      }  </p>
<h2 id="1-3-两种planner（old-amp-blink）的区别"><a href="#1-3-两种planner（old-amp-blink）的区别" class="headerlink" title="1.3 两种planner（old &amp; blink）的区别"></a>1.3 两种planner（old &amp; blink）的区别</h2><p>\1. 批流统一：Blink将批处理作业，视为流式处理的特殊情况。所以，blink不支持表和DataSet之间的转换，批处理作业将不转换为DataSet应用程序，而是跟流处理一样，转换为DataStream程序来处理。</p>
<p>\2. 因为批流统一，Blink planner也不支持BatchTableSource，而使用有界的StreamTableSource代替。</p>
<p>\3. Blink planner只支持全新的目录，不支持已弃用的ExternalCatalog。</p>
<p>\4. 旧planner和Blink planner的FilterableTableSource实现不兼容。旧的planner会把PlannerExpressions下推到filterableTableSource中，而blink planner则会把Expressions下推。</p>
<p>\5. 基于字符串的键值配置选项仅适用于Blink planner。</p>
<p>\6. PlannerConfig在两个planner中的实现不同。</p>
<p>\7. Blink planner会将多个sink优化在一个DAG中（仅在TableEnvironment上受支持，而在StreamTableEnvironment上不受支持）。而旧planner的优化总是将每一个sink放在一个新的DAG中，其中所有DAG彼此独立。</p>
<p>\8. 旧的planner不支持目录统计，而Blink planner支持。</p>
<p>\9. 使用Blink所需依赖</p>
<p>  &lt;**dependency**&gt;     &lt;**groupId**&gt;org.apache.flink&lt;/**groupId**&gt;     &lt;**artifactId**&gt;flink-table-planner-blink_2.12&lt;/**artifactId**&gt;     &lt;**version**&gt;1.10.1&lt;/**version**&gt;   &lt;/**dependency**&gt;  </p>
<h1 id="第二章-API调用"><a href="#第二章-API调用" class="headerlink" title="第二章 API调用"></a>第二章 API调用</h1><h2 id="2-1-基本程序结构"><a href="#2-1-基本程序结构" class="headerlink" title="2.1 基本程序结构"></a>2.1 基本程序结构</h2><p>Table API 和 SQL 的程序结构，与流式处理的程序结构类似；也可以近似地认为有这么几步：首先创建执行环境，然后定义source、transform和sink。</p>
<p>具体操作流程如下：</p>
<p>StreamTableEnvironment tableEnv = …   <em>//</em> <em>创建表的执行环境<br>\</em> <em>//</em> <em>创建一张表，用于读取数据<br>\</em> tableEnv.connect(…).createTemporaryTable(<strong>“inputTable”</strong>);<br> <em>//</em> <em>注册一张表，用于把计算结果输出<br>\</em> tableEnv.connect(…).createTemporaryTable(<strong>“outputTable”</strong>);<br> <em>//</em> <em>通过</em> <em>Table API</em> <em>查询算子，得到一张结果表<br>\</em> Table result = tableEnv.from(<strong>“inputTable”</strong>).select(…);<br> <em>//</em> <em>通过</em> <em>SQL**查询语句，得到一张结果表<br>\</em> Table sqlResult = tableEnv.sqlQuery(<strong>“SELECT … FROM inputTable …”</strong>);<br> <em>//</em> <em>将结果表写入输出表中<br>\</em> result.insertInto(<strong>“outputTable”</strong>);</p>
<h2 id="2-2-创建表环境"><a href="#2-2-创建表环境" class="headerlink" title="2.2 创建表环境"></a>2.2 创建表环境</h2><p>创建表环境最简单的方式，就是基于流处理执行环境，调create方法直接创建：</p>
<p>StreamTableEnvironment tableEnv = StreamTableEnvironment.<em>create</em>(env);</p>
<p>表环境（TableEnvironment）是flink中集成Table API &amp; SQL的核心概念。它负责:</p>
<p>l 注册catalog</p>
<p>l 在内部 catalog 中注册表</p>
<p>l 执行 SQL 查询</p>
<p>l 注册用户自定义函数</p>
<p>l 将 DataStream 或 DataSet 转换为表</p>
<p>l 保存对 ExecutionEnvironment 或 StreamExecutionEnvironment 的引用</p>
<p>在创建TableEnv的时候，可以多传入一个EnvironmentSettings或者TableConfig参数，可以用来配置 TableEnvironment的一些特性。比如：</p>
<p>配置老版本的流式查询（Flink-Streaming-Query）：</p>
<p>EnvironmentSettings settings = EnvironmentSettings.<em>newInstance</em>()<br>  .useOldPlanner()   <em>//</em> <em>使用老版本**planner<br>\</em>  .inStreamingMode()  <em>//</em> <em>流处理模式<br>\</em>  .build();<br> StreamTableEnvironment tableEnv = StreamTableEnvironment.<em>create</em>(env, settings);</p>
<p>基于老版本的批处理环境（Flink-Batch-Query）：</p>
<p>ExecutionEnvironment batchEnv = ExecutionEnvironment.<em>getExecutionEnvironment;</em></p>
<p>BatchTableEnvironment batchTableEnv = BatchTableEnvironment.<em>create</em>(batchEnv);</p>
<p>基于blink版本的流处理环境（Blink-Streaming-Query）：</p>
<p>EnvironmentSettings bsSettings = EnvironmentSettings.<em>newInstance</em>()</p>
<p>.useBlinkPlanner()</p>
<p>.inStreamingMode()</p>
<p>.build();<br> StreamTableEnvironment bsTableEnv = StreamTableEnvironment.<em>create</em>(env, bsSettings);</p>
<p>基于blink版本的批处理环境（Blink-Batch-Query）：</p>
<p>EnvironmentSettings bbSettings = EnvironmentSettings.<em>newInstance</em>()</p>
<p>.useBlinkPlanner()</p>
<p>.inBatchMode()</p>
<p>.build();<br> TableEnvironment bbTableEnv = TableEnvironment.<em>create</em>(bbSettings);</p>
<h2 id="2-3-在Catalog中注册表"><a href="#2-3-在Catalog中注册表" class="headerlink" title="2.3 在Catalog中注册表"></a>2.3 在Catalog中注册表</h2><h3 id="2-3-1-表（Table）的概念"><a href="#2-3-1-表（Table）的概念" class="headerlink" title="2.3.1 表（Table）的概念"></a>2.3.1 表（Table）的概念</h3><p>TableEnvironment可以注册目录Catalog，并可以基于Catalog注册表。它会维护一个Catalog-Table表之间的map。</p>
<p>表（Table）是由一个“标识符”来指定的，由3部分组成：Catalog名、数据库（database）名和对象名（表名）。如果没有指定目录或数据库，就使用当前的默认值。</p>
<p>表可以是常规的（Table，表），或者虚拟的（View，视图）。</p>
<p>常规表（Table）一般可以用来描述外部数据，比如文件、数据库表或消息队列的数据，也可以直接从 DataStream转换而来。</p>
<p>视图可以从现有的表中创建，通常是table API或者SQL查询的一个结果。</p>
<h3 id="2-3-2-读取文件数据（Csv格式）"><a href="#2-3-2-读取文件数据（Csv格式）" class="headerlink" title="2.3.2 读取文件数据（Csv格式）"></a>2.3.2 读取文件数据（Csv格式）</h3><p>连接外部系统在Catalog中注册表，直接调用tableEnv.connect()就可以，里面参数要传入一个ConnectorDescriptor，也就是connector描述器。对于文件系统的connector而言，flink内部已经提供了，就叫做FileSystem()。</p>
<p>代码如下：</p>
<p>  <strong>import</strong> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;   <strong>import</strong> org.apache.flink.table.api.DataTypes;   <strong>import</strong> org.apache.flink.table.api.Table;   <strong>import</strong> org.apache.flink.table.api.java.StreamTableEnvironment;   <strong>import</strong> org.apache.flink.table.descriptors.FileSystem;   <strong>import</strong> org.apache.flink.table.descriptors.OldCsv;   <strong>import</strong> org.apache.flink.table.descriptors.Schema;   <strong>import</strong> org.apache.flink.types.Row;      <strong>public class</strong> FlinkSQL_Test02 {        <strong>public static void</strong> main(String[]  args) <strong>throws</strong> Exception {          <em>//1.**获取执行环境 *      StreamExecutionEnvironment  env = StreamExecutionEnvironment.*getExecutionEnvironment</em>();          <em>//2.**设置并行度 *      env.setParallelism(1);          *//3.**创建 *      StreamTableEnvironment  tableEnv = StreamTableEnvironment.*create</em>(env);          <em>//4.**读取文件数据创建表 *      tableEnv.connect(*</em>new** FileSystem().path(<strong>“input/sensor.txt”</strong>))           .withFormat(<strong>new</strong> OldCsv())           .withSchema(<strong>new</strong> Schema().field(<strong>“id”</strong>, DataTypes.<em>STRING</em>())               .field(<strong>“ts”</strong>, DataTypes.<em>BIGINT</em>())               .field(<strong>“temp”</strong>, DataTypes.<em>DOUBLE</em>()))           .createTemporaryTable(<strong>“inputTable”</strong>);          <em>//5.<strong>执行</strong>SQL**查询数据 *      Table table =  tableEnv.sqlQuery(*</em>“select id,temp from inputTable where  id=’sensor_1’”<strong>);          <em>//6.**将表转换为追加流并打印 *      tableEnv.toAppendStream(table,  Row.*</em>class</strong>).print();          *//7.**执行任务 *      env.execute();     }      }  </p>
<p>这是旧版本的csv格式描述器。由于它是非标的，跟外部系统对接并不通用，所以将被弃用，以后会被一个符合RFC-4180标准的新format描述器取代。新的描述器就叫Csv()，但flink没有直接提供，需要引入依赖flink-csv：</p>
<p>&lt;**dependency**&gt;<br>   &lt;**groupId**&gt;org.apache.flink&lt;/**groupId**&gt;<br>   &lt;**artifactId**&gt;flink-csv&lt;/**artifactId**&gt;<br>   &lt;**version**&gt;1.10.1&lt;/**version**&gt;<br> &lt;/**dependency**&gt;</p>
<p>代码非常类似，只需要把withFormat里的OldCsv改成Csv就可以了。</p>
<h3 id="2-3-3-读取Kafka数据"><a href="#2-3-3-读取Kafka数据" class="headerlink" title="2.3.3 读取Kafka数据"></a>2.3.3 读取Kafka数据</h3><p>kafka的连接器flink-kafka-connector中，1.10版本的已经提供了Table API的支持。我们可以在 connect方法中直接传入一个叫做Kafka的类，这就是kafka连接器的描述器ConnectorDescriptor。</p>
<p>  <strong>import</strong> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;   <strong>import</strong> org.apache.flink.table.api.DataTypes;   <strong>import</strong> org.apache.flink.table.api.Table;   <strong>import</strong> org.apache.flink.table.api.java.StreamTableEnvironment;   <strong>import</strong> org.apache.flink.table.descriptors.Csv;   <strong>import</strong> org.apache.flink.table.descriptors.Kafka;   <strong>import</strong> org.apache.flink.table.descriptors.Schema;   <strong>import</strong> org.apache.flink.types.Row;      <strong>public class</strong> FlinkSQL_Test03 {        <strong>public static void</strong> main(String[]  args) <strong>throws</strong> Exception {          <em>//1.**创建执行环境 *      StreamExecutionEnvironment  env = StreamExecutionEnvironment.*getExecutionEnvironment</em>();       env.setParallelism(1);       StreamTableEnvironment tableEnv  = StreamTableEnvironment.<em>create</em>(env);          <em>//2.<strong>定义</strong>Kafka**连接描述器 *      Kafka kafka = *</em>new** Kafka()           .version(<strong>“0.11”</strong>)           .topic(<strong>“test”</strong>)           .property(<strong>“bootstrap.servers”</strong>, <strong>“hadoop102:9092”</strong>);          <em>//3.<strong>定义表的</strong>Schema**信息 *      Schema schema = *</em>new** Schema()           .field(<strong>“id”</strong>, DataTypes.<em>STRING</em>())           .field(<strong>“ts”</strong>, DataTypes.<em>BIGINT</em>())           .field(<strong>“temp”</strong>, DataTypes.<em>DOUBLE</em>());          <em>//4.<strong>读取</strong>Kafka**数据创建临时表 *      tableEnv.connect(kafka).withFormat(*</em>new** Csv()).withSchema(schema).createTemporaryTable(<strong>“KafkaTable”</strong>);          <em>//5.<strong>执行</strong>SQL**查询数据 *      Table table =  tableEnv.sqlQuery(*</em>“select id,temp from KafkaTable where  id=’sensor_1’”<strong>);          <em>//6.**将表转换为追加流进行打印 *      tableEnv.toAppendStream(table,  Row.*</em>class</strong>).print();          *//7.**执行任务 *      env.execute();        }      }  </p>
<p>当然也可以连接到ElasticSearch、MySql、HBase、Hive等外部系统，实现方式基本上是类似的。</p>
<h2 id="2-4-表的查询"><a href="#2-4-表的查询" class="headerlink" title="2.4 表的查询"></a>2.4 表的查询</h2><p>利用外部系统的连接器connector，我们可以读写数据，并在环境的Catalog中注册表。接下来就可以对表做查询转换了。</p>
<p>Flink给我们提供了两种查询方式：Table API和 SQL。</p>
<h3 id="2-4-1-Table-API的调用"><a href="#2-4-1-Table-API的调用" class="headerlink" title="2.4.1 Table API的调用"></a>2.4.1 Table API的调用</h3><p>Table API是集成在Scala和Java语言内的查询API。与SQL不同，Table API的查询不会用字符串表示，而是在宿主语言中一步一步调用完成的。</p>
<p>Table API基于代表一张“表”的Table类，并提供一整套操作处理的方法API。这些方法会返回一个新的Table对象，这个对象就表示对输入表应用转换操作的结果。有些关系型转换操作，可以由多个方法调用组成，构成链式调用结构。例如table.select(…).filter(…)，其中select（…）表示选择表中指定的字段，filter(…)表示筛选条件。</p>
<p>代码中的实现如下：</p>
<p>Table sensorTable = tableEnv.from(<strong>“inputTable”</strong>);</p>
<p>Table resultTable = senorTable</p>
<p>.select(<strong>“id, temperature”</strong>)</p>
<p>.filter(<strong>“id =’sensor_1’”</strong>);</p>
<h3 id="2-4-2-SQL查询"><a href="#2-4-2-SQL查询" class="headerlink" title="2.4.2 SQL查询"></a>2.4.2 SQL查询</h3><p>Flink的SQL集成，基于的是ApacheCalcite，它实现了SQL标准。在Flink中，用常规字符串来定义SQL查询语句。SQL 查询的结果，是一个新的 Table。</p>
<p>代码实现如下：</p>
<p>Table resultSqlTable = tableEnv.sqlQuery(<strong>“select id, temperature from inputTable where id =’sensor_1’”</strong>);</p>
<p>当然，也可以加上聚合操作，比如我们统计每个sensor温度数据出现的个数，做个count统计：</p>
<p>Table aggResultTable = sensorTable</p>
<p>.groupBy(<strong>“id”</strong>)</p>
<p>.select(<strong>“id</strong>, <strong>id</strong>.count as <strong>count”</strong>);</p>
<p>SQL的实现：</p>
<p>Table aggResultSqlTable = tableEnv.sqlQuery(<strong>“select id, count(id) as cnt from inputTable group by id”</strong>);</p>
<h2 id="2-5-将DataStream-转换成表"><a href="#2-5-将DataStream-转换成表" class="headerlink" title="2.5 将DataStream 转换成表"></a>2.5 将DataStream 转换成表</h2><p>Flink允许我们把Table和DataStream做转换：我们可以基于一个DataStream，先流式地读取数据源，然后map成POJO，再把它转成Table。Table的列字段（column fields），就是POJO里的字段，这样就不用再麻烦地定义schema了。</p>
<h3 id="2-5-1-代码表达"><a href="#2-5-1-代码表达" class="headerlink" title="2.5.1 代码表达"></a>2.5.1 代码表达</h3><p>代码中实现非常简单，直接用tableEnv.fromDataStream()就可以了。默认转换后的 Table schema 和 DataStream 中的字段定义一一对应，也可以单独指定出来。</p>
<p>这就允许我们更换字段的顺序、重命名，或者只选取某些字段出来，相当于做了一次map操作（或者Table API的 select操作）。</p>
<p>代码具体如下：</p>
<pre class="line-numbers language-none"><code class="language-none">DataStream&lt;String&gt; inputStream &#x3D; env.readTextFile(&quot;sensor.txt&quot;);
DataStream&lt;SensorReading&gt; dataStream &#x3D; inputStream
        .map( line -&gt; &#123;
            String[] fields &#x3D; line.split(&quot;,&quot;);
            return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2]));
        &#125; );<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>


<p>Table sensorTable = tableEnv.fromDataStream(dataStream, <strong>“id, timestamp.rowtime as ts, temperature”</strong>);</p>
<h3 id="2-5-2-数据类型与-Table-schema的对应"><a href="#2-5-2-数据类型与-Table-schema的对应" class="headerlink" title="2.5.2 数据类型与 Table schema的对应"></a>2.5.2 数据类型与 Table schema的对应</h3><p>在上节的例子中，DataStream 中的数据类型，与表的 Schema 之间的对应关系，是按照类中的字段名来对应的（name-based mapping），所以还可以用as做重命名。</p>
<p>基于名称的对应：</p>
<p>Table sensorTable = tableEnv.fromDataStream(dataStream, <strong>“timestamp</strong> as <strong>ts</strong>, <strong>id</strong> as <strong>myId</strong>, <strong>temperature”</strong>);</p>
<p>Flink的DataStream和 DataSet API支持多种类型。</p>
<p>组合类型，比如元组（内置Scala和Java元组）、POJO、Scala case类和Flink的Row类型等，允许具有多个字段的嵌套数据结构，这些字段可以在Table的表达式中访问。其他类型，则被视为原子类型。</p>
<h2 id="2-6-创建临时视图（Temporary-View）"><a href="#2-6-创建临时视图（Temporary-View）" class="headerlink" title="2.6. 创建临时视图（Temporary View）"></a>2.6. 创建临时视图（Temporary View）</h2><p>创建临时视图的第一种方式，就是直接从DataStream转换而来。同样，可以直接对应字段转换；也可以在转换的时候，指定相应的字段。</p>
<p>代码如下：</p>
<p>tableEnv.createTemporaryView(<strong>“sensorView”</strong>, dataStream);<br> tableEnv.createTemporaryView(<strong>“sensorView”</strong>, dataStream, <strong>“id</strong>, <strong>temperature</strong>, <strong>timestamp</strong> as <strong>ts”</strong>);</p>
<p>另外，当然还可以基于Table创建视图：</p>
<p>tableEnv.createTemporaryView(<strong>“sensorView”</strong>, sensorTable);</p>
<p>View和Table的Schema完全相同。事实上，在Table API中，可以认为View和Table是等价的。</p>
<h2 id="2-7-输出表"><a href="#2-7-输出表" class="headerlink" title="2.7. 输出表"></a>2.7. 输出表</h2><p>表的输出，是通过将数据写入 TableSink 来实现的。TableSink 是一个通用接口，可以支持不同的文件格式、存储数据库和消息队列。</p>
<p>具体实现，输出表最直接的方法，就是通过 Table.insertInto() 方法将一个 Table 写入注册过的 TableSink 中。</p>
<h3 id="2-7-1-将表转换成DataStream打印"><a href="#2-7-1-将表转换成DataStream打印" class="headerlink" title="2.7.1 将表转换成DataStream打印"></a>2.7.1 将表转换成DataStream打印</h3><p>表可以转换为DataStream或DataSet。这样，自定义流处理或批处理程序就可以继续在 Table API或SQL查询的结果上运行了。</p>
<p>将表转换为DataStream或DataSet时，需要指定生成的数据类型，即要将表的每一行转换成的数据类型。通常，最方便的转换类型就是Row。当然，因为结果的所有字段类型都是明确的，我们也经常会用元组类型来表示。</p>
<p>代码实现如下：</p>
<p>  <strong>import</strong> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;   <strong>import</strong> org.apache.flink.table.api.DataTypes;   <strong>import</strong> org.apache.flink.table.api.Table;   <strong>import</strong> org.apache.flink.table.api.java.StreamTableEnvironment;   <strong>import</strong> org.apache.flink.table.descriptors.FileSystem;   <strong>import</strong> org.apache.flink.table.descriptors.OldCsv;   <strong>import</strong> org.apache.flink.table.descriptors.Schema;      <strong>import</strong> org.apache.flink.types.Row;      <strong>public class</strong> FlinkSQL_Test04 {        <strong>public static void</strong> main(String[]  args) <strong>throws</strong> Exception {       StreamExecutionEnvironment env  = StreamExecutionEnvironment.<em>getExecutionEnvironment</em>();       env.setParallelism(1);          <em>//1.</em> <em>创建表环境 *      StreamTableEnvironment  tableEnv = StreamTableEnvironment.*create</em>(env);          <em>//2.</em> <em>连接外部系统，读取数据，创建表 *      *//2.1</em> <em>读取文件 *      String filePath  = *</em>“D:<strong><strong>\</strong></strong>Projects*<strong><em>\*</em></strong>BigData*<strong><em>\*</em></strong>FlinkTurtorial*<strong><em>\*</em></strong>src*<strong><em>\*</em></strong>main*<strong><em>\*</em></strong>resources*<strong><em>\*</em></strong>sensor.txt”<strong>;       tableEnv.connect(**new</strong> FileSystem().path(filePath))           .withFormat(<strong>new</strong> OldCsv())           .withSchema(               <strong>new</strong> Schema()                   .field(<strong>“id”</strong>, DataTypes.<em>STRING</em>())                   .field(<strong>“ts”</strong>, DataTypes.<em>BIGINT</em>())                   .field(<strong>“temp”</strong>, DataTypes.<em>DOUBLE</em>())           )           .createTemporaryTable(<strong>“inputTable”</strong>);          <em>//3.</em> <em>查询转换 *      *//3.1</em> <em>简单查询 *      *//3.1.1 Table API *      Table inputTable =  tableEnv.from(*</em>“inputTable”<strong>);       Table resultTable =  inputTable.select(</strong>“id, tem”<strong>)           .filter(</strong>“id ===  ‘sensor_1’”<strong>);          *//3.1.2 SQL *      Table  resultSqlTable = tableEnv.sqlQuery(</strong>“select id,  temperature from inputTable where id = ‘sensor_1’”<strong>);          <em>//3.2</em> *聚合统计 *      *//3.2.1 Table API *      Table aggTable =  inputTable           .groupBy(</strong>“id”<strong>)           .select(</strong>“id,  id.count as count”<strong>);                *//3.2.2 SQL *      Table aggSqlTable =  tableEnv.sqlQuery(</strong>“select id, count(id) as ct from  inputTable group by id”<strong>);          <em>// 4.</em> <em>转换成流打印输出 *      tableEnv.toAppendStream(resultTable,  Row.*</em>class</strong>).print(<strong>“result”</strong>);         tableEnv.toAppendStream(resultSqlTable, Row.<strong>class</strong>).print(<strong>“resultSQL”</strong>);       tableEnv.toRetractStream(aggTable, Row.<strong>class</strong>).print(<strong>“agg”</strong>);       tableEnv.toRetractStream(aggSqlTable,  Row.<strong>class</strong>).print(<strong>“aggSQL”</strong>);          env.execute();     }   }  </p>
<p>表作为流式查询的结果，是动态更新的。所以，将这种动态查询转换成的数据流，同样需要对表的更新操作进行编码，进而有不同的转换模式。</p>
<p>Table API中表到DataStream有两种模式：</p>
<p>l 追加模式（Append Mode）</p>
<p>用于表只会被插入（Insert）操作更改的场景。</p>
<p>l 撤回模式（Retract Mode）</p>
<p>用于任何场景。有些类似于更新模式中Retract模式，它只有Insert和Delete两类操作。</p>
<p>得到的数据会增加一个Boolean类型的标识位（返回的第一个字段），用它来表示到底是新增的数据（Insert），还是被删除的数据（老数据， Delete）。</p>
<p>所以，没有经过groupby之类聚合操作，可以直接用 toAppendStream 来转换；而如果经过了聚合，有更新操作，一般就必须用 toRetractDstream。</p>
<h3 id="2-7-2-输出到文件"><a href="#2-7-2-输出到文件" class="headerlink" title="2.7.2 输出到文件"></a>2.7.2 输出到文件</h3><p>代码如下：</p>
<p>  <strong>import</strong> com.atguigu.bean.SensorReading;   <strong>import</strong> org.apache.flink.streaming.api.datastream.DataStreamSource;   <strong>import</strong> org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;   <strong>import</strong> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;   <strong>import</strong> org.apache.flink.table.api.DataTypes;   <strong>import</strong> org.apache.flink.table.api.Table;   <strong>import</strong> org.apache.flink.table.api.java.StreamTableEnvironment;   <strong>import</strong> org.apache.flink.table.descriptors.Csv;   <strong>import</strong> org.apache.flink.table.descriptors.FileSystem;   <strong>import</strong> org.apache.flink.table.descriptors.Schema;      <strong>public class</strong> FlinkSQL_Test05 {        <strong>public static void</strong> main(String[]  args) <strong>throws</strong> Exception {          <em>//1.**获取执行环境并设置并行度 *      StreamExecutionEnvironment  env = StreamExecutionEnvironment.*getExecutionEnvironment</em>();       env.setParallelism(1);          <em>//2.**读取文本数据创建流 *      DataStreamSource<String>  readTextFile = env.readTextFile(*</em>“sensor”<strong>);          *//3.<strong>将每一行数据转换为</strong>JavaBean *      SingleOutputStreamOperator<SensorReading>  sensorDataStream = readTextFile.map(line -&gt; {         String[] fields =  line.split(</strong>“,”<strong>);         **return new</strong> SensorReading(fields[0], Long.<em>parseLong</em>(fields[1]), Double.<em>parseDouble</em>(fields[2]));       });          <em>//4.<strong>创建</strong>TableAPI**执行环境 *      StreamTableEnvironment  tableEnv = StreamTableEnvironment.*create</em>(env);          <em>//5.**从流中创建表 *      Table table =  tableEnv.fromDataStream(sensorDataStream);          *//6.**转换数据 *      Table result =  table.select(*</em>“id,temperature”<strong>).filter(</strong>“id =  ‘sensor_1’”<strong>);          <em>//7.**将数据写入本地文件系统 *      tableEnv.connect(           *</em>new</strong> FileSystem().path(<strong>“output/out.txt”</strong>)) <em>//**定义文件系统连接,文件不能存在 *          .withFormat(*</em>new** Csv()) <em>//</em> <em>定义格式化方法，<strong>Csv</strong>格式 *          .withSchema(*</em>new** Schema()               .field(<strong>“id”</strong>, DataTypes.<em>STRING</em>())               .field(<strong>“temp”</strong>, DataTypes.<em>DOUBLE</em>())           ) <em>//</em> <em>定义表结构 *          .createTemporaryTable(*</em>“outputTable”<strong>); <em>//</em> *创建临时表    *      tableEnv.insertInto(</strong>“outputTable”*<em>, result);          <em>//8.</em></em>执行任务 *      env.execute();        }   }  </p>
<h3 id="2-7-3-更新模式（Update-Mode）"><a href="#2-7-3-更新模式（Update-Mode）" class="headerlink" title="2.7.3 更新模式（Update Mode）"></a>2.7.3 更新模式（Update Mode）</h3><p>在流处理过程中，表的处理并不像传统定义的那样简单。</p>
<p>对于流式查询（Streaming Queries），需要声明如何在（动态）表和外部连接器之间执行转换。与外部系统交换的消息类型，由<strong>更新模式</strong>（update mode）指定。</p>
<p>Flink Table API中的更新模式有以下三种：</p>
<h4 id="1）追加模式（Append-Mode）"><a href="#1）追加模式（Append-Mode）" class="headerlink" title="1）追加模式（Append Mode）"></a>1）追加模式（Append Mode）</h4><p>在追加模式下，表（动态表）和外部连接器只交换插入（Insert）消息。</p>
<h4 id="2）撤回模式（Retract-Mode）"><a href="#2）撤回模式（Retract-Mode）" class="headerlink" title="2）撤回模式（Retract Mode）"></a>2）撤回模式（Retract Mode）</h4><p>在撤回模式下，表和外部连接器交换的是：添加（Add）和撤回（Retract）消息。</p>
<p>l 插入（Insert）会被编码为添加消息；</p>
<p>l 删除（Delete）则编码为撤回消息；</p>
<p>l 更新（Update）则会编码为，已更新行（上一行）的撤回消息，和更新行（新行）的添加消息。</p>
<p>在此模式下，不能定义key，这一点跟upsert模式完全不同。</p>
<h4 id="3）Upsert（更新插入）模式"><a href="#3）Upsert（更新插入）模式" class="headerlink" title="3）Upsert（更新插入）模式"></a>3）Upsert（更新插入）模式</h4><p>在Upsert模式下，动态表和外部连接器交换Upsert和Delete消息。</p>
<p>这个模式需要一个唯一的key，通过这个key可以传递更新消息。为了正确应用消息，外部连接器需要知道这个唯一key的属性。</p>
<p>l 插入（Insert）和更新（Update）都被编码为Upsert消息；</p>
<p>l 删除（Delete）编码为Delete信息。</p>
<p>这种模式和Retract模式的主要区别在于，Update操作是用单个消息编码的，所以效率会更高。</p>
<h3 id="2-7-4-输出到Kafka"><a href="#2-7-4-输出到Kafka" class="headerlink" title="2.7.4 输出到Kafka"></a>2.7.4 输出到Kafka</h3><p>除了输出到文件，也可以输出到Kafka。我们可以结合前面Kafka作为输入数据，构建数据管道，kafka进，kafka出。</p>
<p>代码如下：</p>
<p>  <strong>import</strong> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;   <strong>import</strong> org.apache.flink.table.api.DataTypes;   <strong>import</strong> org.apache.flink.table.api.Table;   <strong>import</strong> org.apache.flink.table.api.java.StreamTableEnvironment;   <strong>import</strong> org.apache.flink.table.descriptors.Csv;   <strong>import</strong> org.apache.flink.table.descriptors.Kafka;   <strong>import</strong> org.apache.flink.table.descriptors.Schema;   <strong>import</strong> org.apache.flink.types.Row;      <strong>public class</strong> FlinkSQL_Test06 {        <strong>public static void</strong> main(String[]  args) <strong>throws</strong> Exception {          <em>//1.**创建执行环境 *      StreamExecutionEnvironment  env = StreamExecutionEnvironment.*getExecutionEnvironment</em>();       env.setParallelism(1);       StreamTableEnvironment tableEnv  = StreamTableEnvironment.<em>create</em>(env);          <em>//2.<strong>定义</strong>Kafka**输入描述器 *      Kafka kafkaInput  = *</em>new** Kafka()           .version(<strong>“0.11”</strong>)           .topic(<strong>“test”</strong>)           .property(<strong>“bootstrap.servers”</strong>, <strong>“hadoop102:9092”</strong>);       Schema schemaInput = <strong>new</strong> Schema()           .field(<strong>“id”</strong>, DataTypes.<em>STRING</em>())           .field(<strong>“ts”</strong>, DataTypes.<em>BIGINT</em>())           .field(<strong>“temp”</strong>, DataTypes.<em>DOUBLE</em>());          <em>//3.<strong>读取</strong>Kafka**数据创建临时表 *      tableEnv.connect(kafkaInput).withFormat(*</em>new** Csv()).withSchema(schemaInput).createTemporaryTable(<strong>“KafkaInput”</strong>);          <em>//4.<strong>执行</strong>SQL**查询数据 *      Table table =  tableEnv.sqlQuery(*</em>“select id,temp from KafkaInput where  id=’sensor_1’”<strong>);          <em>//5.<strong>定义</strong>Kafka**输出连接器 *      Kafka  kafkaOutPut = *</em>new</strong> Kafka()           .version(<strong>“0.11”</strong>)           .topic(<strong>“sinkTest”</strong>)            .property(<strong>“bootstrap.servers”</strong>, <strong>“hadoop102:9092”</strong>);       Schema schemaOutPut = <strong>new</strong> Schema()           .field(<strong>“id”</strong>, DataTypes.<em>STRING</em>())           .field(<strong>“temp”</strong>, DataTypes.<em>DOUBLE</em>());         tableEnv.connect(kafkaOutPut).withFormat(<strong>new</strong> Csv()).withSchema(schemaOutPut).createTemporaryTable(<strong>“KafkaOutPut”</strong>);          <em>//6.<strong>将数据输出到</strong>Kafka**的另一个主题 *      tableEnv.insertInto(*</em>“KafkaOutPut”*<em>, table);          <em>//7.</em></em>执行任务 *      env.execute();        }      }  </p>
<h3 id="2-7-5-输出到ElasticSearch"><a href="#2-7-5-输出到ElasticSearch" class="headerlink" title="2.7.5 输出到ElasticSearch"></a>2.7.5 输出到ElasticSearch</h3><p>ElasticSearch的connector可以在upsert（update+insert，更新插入）模式下操作，这样就可以使用Query定义的键（key）与外部系统交换UPSERT/DELETE消息。</p>
<p>另外，对于“仅追加”（append-only）的查询，connector还可以在append 模式下操作，这样就可以与外部系统只交换insert消息。</p>
<p>es目前支持的数据格式，只有Json，而flink本身并没有对应的支持，所以还需要引入依赖：</p>
<p>&lt;**dependency**&gt;<br>   &lt;**groupId**&gt;org.apache.flink&lt;/**groupId**&gt;<br>   &lt;**artifactId**&gt;flink-json&lt;/**artifactId**&gt;<br>   &lt;**version**&gt;1.10.1&lt;/**version**&gt;<br> &lt;/**dependency**&gt;</p>
<p>代码实现如下：</p>
<p>  <strong>import</strong> com.atguigu.bean.SensorReading;   <strong>import</strong> org.apache.flink.streaming.api.datastream.DataStreamSource;   <strong>import</strong> org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;   <strong>import</strong> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;   <strong>import</strong> org.apache.flink.table.api.DataTypes;   <strong>import</strong> org.apache.flink.table.api.Table;   <strong>import</strong> org.apache.flink.table.api.java.StreamTableEnvironment;   <strong>import</strong> org.apache.flink.table.descriptors.Elasticsearch;   <strong>import</strong> org.apache.flink.table.descriptors.FormatDescriptor;   <strong>import</strong> org.apache.flink.table.descriptors.Json;   <strong>import</strong> org.apache.flink.table.descriptors.Schema;      <strong>import</strong> java.util.Map;      <strong>public class</strong> FlinkSQL_Test07 {        <strong>public static void</strong> main(String[]  args) <strong>throws</strong> Exception {          <em>//1.**获取执行环境并设置并行度 *      StreamExecutionEnvironment  env = StreamExecutionEnvironment.*getExecutionEnvironment</em>();       env.setParallelism(1);          <em>//2.**读取文本数据创建流 *      DataStreamSource<String>  readTextFile = env.readTextFile(*</em>“sensor”<strong>);          *//3.<strong>将每一行数据转换为</strong>JavaBean *      SingleOutputStreamOperator<SensorReading>  sensorDataStream = readTextFile.map(line -&gt; {         String[] fields =  line.split(</strong>“,”<strong>);         **return new</strong> SensorReading(fields[0], Long.<em>parseLong</em>(fields[1]), Double.<em>parseDouble</em>(fields[2]));       });          <em>//4.<strong>创建</strong>TableAPI**执行环境 *      StreamTableEnvironment  tableEnv = StreamTableEnvironment.*create</em>(env);          <em>//5.**从流中创建表 *      Table table =  tableEnv.fromDataStream(sensorDataStream);          *//6.**转换数据 *      tableEnv.createTemporaryView(*</em>“sensor”<strong>, table);       Table sqlQuery =  tableEnv.sqlQuery(</strong>“select id,count(id) as ct from sensor  group by id”<strong>);          <em>//7.<strong>构建</strong>Es**连接器 *      tableEnv.connect(*</em>new</strong> Elasticsearch()           .host(<strong>“hadoop102”</strong>, 9200, <strong>“http”</strong>)           .version(<strong>“6”</strong>)           .index(<strong>“flink_index”</strong>)           .documentType(<strong>“_doc”</strong>))           .inUpsertMode()           .withFormat(<strong>new</strong> Json())           .withSchema(<strong>new</strong> Schema()               .field(<strong>“id”</strong>, DataTypes.<em>STRING</em>())               .field(<strong>“ct”</strong>, DataTypes.<em>BIGINT</em>()))           .createTemporaryTable(<strong>“EsIndex”</strong>);          <em>//8.<strong>将数据写入</strong>ES *      tableEnv.insertInto(*</em>“EsIndex”*<em>, sqlQuery);          <em>//9.</em></em>开启任务 *      env.execute();        }      }  </p>
<h3 id="2-7-6-输出到MySql"><a href="#2-7-6-输出到MySql" class="headerlink" title="2.7.6 输出到MySql"></a>2.7.6 输出到MySql</h3><p>Flink专门为Table API的jdbc连接提供了flink-jdbc连接器，我们需要先引入依赖：</p>
 <pre class="line-numbers language-xml" data-language="xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">></span></span>
   <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>org.apache.flink<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>
   <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>flink-jdbc_2.12<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>
   <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>1.10.1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>
 <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>jdbc连接的代码实现比较特殊，因为没有对应的java/scala类实现ConnectorDescriptor，所以不能直接tableEnv.connect()。不过Flink SQL留下了执行DDL的接口：tableEnv.sqlUpdate()。</p>
<p>对于jdbc的创建表操作，天生就适合直接写DDL来实现，所以我们的代码可以这样写：</p>
 <pre class="line-numbers language-java" data-language="java"><code class="language-java">impor com<span class="token punctuation">.</span>atguigu<span class="token punctuation">.</span>bean<span class="token punctuation">.</span><span class="token class-name">SensorReading</span><span class="token punctuation">;</span>   
<span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>datastream<span class="token punctuation">.</span></span><span class="token class-name">DataStreamSource</span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>datastream<span class="token punctuation">.</span></span><span class="token class-name">SingleOutputStreamOperator</span><span class="token punctuation">;</span>   
<span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>environment<span class="token punctuation">.</span></span><span class="token class-name">StreamExecutionEnvironment</span><span class="token punctuation">;</span>   
<span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>table<span class="token punctuation">.</span>api<span class="token punctuation">.</span></span><span class="token class-name">Table</span><span class="token punctuation">;</span>   
<span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>table<span class="token punctuation">.</span>api<span class="token punctuation">.</span>java<span class="token punctuation">.</span></span><span class="token class-name">StreamTableEnvironment</span><span class="token punctuation">;</span>      
<span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">FlinkSQL_Test08</span> <span class="token punctuation">&#123;</span> <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token class-name">String</span><span class="token punctuation">[</span><span class="token punctuation">]</span>  args<span class="token punctuation">)</span> <span class="token keyword">throws</span> <span class="token class-name">Exception</span> <span class="token punctuation">&#123;</span>          <span class="token operator">*</span><span class="token comment">//1.**获取执行环境并设置并行度 *      </span>
    <span class="token class-name">StreamExecutionEnvironment</span>  env <span class="token operator">=</span> <span class="token class-name">StreamExecutionEnvironment</span><span class="token punctuation">.</span>*getExecutionEnvironment<span class="token operator">*</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>       env<span class="token punctuation">.</span><span class="token function">setParallelism</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>          <span class="token operator">*</span><span class="token comment">//2.**读取文本数据创建流 *      DataStreamSource&lt;String>  readTextFile = env.readTextFile(**"sensor"**);          *//3.**将每一行数据转换为**JavaBean *      </span>
    <span class="token class-name">SingleOutputStreamOperator</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">SensorReading</span><span class="token punctuation">></span></span>  sensorDataStream <span class="token operator">=</span> readTextFile<span class="token punctuation">.</span><span class="token function">map</span><span class="token punctuation">(</span>line <span class="token operator">-></span> <span class="token punctuation">&#123;</span>         <span class="token class-name">String</span><span class="token punctuation">[</span><span class="token punctuation">]</span> fields <span class="token operator">=</span>  line<span class="token punctuation">.</span><span class="token function">split</span><span class="token punctuation">(</span><span class="token operator">*</span><span class="token operator">*</span><span class="token string">","</span><span class="token operator">*</span><span class="token operator">*</span><span class="token punctuation">)</span><span class="token punctuation">;</span>         <span class="token operator">*</span><span class="token operator">*</span><span class="token keyword">return</span> <span class="token keyword">new</span><span class="token operator">*</span><span class="token operator">*</span> <span class="token class-name">SensorReading</span><span class="token punctuation">(</span>fields<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token class-name">Long</span><span class="token punctuation">.</span>*parseLong<span class="token operator">*</span><span class="token punctuation">(</span>fields<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token class-name">Double</span><span class="token punctuation">.</span>*parseDouble<span class="token operator">*</span><span class="token punctuation">(</span>fields<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>       <span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>          <span class="token operator">*</span><span class="token comment">//4.**创建**TableAPI**执行环境 *      StreamTableEnvironment  tableEnv = StreamTableEnvironment.*create*(env);          *//5.**从流中创建表 *     </span>
    <span class="token class-name">Table</span> table <span class="token operator">=</span>  tableEnv<span class="token punctuation">.</span><span class="token function">fromDataStream</span><span class="token punctuation">(</span>sensorDataStream<span class="token punctuation">)</span><span class="token punctuation">;</span>          <span class="token operator">*</span><span class="token comment">//6.**转换数据 *      </span>
    tableEnv<span class="token punctuation">.</span><span class="token function">createTemporaryView</span><span class="token punctuation">(</span><span class="token operator">*</span><span class="token operator">*</span><span class="token string">"sensor"</span><span class="token operator">*</span><span class="token operator">*</span><span class="token punctuation">,</span> table<span class="token punctuation">)</span><span class="token punctuation">;</span>       <span class="token class-name">Table</span> sqlQuery <span class="token operator">=</span>  tableEnv<span class="token punctuation">.</span><span class="token function">sqlQuery</span><span class="token punctuation">(</span><span class="token operator">*</span><span class="token operator">*</span><span class="token string">"select id,count(\*) as ct from sensor  group by id"</span><span class="token operator">*</span><span class="token operator">*</span><span class="token punctuation">)</span><span class="token punctuation">;</span>          <span class="token operator">*</span><span class="token comment">//7.**定义**MySQL**连接器 *      </span>
    <span class="token class-name">String</span> sinkDDL <span class="token operator">=</span>  <span class="token operator">*</span><span class="token operator">*</span><span class="token string">"create table jdbcOutputTable ("</span><span class="token operator">*</span><span class="token operator">*</span> <span class="token operator">+</span>           <span class="token operator">*</span><span class="token operator">*</span><span class="token string">" id  varchar(20) not null, "</span><span class="token operator">*</span><span class="token operator">*</span> <span class="token operator">+</span>           <span class="token operator">*</span><span class="token operator">*</span><span class="token string">" ct bigint  not null "</span><span class="token operator">*</span><span class="token operator">*</span> <span class="token operator">+</span>           <span class="token operator">*</span><span class="token operator">*</span><span class="token string">") with  ("</span><span class="token operator">*</span><span class="token operator">*</span> <span class="token operator">+</span>           <span class="token operator">*</span><span class="token operator">*</span><span class="token string">"  'connector.type' = 'jdbc', "</span><span class="token operator">*</span><span class="token operator">*</span> <span class="token operator">+</span>           <span class="token operator">*</span><span class="token operator">*</span><span class="token string">"  'connector.url' = 'jdbc:mysql://hadoop102:3306/test', "</span><span class="token operator">*</span><span class="token operator">*</span> <span class="token operator">+</span>           <span class="token operator">*</span><span class="token operator">*</span><span class="token string">"  'connector.table' = 'sensor_count', "</span><span class="token operator">*</span><span class="token operator">*</span> <span class="token operator">+</span>           <span class="token operator">*</span><span class="token operator">*</span><span class="token string">"  'connector.driver' = 'com.mysql.jdbc.Driver', "</span><span class="token operator">*</span><span class="token operator">*</span> <span class="token operator">+</span>           <span class="token operator">*</span><span class="token oper